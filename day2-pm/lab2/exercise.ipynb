{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nyp.edu.sg/content/dam/nyp/logo.png\" width='200'/>\n",
    "\n",
    "Welcome to the lab! Before we get started here are a few pointers on Jupyter notebooks.\n",
    "\n",
    "1. The notebook is composed of cells; cells can contain code which you can run, or they can hold text and/or images which are there for you to read.\n",
    "\n",
    "2. You can execute code cells by clicking the ```Run``` icon in the menu, or via the following keyboard shortcuts ```Shift-Enter``` (run and advance) or ```Ctrl-Enter``` (run and stay in the current cell).\n",
    "\n",
    "3. To interrupt cell execution, click the ```Stop``` button on the toolbar or navigate to the ```Kernel``` menu, and select ```Interrupt ```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55y4bNNIC3Jt"
   },
   "source": [
    "# Lab 2 - Sentiment Analysis with Deep Learning (Keras)\n",
    "\n",
    "Now that we have learn how to use Naive Bayes and SVM to classify sentiments in a document of text, let's now learn how to use Deep Learning to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "print (\"Import helpers complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TP9-KK6JgMLr"
   },
   "source": [
    "## Section 2.1 - Load Data from CSV\n",
    "\n",
    "Update the following code below to load the training and test data from the CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "OSav2mIAFzWy",
    "outputId": "5c3c16c0-9e96-444c-bd37-549ff93395b7"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Update the code below to indicate the correct file names, the columns used \n",
    "# input text, and the output label class, and some maximum limits\n",
    "#\n",
    "load_text_data_from_csv(\n",
    "    \"???\",                              # CSV File to load the training data from\n",
    "    \"???\",                              # CSV File to load the test data from\n",
    "    \"???\",                              # Column name of the CSV that contains the input text\n",
    "    \"???\")                              # Column name of the CSV that contains the class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to see how the data loaded from the CSV files look like, when it's stored in Python variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_trainx_trainy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2 - Build a Word Dictionary and Tokenize Text\n",
    "\n",
    "The following function creates a dictionary of n words, based the words that appeared in all the training data. \n",
    "\n",
    "It also performs tokenization, which is a necessary step to split up a sentence into into words, and assigning a numeric identifier to the word. \n",
    "\n",
    "With Scikit-Learn, the tokenization (as well as Lemmatization) is handled by the NLTK toolkit and integrated into the Scikit-Learn processing pipeline. But in the case of Keras, we have to handle that by ourselves. While lemmatization is beneficial in Classical Machine Learning algorithms, it's improvement to performance may be less significant in Deep Learning, depending on the written language that you are trying to classify. In our case, we will proceed to classify our movie reviews without lemmatizing the text.\n",
    "\n",
    "Go ahead to run the cell below to build our dictionary and data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and tokenize all the strings into individual word index\n",
    "# in a dictionary.\n",
    "#\n",
    "# NOTE: If you want to this again, you must re-load the data from CSV in\n",
    "#       in step 2.1.\n",
    "#\n",
    "build_dictionary_and_tokenize_data(\n",
    "    50000,                                        # Max number of words in dictionary\n",
    "    2500)                                        # Max number of words per sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see how our sentences now look like.\n",
    "\n",
    "You can see that each sentence has been converted into a series of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_trainx_trainy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3 - Load the Glove Word Embeddings\n",
    "\n",
    "Word Embeddings are a well-recognized way of representing the meaning of a word in Machine Learning. Thanks to the vast sources of written text on the internet today from written articles, news, Wikipedias, user-generated content on social media, we have amassed a huge corpus of language data useful in training and producing a machine-learned representation of words.\n",
    "\n",
    "A few of the Word Embeddings that have been pre-trained and made available for download are the Glove and Word2Vec embeddings. These embeddings are basically a dictionary look-up that maps a word to a series of numbers.\n",
    "\n",
    "For this exercise, we will use the Glove Embeddings available here: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "We have already downloaded the Glove Embeddings file to the **\"data/glove.6B.200d.txt\"**. Update the path to that file and run the cell to load it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Set the path to the Glove Word Embedding file\n",
    "#\n",
    "load_glove_embedding(\"???\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the ??? with any known English word, and run the following cell to see how a real word embedding looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Set any known English word to see its Word Embedding.\n",
    "#\n",
    "display_word_embedding(\"???\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humans can visualize nearby words if we wrote all of them in Post-Its and pasted them in a 2-dimensional flipchart. But in practice, we need more than 2 dimensions to capture meaning in a word. The Glove Embedding that we use captures 200 dimensions of numbers per word. We won't be able to visualize nearby words with a 200-dimension representation, but machines will have no problems computing distances of words represented with any number of dimensions. \n",
    "\n",
    "By feeding the Word Embeddings of each word in a sentence to the Deep Learning model, we are essentially telling the Deep Learning model to make use of the meaning of each words in a paragraph to classify that paragraph. \n",
    "\n",
    "Run the following cell with any word to see how the machine can determine the closest matching/meaning words with the help of the Glove Word Embedding look up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Set any word here and run the cell to see which word is\n",
    "# close in meaning to the one you supplied.\n",
    "#\n",
    "display_nearby_words(\"???\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDgGHjCDgU6e"
   },
   "source": [
    "## Section 2.4 - Create the Deep Learning Text Classification Model\n",
    "\n",
    "The following creates the Deep Learning model for our Text Classification task. A typical Recurrent Neural Network will look like the following:\n",
    "\n",
    "<img src=\"files/we_rnn_dl.PNG\" height=\"100\">\n",
    "\n",
    "The Word Embedding captures the meaning of the words, while the Recurrent Neural Network attempts to make sense of the order and position of the words. \n",
    "\n",
    "---\n",
    "\n",
    "In the cell below, we have some codes to create the above model architecture, with some hyper-parameters that you can change to alter the structure of the model and how the model learns.\n",
    "\n",
    "For example, you may consider using GRU instead of LSTM, or you may choose to use a Bi-directional/Uni-directional model. A Uni-directional model takes in a sequence of words in the natural reading order. A Bi-directional model takes in a sentence in from first-to-last word and last-to-first word, allowing the model to capture contexts in both directions.\n",
    "\n",
    "Update the hyper-parameters and then run the cell to create the model.\n",
    "\n",
    "1. Recurrent Neural Network variant in the Recurrent Layer: **'lstm'**\n",
    "2. Neurons in Recurrent Layer: **32**\n",
    "2. Bi-directional: **True**\n",
    "3. Optimizer: **'adam'**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "geyYJIUoFzW3",
    "outputId": "40213176-e6d8-4d0d-c0f2-b6cd581ce2f9"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Update the hyper parameters before starting the training.\n",
    "#\n",
    "create_text_classifier_model_rnn(\n",
    "    2,                                        # Number of classes to predict \n",
    "    2500,                                     # Max number words per sentence\n",
    "    'glove',                                  # Word Embedding                  ('glove' / 'new')\n",
    "    '???',                                    # RNN variant                     ('rnn' / 'gru' / 'lstm')\n",
    "    0,                                        # Neurons in RNN layer            (typically 16 to 1024)\n",
    "    True,                                     # Use bi-directional RNN?         (False - Uni-directional / True - Bi-directional)\n",
    "    '???'                                     # Optimizer to learn              ('sgd' / 'adam')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlqlFTq_g0Oo"
   },
   "source": [
    "## Section 2.5 - Train Your Model\n",
    "\n",
    "Update the following parameters and then run the cell below to start the training. \n",
    "\n",
    "1. Learning rate = **0.01** (for 'sgd' optimizer), **0.001** (for 'adam' optimizer)\n",
    "2. Batch size = **100**,\n",
    "3. Number of epochs = **5**\n",
    "\n",
    "Take a look at the accuracy of your classifications on the test data and compare its F1-score with your classical Machine Learning models. Try to adjust some of the hyper-parameters above including the RNN variant, the number of neurons, using/not using bi-directional networks, using a different optimizer, changing the batch size and the number of epochs. The run Section 2.4 and 2.5 again to train.\n",
    "\n",
    "Try experimenting with different combinations of hyper-parameters to see if you can achieve a good F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9GOPphNXFzW6",
    "outputId": "e3831a81-8ccc-410f-d222-42c0852f2c1b"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Set the learning rate of the deep learning network\n",
    "# A large learning rate helps to adjust quickly the network weights to the optimal \n",
    "# goal, but it may also cause it to over-shoot its goal. A small learning rate\n",
    "# causes the network to learn very slowly, but it may bring it to its nearest goal\n",
    "#\n",
    "set_learning_rate(0)\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# Update the batch size, and the number of epochs to train.\n",
    "#\n",
    "train_text_classifier_model(\n",
    "    0,                         # Batch size.\n",
    "    0                          # Number of epochs/iterations to train this model.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZiyb6wXhCzU"
   },
   "source": [
    "## Section 2.6 - Save Your Text Classification Model\n",
    "\n",
    "Run the following code below to save your Text Classification model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CC-_8hDFzXB"
   },
   "outputs": [],
   "source": [
    "save_text_classifier_model(\"models/text_classifier.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0FY1W-9hJ_L"
   },
   "source": [
    "## Section 2.7 - Load Your Text Classification Model\n",
    "\n",
    "Run the following code below to load your Text Classification model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-2hmOetFzXD"
   },
   "outputs": [],
   "source": [
    "load_text_classifier_model(\"models/text_classifier.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ExHRj7ehTbg"
   },
   "source": [
    "## Section 2.8 - Try Out Your Model\n",
    "\n",
    "Run the following code to try your model.\n",
    "\n",
    "Discuss how you feel about this new classifier:\n",
    "\n",
    "1. Did the accuracy of your model performance increase with Deep Learning?\n",
    "2. What do you think you can do to further improve the accuracy of your model?\n",
    "3. How does the accuracy of your model feel when you are testing it manually?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "_wMECaArFzXG",
    "outputId": "def6d8ca-fd5f-47be-8629-7a2fae0c5bf1"
   },
   "outputs": [],
   "source": [
    "print (\"Enter some text:\")\n",
    "user_text = input()\n",
    "classify_text(user_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.9 - Explore helpers.py\n",
    "\n",
    "Go ahead again to examine the codes in helpers.py to see how we create the Deep Learning model in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "exercise01b-text-classification-deeplearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
