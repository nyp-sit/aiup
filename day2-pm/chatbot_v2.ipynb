{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/aiup/blob/main/day2-pm/chatbot_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejSugy-gTX5U"
      },
      "source": [
        "# Build a ChatBot with Large Language Model\n",
        "\n",
        "In this exercise, you will learn how to build a LLM-based chatbot.\n",
        "We will explore two options:\n",
        "1. using cloud-based services such as (Azure) Open AI servies or\n",
        "2. using locally hosted open source model such as Llama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade gradio"
      ],
      "metadata": {
        "id": "HdNvaKiq973s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1 - Using cloud API\n",
        "\n",
        "We will be using Azure Open AI services in this lab exercise"
      ],
      "metadata": {
        "id": "5mNqdBgS5FQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "nSo8SIN25m9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AzureOpenAI\n",
        "\n",
        "AZURE_ENDPOINT = \"https://nypopenai2.openai.azure.com/\"\n",
        "API_KEY = \"c0f0043bfe9b443eb7d02fc5edc525d2\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_key=API_KEY,\n",
        "    api_version=\"2024-07-01-preview\",\n",
        "    azure_endpoint=AZURE_ENDPOINT)"
      ],
      "metadata": {
        "id": "aq7xJZCj5Tr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_from_messages(messages, model=\"gpt-4o-global\", temperature=0):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "#     print(str(response.choices[0].message))\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "yhP4_MmB6A8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code below, we first define context where the LLM will use to generate response. Context provides a reference (grounding) to guide LLM response."
      ],
      "metadata": {
        "id": "XR-BgJnA9v1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = [ {'role':'system', 'content':\"\"\"\n",
        "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
        "You first greet the customer, then collects the order, \\\n",
        "and then asks if it's a pickup or delivery. \\\n",
        "You wait to collect the entire order, then summarize it and check for a final \\\n",
        "time if the customer wants to add anything else. \\\n",
        "If it's a delivery, you ask for an address. \\\n",
        "Finally you collect the payment.\\\n",
        "Make sure to clarify all options, extras and sizes to uniquely \\\n",
        "identify the item from the menu.\\\n",
        "You respond in a short, very conversational friendly style. \\\n",
        "The menu includes \\\n",
        "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
        "cheese pizza   10.95, 9.25, 6.50 \\\n",
        "eggplant pizza   11.95, 9.75, 6.75 \\\n",
        "fries 4.50, 3.50 \\\n",
        "greek salad 7.25 \\\n",
        "Toppings: \\\n",
        "extra cheese 2.00, \\\n",
        "mushrooms 1.50 \\\n",
        "sausage 3.00 \\\n",
        "canadian bacon 3.50 \\\n",
        "AI sauce 1.50 \\\n",
        "peppers 1.00 \\\n",
        "Drinks: \\\n",
        "coke 3.00, 2.00, 1.00 \\\n",
        "sprite 3.00, 2.00, 1.00 \\\n",
        "bottled water 5.00 \\\n",
        "\"\"\"} ]  # accumulate messages\n",
        "\n",
        "\n",
        "\n",
        "def get_response(message, history):\n",
        "    context.append({'role':'user', 'content':f\"{message}\"})\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-global\",\n",
        "        messages=context,\n",
        "        temperature=0.0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "af7q4CJy6Yqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.ChatInterface(get_response, type=\"messages\").launch(debug=True)"
      ],
      "metadata": {
        "id": "U6MgHtma7bYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CieOXtGqpv9"
      },
      "source": [
        "## Option 2 - Locally hosted LLM\n",
        "\n",
        "We can use tools such as Ollama to run a supported LLM model. Ollama is a very popular serving platform to run large language models locally on your PC, and has optimized to accelerate the model's inference speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VLBPrpiUJgR"
      },
      "source": [
        "### Setting Up the Environment\n",
        "\n",
        " and use Ollama to run the Lllama model.\n",
        "Ollama is a very popular serving platform to run large language models locally on your PC, and has optimized to accelerate the model's inference speed.\n",
        "First, we need to set up our Colab notebook to support command-line operations, so that we can use command line to install the Ollama.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wis4oD1MOvwi"
      },
      "outputs": [],
      "source": [
        "!pip install colab-xterm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6MiEgyLUh62"
      },
      "source": [
        "### Run Ollama using terminal\n",
        "\n",
        "Run the following command in the terminal window to install Ollama:\n",
        "\n",
        "```\n",
        "curl https://ollama.ai/install.sh | sh\n",
        "```\n",
        "\n",
        "Start the Ollama server using the following command:\n",
        "\n",
        "```\n",
        "ollama serve &\n",
        "```\n",
        "\n",
        "The `&` at the end runs the command in the background, allowing you to continue using your terminal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ1_YSGANQhj"
      },
      "outputs": [],
      "source": [
        "%load_ext colabxterm\n",
        "%xterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA3L4eHOVVzN"
      },
      "source": [
        "### Pulling AI Models\n",
        "\n",
        "Now that the Ollama server is running, we can pull AI models to use with our server. Letâ€™s pull a Llama 3.2 1B parameters model as an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tIpczzAO0hI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!ollama pull llama3.1:8b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sPBF8kLVui7"
      },
      "source": [
        "### Interacting with Ollama using Python API\n",
        "\n",
        "We will need to install Ollama python package to allow us to write code to interact with Ollama-hosted models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryGh9HMVRSj9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install ollama\n",
        "!pip install jupyter_bokeh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyM6-CCJQGHp"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "response = ollama.chat(model='llama3.1:8b', messages=[\n",
        "  {\n",
        "    'role': 'user',\n",
        "    'content': 'Why is the sky blue?',\n",
        "  }], options = {'temperature': 0.0})\n",
        "print(response['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVEsnslYWNRU"
      },
      "outputs": [],
      "source": [
        "context = [ {'role':'system', 'content':\"\"\"\n",
        "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
        "When you collects the order, \\\n",
        "asks if it's a pickup or delivery. \\\n",
        "You wait to collect the entire order, then summarize it and check for a final \\\n",
        "time if the customer wants to add anything else. \\\n",
        "If it's a delivery, you ask for an address. \\\n",
        "Finally you collect the payment.\\\n",
        "Make sure to clarify all options, extras and sizes to uniquely \\\n",
        "identify the item from the menu.\\\n",
        "You respond in a short, conversational and professional style. \\\n",
        "Once you have all the order details, just say bye and thank you. \\\n",
        "The menu includes \\\n",
        "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
        "cheese pizza   10.95, 9.25, 6.50 \\\n",
        "eggplant pizza   11.95, 9.75, 6.75 \\\n",
        "fries 4.50, 3.50 \\\n",
        "greek salad 7.25 \\\n",
        "Toppings: \\\n",
        "extra cheese 2.00, \\\n",
        "mushrooms 1.50 \\\n",
        "sausage 3.00 \\\n",
        "canadian bacon 3.50 \\\n",
        "AI sauce 1.50 \\\n",
        "peppers 1.00 \\\n",
        "Drinks: \\\n",
        "coke 3.00, 2.00, 1.00 \\\n",
        "sprite 3.00, 2.00, 1.00 \\\n",
        "bottled water 5.00 \\\n",
        "\"\"\"} ]  # accumulate messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M6QSOJ9tNQz"
      },
      "outputs": [],
      "source": [
        "response = ollama.chat(model='llama3.1:8b', messages=context)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW2odLe7sOVP"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import ollama\n",
        "\n",
        "context = [ {'role':'system', 'content':\"\"\"\n",
        "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
        "You first greet the customer, then collects the order, \\\n",
        "and then asks if it's a pickup or delivery. \\\n",
        "You wait to collect the entire order, then summarize it and check for a final \\\n",
        "time if the customer wants to add anything else. \\\n",
        "If it's a delivery, you ask for an address. \\\n",
        "Finally you collect the payment.\\\n",
        "Make sure to clarify all options, extras and sizes to uniquely \\\n",
        "identify the item from the menu.\\\n",
        "You respond in a short, very conversational friendly style. \\\n",
        "The menu includes \\\n",
        "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
        "cheese pizza   10.95, 9.25, 6.50 \\\n",
        "eggplant pizza   11.95, 9.75, 6.75 \\\n",
        "fries 4.50, 3.50 \\\n",
        "greek salad 7.25 \\\n",
        "Toppings: \\\n",
        "extra cheese 2.00, \\\n",
        "mushrooms 1.50 \\\n",
        "sausage 3.00 \\\n",
        "canadian bacon 3.50 \\\n",
        "AI sauce 1.50 \\\n",
        "peppers 1.00 \\\n",
        "Drinks: \\\n",
        "coke 3.00, 2.00, 1.00 \\\n",
        "sprite 3.00, 2.00, 1.00 \\\n",
        "bottled water 5.00 \\\n",
        "\"\"\"} ]  # accumulate messages\n",
        "\n",
        "def get_response(message, history):\n",
        "\n",
        "    context.append({'role':'user', 'content':f\"{message}\"})\n",
        "    response = ollama.chat(model='llama3.1:8b', messages=context, options = {'temperature': 0.0})\n",
        "    response_msg = response['message']['content']\n",
        "    return response_msg\n",
        "\n",
        "\n",
        "gr.ChatInterface(get_response, type=\"messages\").launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMoRHy5say7Z4qZXhHW/T8E",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}