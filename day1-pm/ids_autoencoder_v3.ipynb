{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hz79Xs_YVMxC"
   },
   "source": [
    "<img src=\"https://www.nyp.edu.sg/content/dam/nyp/logo.png\" width=\"200\" height=\"150\"/>\n",
    "\n",
    "Welcome to the lab! Before we get started here are a few pointers on Jupyter notebooks.\n",
    "\n",
    "1. The notebook is composed of cells; cells can contain code which you can run, or they can hold text and/or images which are there for you to read.\n",
    "\n",
    "2. You can execute code cells by clicking the ```Run``` icon in the menu, or via the following keyboard shortcuts ```Shift-Enter``` (run and advance) or ```Ctrl-Enter``` (run and stay in the current cell).\n",
    "\n",
    "3. To interrupt cell execution, click the ```Stop``` button on the toolbar or navigate to the ```Kernel``` menu, and select ```Interrupt ```.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_U--l1zVMxD"
   },
   "source": [
    "# Anomaly Detection for Network Traffic\n",
    "\n",
    "<div>           \n",
    "<center>\n",
    "    <img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/intrusion_alert.png\" width=\"300\" height=\"200\"/>\n",
    "</center>\n",
    "</div>\n",
    "\n",
    "In this lab, we will try to build an anomaly detector for network traffic, which can detect attack traffic such as Denial of Service (DoS), Heartbleed, etc. We will make use of two pre-processed network traffic files, which contain features extracted from the raw traffic. These features are extracted from TCP and UDP flows. One file contains only normal traffic, which we will use for training an Autoencoder neural network. The other file contains mixture of normal and attack traffic, which we will use for fine-tuning the threshold of anomaly score, and for final testing.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APMeGN5YVMxD"
   },
   "source": [
    "## Import libraries\n",
    "\n",
    "We begin by importing the libraries that we need, mainly *scikit-learn* (which contains some useful methods for scaling the data, and  for calculating various evaluation metrics, such as precision/recall scores), *tensorflow* (which is the framework that we use to build the autoencoder neural network) and *matplotlib* (which we use for data visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YPh5y0jVMxE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scikit-learn libraries used for evaluation metrics, scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# pandas libraries for manipulating dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# numerical library \n",
    "import numpy as np\n",
    "\n",
    "#import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Nadam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import mse\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rf1eqRZDVMxG"
   },
   "source": [
    "## Getting the data\n",
    "\n",
    "We download the data that we need for this lab. The is a zip file which contain csv files for different days of network traffic data. After download, the files are unzipped to a directory called ```'ids_dataset'```. You should be able to see the directory at the left sidebar (file browser). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6nXWoCRzVMxH",
    "outputId": "ce783b55-4743-41dd-f9a7-80f851fbc842"
   },
   "outputs": [],
   "source": [
    "base_dataset_dir = 'ids_dataset'\n",
    "datafile_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/ids_dataset.zip'\n",
    "download_data(base_dataset_dir, datafile_url, extract=True, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6Dizj00VMxJ"
   },
   "source": [
    "For this lab exercise, we will use the following two files in the ```ids_dataset``` directory: \n",
    "\n",
    "1. Monday-WorkingHours.pcap_ISCX.csv - contains features extracted from normal traffic \n",
    "2. Wednesday-workingHours.pcap_ISCX.csv - contains a mixture of normal traffic and attack traffic such as DoS/DDoS, Heartbleed, slowloris, Goldeneye, etc.  \n",
    "\n",
    "We use the pandas library to read data from CSV files into panda dataframe (dataframe is a 2D data structure similar to database table that contains columns and rows. Row represents individual data sample, and columns represent the different features of the data sample) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2KpNTd5VMxK"
   },
   "outputs": [],
   "source": [
    "normal_traffic = os.path.join(base_dataset_dir, 'Monday-WorkingHours.pcap_ISCX.csv')\n",
    "mixed_traffic = os.path.join(base_dataset_dir, 'Wednesday-workingHours.pcap_ISCX.csv')\n",
    "\n",
    "# Read the normal (benign) traffic data into pandas dataframe called df_normal\n",
    "df_normal = pd.read_csv(normal_traffic)\n",
    "\n",
    "# Read the mixed (benign + malicious) traffic data into pandas dataframe called df_mixed\n",
    "df_mixed = pd.read_csv(mixed_traffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjKrrj3kVMxM"
   },
   "source": [
    "## Data Exploration \n",
    "\n",
    "Let us explore the data a bit more. We use panda dataframe ```info()``` method to get more information about the features (columns), number of rows with non-null values for each column, and the data type of each column (feature). As we we will see from the display below, that we have 78 features (columns 0-77) and the last column (column 78) is the label of the traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SnClZS1oVMxM",
    "outputId": "45b4db9b-0d28-4e29-ddfe-0f5c881931f9"
   },
   "outputs": [],
   "source": [
    "# display the information about the dataframe\n",
    "df_normal.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttp7kFxUVMxP"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Take a look athe feature column ```Flow Bytes/s``` from the display, what do you observe? \n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "We can see that there a few rows that have missing values for ```Flow Bytes/s```. There are 529854 rows which have non-null values for ```Flow Bytes/s```, compared to others that have 529918 non-null rows. That means there are a total of 64 empty values (or in Machine learning jargon, we call it NaN or Not-a-Number).\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's us take a look at the values of the target label to see what are different values for the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "20LOcfb6VMxQ",
    "outputId": "987a451f-d25b-429d-b76f-f1f847cda837"
   },
   "outputs": [],
   "source": [
    "df_normal[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCaLlCjaVMxS"
   },
   "source": [
    "Since this dataset only contains 'normal' traffic, the label should only have one value **BENIGN**. Let us also find out what type of labels we have for mixed_traffic.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "In the following cell, modify the codes to display the types of labels we have for mixed traffic data. What are different types of attack traffic? \n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "    \n",
    "```\n",
    "df_mixed[' Label'].value_counts()\n",
    "```\n",
    "    \n",
    "<br/>\n",
    "    \n",
    "From the display, we see that most of the attack traffic are ``DoS Hulk``, the rest being DoS ``GoldenEye``, ``DoS slowloris``, ``DoS Slowhttptest``, and ``Heartbleed``.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "LLC-EUMDVMxS",
    "outputId": "02a284cd-9ad5-4eff-dc97-943da3303260"
   },
   "outputs": [],
   "source": [
    "## TODO: MODIFY THE LINE BELOW ##\n",
    "\n",
    "df_normal[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the display, we see that most of the attack traffic are DoS Hulk, the rest being DoS GoldenEye, DoS slowloris, DoS Slowhttptest, and Heartbleed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7CzXRZTVMxV"
   },
   "source": [
    "Let us also take a look at the statistics of the different feature columns to have some ideas of the numeric values we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qiGvRTvNVMxV",
    "outputId": "e8c51baf-90a7-4fbd-a08c-d8fc7d75d4b0"
   },
   "outputs": [],
   "source": [
    "## By default, jupyter notebook only display certain number of rows, we set it to 500 so we can see all the info\n",
    "pd.set_option('display.max_rows', 500)\n",
    "df_normal.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qaPOvjjyVMxX"
   },
   "source": [
    "From the display, we can see that there are a few feature columns that have ```infinite``` (indicated by```inf```) values (as caclulated by CICFlowMeter). \n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Can you identify which columns? \n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "    \n",
    "The columns ```Flow Bytes/s``` and ```Flow Packets/s``` have **inf** as max value, and thus also have **inf** as mean amd stardard deviation.  \n",
    "    \n",
    "<br/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XT4HF60RVMxX"
   },
   "source": [
    "**Exercise** \n",
    "\n",
    "Now modify the codes below to examine the values of `df_mixed`. Are there any abnormal values in `df_mixed`?\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "\n",
    "```\n",
    "df_mixed.describe().T\n",
    "```\n",
    "\n",
    "<br/>\n",
    "    \n",
    "The columns ```Flow Bytes/s``` and ```Flow Packets/s``` also contain inf as max value.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wnW1fFrJVMxY",
    "outputId": "922aeb4b-8c25-4a7b-8e8d-150ed097fff7"
   },
   "outputs": [],
   "source": [
    "# TODO: Modify the line below to find out more info about the numeric values of df_mixed\n",
    "df_normal.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zq7AbayRVMxZ"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Data Cleaning \n",
    "\n",
    "We will need to clean up the data by removing the ```NaN``` values and the ```Inf``` values, as machine learning algorithm has problem dealing with these types of values.  Since we don't have too many rows of ```NaN``` and ```Inf``` values, the easiest way is to drop them. The code below first tells pandas to treat `Inf` values as `NaN` (```'mode.use_inf_as_na'```) and then drops all rows that contains ```NaN``` values (effectively dropping both rows with ```NaN``` and  ```Inf``` values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPKF_aiHVMxa"
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN and Inf values\n",
    "\n",
    "with pd.option_context('mode.use_inf_as_na', True):\n",
    "    df_normal = df_normal.dropna(subset=['Flow Bytes/s', ' Flow Packets/s'], how='all')\n",
    "    df_mixed = df_mixed.dropna(subset=['Flow Bytes/s', ' Flow Packets/s'], how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** \n",
    "\n",
    "Now add the code here to see if you still have `NaN` or `Inf` values.  (Hint: refers to the codes in previous cells to see how you can do this)\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "    \n",
    "You can use `info()`, as well as `describe()` methods to check the values of df_normal and df_mixed\n",
    "    \n",
    "<br/>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add code her to check if you still have NaN or Inf values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzZCnvGPVMxc"
   },
   "source": [
    "### Simplify the number of labels into binary labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DBVAGuNDVMxc"
   },
   "source": [
    "In this exercise, we are only interested in knowing if the traffic is 'normal' or 'abnormal', so we will group all the different attack traffic types into just one label, i.e. label '1' (think of 1 as positive case) while the benign traffic will be labelled as 0 (negative case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDDqnIOEVMxd"
   },
   "outputs": [],
   "source": [
    "df_normal[' Label'] = df_normal[' Label'].apply(lambda x: 1 if x != 'BENIGN' else 0)\n",
    "df_mixed[' Label'] = df_mixed[' Label'].apply(lambda x: 1 if x != 'BENIGN' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8R47iBhuVMxg"
   },
   "source": [
    "Now let's see our new traffic labels for ```df_mixed```. We should see that they now have only two values 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "htWlloBKVMxg",
    "outputId": "2548bb8e-9c74-499a-bdc2-1b13bf363e54"
   },
   "outputs": [],
   "source": [
    "df_mixed[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wI3iJKinVMxi"
   },
   "source": [
    "### Create Training dataset and Validation dataset\n",
    "\n",
    "Here we will create the training set and the validation set. We will use the normal-only traffic (df_normal) as training and validation set. Since we are using unsupervised learning, we do not need the 'Label', so we will drop the label column during training.\n",
    "\n",
    "In the code below, we use 20% of data as validation set, and the remaining 80% for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXja-fuNVMxj"
   },
   "outputs": [],
   "source": [
    "# Drop the label column\n",
    "df_normal_nolabel = df_normal.drop([' Label'], axis=1)\n",
    "\n",
    "# Split the normal traffic data into Train and Validation set. \n",
    "X_train, X_val = train_test_split(df_normal_nolabel, test_size=0.2, shuffle=True, stratify=df_normal[' Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RU_vNHdVMxl"
   },
   "source": [
    "### Create Test dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BN8kfSQUVMxl"
   },
   "source": [
    "As mentioned, we will use the mixed traffic data as our test set. We have more than 600,000 samples, we will just use half of the samples for testing. We can use the same `train_test_split` method as convenience method to split the data into half, with the advantage of random shuffling and also make sure each labels are well represented in each half of the split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate 50% to fine-tuning set, and 50% to final test set\n",
    "df_test, _  = train_test_split(df_mixed, test_size=0.5, shuffle=True, stratify=df_mixed[' Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gh_qM68pVMxn"
   },
   "source": [
    "In the code below, we will separate out the label column and call it `y_test`. The remaining feature columns will be called `X_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgvdFVtZVMxn"
   },
   "source": [
    "***Note***: Although we do not need label in unsupervised learning, however, in this case, we will keep the labels and use them to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcevTAncVMxw"
   },
   "outputs": [],
   "source": [
    "y_test = df_test[' Label']\n",
    "X_test = df_test.drop(' Label', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-pC2aCJVMxy"
   },
   "source": [
    "### Scaling the data \n",
    "\n",
    "The last step we need to do in our Data Preparation task is to scale the data by removing the mean and scaling to unit variance. Most machine learning algorithms (including neural networks) required data to be scaled to some small values to be able to learn better. Note that we compute the mean and standard deviation from the **training set only** and use the computed mean and standard deviation to scale the validation and the test set. This is to prevent information leakage from validation/test set to training set. We use the `StandardScaler()` from Scikit-Learn to compute mean/standard deviation and to scale the data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMriNJIsVMxy"
   },
   "outputs": [],
   "source": [
    "# Compute the mean and standard deviation using StandardScaler() and transform (scale) the X_train values.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFazXrT4VMx0"
   },
   "outputs": [],
   "source": [
    "# Now we use the scaler computed from X_train to scale the Validation, Tuning and Test set.\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Complete the codes below to scale the  `X_test`. \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "<br/>\n",
    "    \n",
    "```\n",
    "X_tune_scaled = scaler.transform(X_tune)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "</details>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the code below to scale the test set \n",
    "X_test_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SV9jlYxnVMx1"
   },
   "outputs": [],
   "source": [
    "# Good to make sure that our training set does not contain any NaN values, just in case, before we start training.\n",
    "assert not np.any(np.isnan(X_train_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nzkQGiJWVMx3"
   },
   "source": [
    "Congratulations, we have finished our data preparation !!! As in all machine learning projects, data preparation is always the **most** time consuming phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8lJYAsAVMx3"
   },
   "source": [
    "## Build the Variational Autoencoder (VAE) network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a couple of diemensions. \n",
    "\n",
    "We set the input dimension (input_dim) to be number of columns (features) in X_train.  X_train.shape will give you a tuple of (#rows, #columns), to get the #columns, we take the 2nd value in the tuple (0 being 1st value, 1 being 2nd value)\n",
    "\n",
    "We also set the latent dimension. Recall from the lecture that Autoencoder will try to learn the essential information from the data and try to compress the information into the 'latent space'. `latent_dim` is the size of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpBJwkeKVMx3"
   },
   "outputs": [],
   "source": [
    "# we set the input dimensions to the number of features in X, i.e. 78.  \n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# we set the dimension of the latent representation to 20 neurons, \n",
    "# in other words, we want to condense the information in the input into the latent space of 20 neurons\n",
    "latent_dim = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FwThzk_VMx5"
   },
   "source": [
    "### Encoder Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6XqjY9_VMx5"
   },
   "source": [
    "We will now build the Encoder part of the Variational Autoencoder (VAE). One main difference between a Variational Autoencoder and a normal Autoencoder is that VAE learns the mean and the standard deviation of the inputs, basically learning the distribution of the inputs. It will then sample from this learnt distribution to produce codings that can be used by the decoder to produce an output.\n",
    "\n",
    "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/vae_encoder.png\" height='500' width='400'>\n",
    "\n",
    "In the following code, we construct our encoder which consists of 3 hidden layers: 1st hidden layer with 80 neurons, 2nd hidden layer with 64 neurons and the 3rd hidden layer with 32 neurons). Our latent layers consist of `laten_dim` neurons. \n",
    "\n",
    "We have two latent layers, one for ```codings_mean``` which is the mean, and one for ```log_var``` which is the log of variance (instead of using standard deviation, it is mathematically easier to use log of variance, for computation purpose) learnt. ```Sampling()``` is a function used to sample from this learnt normal distribution with the learnt mean and variance. (Note: You can examine the `utils.py` file if you want to see how the sampling is done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "id": "765R2lXTVMx5",
    "outputId": "84bb4863-10de-4610-ca52-1d27be2800ee"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(input_dim,), name='Input')\n",
    "z = Dense(80, activation='tanh', name='hidden1')(inputs)\n",
    "z = Dense(60, activation='tanh', name='hidden2')(z)\n",
    "z = Dense(50, activation='tanh', \n",
    "          activity_regularizer=tf.keras.regularizers.l1(1e-3), \n",
    "          name = 'hidden3')(z)\n",
    "z = Dense(32, activation='tanh', \n",
    "          activity_regularizer=tf.keras.regularizers.l1(1e-3), \n",
    "          name = 'hidden4')(z)\n",
    "codings_mean = Dense(latent_dim, name='mean')(z)\n",
    "codings_log_var = Dense(latent_dim, name='log_var')(z)\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "vae_encoder = Model(inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n",
    "\n",
    "vae_encoder.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUqBcvkrVMx-"
   },
   "source": [
    "### Decoder Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6FfEG_BVMx_"
   },
   "source": [
    "Here, we will build the Decoder part of the VAE. \n",
    "\n",
    "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/vae_decoder.png\" height=\"500\" width=\"400\"/>\n",
    "\n",
    "The decoder will be 'inverse' copy of the encoder, with each layer having more and more neurons (32 -> 64 -> 80) until it get backs the original input size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "_ybr2Iu4VMx_",
    "outputId": "fce9ecec-a6c1-4781-e6e2-c306caecedc6"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=[latent_dim], name='codings')\n",
    "x = Dense(32, activation='tanh', name='hidden5')(decoder_inputs)\n",
    "x = Dense(50, activation='tanh', name='hidden6')(x)\n",
    "x = Dense(60, activation='tanh', name='hidden7')(x)\n",
    "x = Dense(80, activation='tanh', name='hidden8')(x)\n",
    "outputs = Dense(input_dim, activation=\"linear\", name='output')(x)\n",
    "vae_decoder = Model(inputs=[decoder_inputs], outputs=[outputs])\n",
    "vae_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFtTdPvNVMyB"
   },
   "source": [
    "The codes below are just specifying the two loss functions that the model will use to try to minimise the losses at each training step. \n",
    "\n",
    "The first loss function is the MSE (mean-squared-error or mse) loss which is computed from the differences between predicted output and expected output.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{m}\\sum_{i=1}^m (\\hat{y}_{i} - y_i)^2\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\hat{y}$ is the predicted output and $y$ is the actual value. \n",
    "\n",
    "The second loss which is the latent_loss is given by the following formula: \n",
    "\n",
    "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/latent_loss_formula.png\" height='30' width='400'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Y1wBPy5VMyD"
   },
   "outputs": [],
   "source": [
    "_, _, codings = vae_encoder(inputs)\n",
    "reconstructions = vae_decoder(codings)\n",
    "vae = Model(inputs=[inputs], outputs=[reconstructions])\n",
    "\n",
    "# Compute the latent loss (based on mean and variance)\n",
    "latent_loss = -0.5 * K.sum(\n",
    "    1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),\n",
    "    axis=-1)\n",
    "\n",
    "# reconstruction loss is based on MSE loss\n",
    "reconstruction_loss = losses.mse(inputs, reconstructions)\n",
    "reconstruction_loss *= input_dim\n",
    "\n",
    "# total loss for VAE is the mean of both latest and reconstruction loss\n",
    "vae_loss = K.mean(latent_loss + reconstruction_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "rmsprop = RMSprop()\n",
    "vae.compile(loss='mean_squared_error', optimizer=rmsprop, metrics=['mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "colab_type": "code",
    "id": "z0_xHDnLVMyE",
    "outputId": "42b9209f-461e-4f28-8f31-9b8d93528e12"
   },
   "outputs": [],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZAPUB3CVMyH"
   },
   "source": [
    "### Training the Network \n",
    "\n",
    "Now we are ready to train our VAE. Neural network like VAE learns by trying to minimise the training loss (i.e the error between its predicted output and the actual expected output) at each training cycle (which we call *epoch*). \n",
    "\n",
    "As the network learns, training loss and validation loss will be less as the network makes less and less mistakes. So normally we want to train the network until it has very low loss or the loss has plateaud. \n",
    "\n",
    "In the `fit()` function below, we pass `X_train_scaled` as both the input and expected output (so called target labels). We also pass `X_val_scaled` for calculating our validation errors to see if our model is overfitting.\n",
    "\n",
    "An overfitted model means it performs very well on the training data but does very badly on unseen new data. The performance on unseen data is the one we are most interested in. The larger the difference between validation and training loss, the more overfitting it is. So we would want the gap to be small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1tJQP00OVMyG",
    "outputId": "0fb4c870-97a5-49b7-f2e1-142604d8fad7"
   },
   "outputs": [],
   "source": [
    "train = True\n",
    "num_epochs = 5\n",
    "batch_size = 512\n",
    "save_model = False\n",
    "\n",
    "if train:     \n",
    "    run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "    history = vae.fit(X_train_scaled, X_train_scaled, \n",
    "                      epochs=num_epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      verbose=1, \n",
    "                      shuffle=True,\n",
    "                      validation_data = (X_val_scaled, X_val_scaled),\n",
    "                      callbacks=[tensorboard_cb])\n",
    "    \n",
    "    if save_model: \n",
    "        vae.save('trained_model')\n",
    "else:\n",
    "    vae = tf.keras.models.load_model('trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PK3MszQGVMyI"
   },
   "source": [
    "Now we will plot the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQGAjk_vVMyI",
    "outputId": "817eb2cf-ef5b-45aa-ece2-98576dd55a5b"
   },
   "outputs": [],
   "source": [
    "## plot the training loss\n",
    "if train:\n",
    "    with open('training.history', 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    plot_training_loss(history.history)\n",
    "else:\n",
    "    with open('training.history', 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "    plot_training_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Change the epochs to larger value and train. What do you observe? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xvsHmm9LVMyK"
   },
   "source": [
    "### Evaluate the trained model on Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xXHqEDNIVMyK"
   },
   "source": [
    "Now let us compare the reconstruction error (MSE) between training set (the clean, benign traffic) and that of test set (the one with benign + attack traffic). In the below, we compute the mean of the entire data set (of X_train_scaled and X_tune_scaled), as we just want to have a feel of the overall error of the two different data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUvh6wX6VMyK"
   },
   "outputs": [],
   "source": [
    "X_train_preds = vae.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GWcsUaIVVMyM",
    "outputId": "e9aa01aa-b297-4152-e3ec-9b173367844b"
   },
   "outputs": [],
   "source": [
    "X_train_mse  =   np.square(X_train_scaled - X_train_preds).mean()\n",
    "print(X_train_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Complete the code below to compute and display the MSE of test dataset. \n",
    "\n",
    "What do you observe about the value of MSE? Is it higher or lower than the MSE of the training set?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "<br/>\n",
    "    \n",
    "```\n",
    "X_tune_preds = vae.predict(X_tune_scaled)\n",
    "X_tune_mse = np.square(X_tune_scaled - X_tune_preds).mean()\n",
    "print(X_tune_mse)\n",
    "```\n",
    "\n",
    "<br/>\n",
    "    \n",
    "From the value of `X_tune_mse`, we can see that the MSE (the mean squared error) for the tuning dataset is much much than the training set. The presence of attack traffic make it more difficult for Variational Autoencoder to reconstruct the input accurately, leading to higher MSE.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_l3kqOKVMyO",
    "outputId": "0f7f6124-0eef-4605-c1a1-11c5cf1e51c0"
   },
   "outputs": [],
   "source": [
    "# TODO: complete the code below\n",
    "\n",
    "X_test_preds = None\n",
    "X_test_mse = None \n",
    "print(X_test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eh9Z7HdCVMyQ"
   },
   "source": [
    "### Compute the distance between Train set and the 'Tuning' set. \n",
    "\n",
    "Now let us compute the 'distance' between each sample and its reconstructed version. Think of 'distance' as the difference between two different multi-dimensional vector. As each sample is a multi-demensional vector, the difference will be multi-dimensional as well. Therefore we will use the [*Frobenius norm*](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) to measure the distance between the sample and its reconstruction.  In the following, we use the numpy library function `np.linalg.norm()` to compute the *Frobenius norm*.  \n",
    "\n",
    "**Note:** \n",
    "\n",
    "In this section, we are interested in knowing error of each sample, unlike the previous section, where we are just looking at mean error rate of entire train or tuning set.  The reason is we want to compare error of each sample with the error threshold to determine if the sample is anomalous or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyZD3nEYVMyQ",
    "outputId": "ddbc58bb-6034-4cb8-f61f-1ed90dc71037"
   },
   "outputs": [],
   "source": [
    "X_train_preds = vae.predict(X_train_scaled)\n",
    "X_train_errors = np.linalg.norm(X_train_scaled - X_train_preds, axis=1)\n",
    "print('min = ',X_train_errors.min())\n",
    "print('max = ',X_train_errors.max())\n",
    "print('mean = ',X_train_errors.mean())\n",
    "print('std = ',X_train_errors.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mSDmT8yqVMyU",
    "outputId": "fc441911-fde2-441f-8454-eb672829bfaa"
   },
   "outputs": [],
   "source": [
    "X_test_preds = vae.predict(X_test_scaled)\n",
    "X_test_errors = np.linalg.norm(X_test_scaled - X_test_preds, axis=1)\n",
    "print('min = ', X_test_errors.min())\n",
    "print('max = ',X_test_errors.max())\n",
    "print('mean = ',X_test_errors.mean())\n",
    "print('std = ', X_test_errors.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcLd_CxtVMyV"
   },
   "source": [
    "## Choosing the Anomlay Score Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AV6MdaGBVMyW"
   },
   "source": [
    "Based on above, it seems reasonable to set the threshold at mean + 3 std deviation of training errors. In the absence of labels to help us adjust threshold for the anomaly detector, we could set the threshold derived by statistical means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUOvsekbVMyW",
    "outputId": "0859e44f-7e6d-4c8c-eb1f-f4ad9075d17d"
   },
   "outputs": [],
   "source": [
    "threshold = X_train_errors.mean() + 3 * X_train_errors.std()\n",
    "\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nu1NFVxZVMyY"
   },
   "source": [
    "Let us use this threshold on the test set. If the error of a test sample is greater than the threshold, assign it a label 1 (attack), otherwise, assign it a label 0 (benign). We use `y_test_pred` to keep our predicted labels and compare `y_test_pred` to `y_test` (the ground truth label we have) to compute various evaluation metrics (such as precision recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = X_test_errors > threshold\n",
    "y_test_pred = []\n",
    "for is_anomaly in anomalies: \n",
    "    if is_anomaly: \n",
    "        y_test_pred.append(1)\n",
    "    else:\n",
    "        y_test_pred.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how well this statistically determined threshold perform on our test set. We are able to compute the precision, recall and accuracy of our model based on our test set labels (In many cases, you do not enjoy the luxury of having a labelled dataset like what we have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_test_pred, pos_label=1)\n",
    "recall = recall_score(y_test, y_test_pred, pos_label=1)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('threshold={:.2f}, precision={}, recall={}, acc={}'.format(threshold, precision, recall, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYeZt7CqVMyb"
   },
   "source": [
    "Let's just plot the data points. The normal data is marked green and the abnormal data is marked as red. The current threshold is shown as blue horizontal line. As there are close to 350,000 data points, it will be difficult to visualize on the graph. Let's us just plot first 1000 data points to see how the errors of each data point are distributed and how are they compared to the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7AzjBWTVMyb",
    "outputId": "050058d7-7bcf-48ab-c9bd-19e2bbdb8285"
   },
   "outputs": [],
   "source": [
    "draw_anomaly(y_test[:1000], X_test_errors[:1000], threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that very few 'normal' data points lie above the threshold set. Those that are above are mostly anomalous data points. We thus have very few false positives. This shows that we have very **high precision**.  On the other hand, we find that many 'anomalous' data points actually found the threshold, and thus not flagged as 'anomaly'. We thus have quite a lot of false negatives. This shows that we have very **low recall**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AipepLJEVMyd"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "How do you improve the recall (sensitivity)? If you improve the recall, what happens to the precision? \n",
    "Try out your idea by modifying the codes. \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "<br/>\n",
    "    \n",
    "You can change the threshold to a lower value to improve the recall (less false negatives).\n",
    "However, it will also decrease the precision (more false positives). \n",
    "There is trade-off between recall and precision. \n",
    "    \n",
    "Change threshold to say some lower value (e.g. 5) and re-run the cells.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises (Optional)**\n",
    "\n",
    "- Try your trained model on another test dataset (e.g. Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv) and see how it performs.- You can try to change the Autoencoder's hidden layers to have more or less units and observe the training loss and validation loss\n",
    "- You can try to change the latent dimension to be larger or smaller and observe the training and validation loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wI3iJKinVMxi",
    "_RU_vNHdVMxl",
    "O-pC2aCJVMxy",
    "_FwThzk_VMx5",
    "ZUqBcvkrVMx-",
    "xvsHmm9LVMyK",
    "Eh9Z7HdCVMyQ"
   ],
   "include_colab_link": true,
   "name": "ids-autoencoder.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (tf2env)",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
