{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/mindef-ai/blob/main/day1-pm/video-anomaly-v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nyp.edu.sg/content/dam/nyp/logo.png\" width=\"238\" height=\"70\"/>\n",
    "\n",
    "Welcome to the lab! Before we get started here are a few pointers on Jupyter notebooks.\n",
    "\n",
    "1. The notebook is composed of cells; cells can contain code which you can run, or they can hold text and/or images which are there for you to read.\n",
    "\n",
    "2. You can execute code cells by clicking the ```Run``` icon in the menu, or via the following keyboard shortcuts ```Shift-Enter``` (run and advance) or ```Ctrl-Enter``` (run and stay in the current cell).\n",
    "\n",
    "3. To interrupt cell execution, click the ```Stop``` button on the toolbar or navigate to the ```Kernel``` menu, and select ```Interrupt ```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Anomaly Detection \n",
    "                                                             \n",
    "<center>\n",
    "    <img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/video-anomaly.png\" height=\"400\" width=\"400\" style=\"vertical-align:middle;margin:10px 20px\"/><center>\n",
    "                \n",
    "\n",
    "In this lab, we will build an anomaly detector that can help to detect unusual activities in video frames. We will make use of two sets of videos: one set of videos contains only normal pedestrian traffic, and another set of videos contains anomalous activities, such as someone riding a bicycle or a car moving through the scene. We will train an autoencoder model to learn what is normal pedestrian traffic and then use it to detect unusual activities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "We begin by importing the libraries that we need, mainly the tensorflow (which is the framework that we use to build the autoencoder neural network) and some utility libraries that help us draw and display the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from utils import *\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as ipyImage\n",
    "from dataset_util import prepare_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this is to fix the libtiff bugs on Windows\n",
    "# !conda install libtiff=4.1.0=h885aae3_4 -c conda-forge -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "**UCSD Anomaly Detection Dataset**\n",
    "\n",
    "The UCSD Anomaly Detection Dataset is a set of video frames from a stationary camera overlooking pedestrian walkways. The crowd density in the walkways was variable, ranging from sparse to very crowded. In the normal setting, the video contains only pedestrians. Abnormal events include bikers, skaters, small carts, and people walking across a walkway or in the grass that surrounds it. The data was split into 2 subsets, each corresponding to a different scene. The video footage recorded from each scene was split into various clips of around 200 frames:\n",
    "\n",
    "- *Peds1*: scenes of people walking towards and away from the camera, and some amount of perspective distortion. Contains 34 training video samples and 36 testing video samples.\n",
    "\n",
    "- *Peds2*: scenes with pedestrian movement parallel to the camera plane. Contains 16 training video samples and 12 testing video samples.\n",
    "\n",
    "In this lab, we will use the *Peds1* dataset. In the next lab, you will experiment with *Peds2* dataset. \n",
    "\n",
    "**Note:** The [original dataset](http://www.svcl.ucsd.edu/projects/anomaly/UCSD_Anomaly_Dataset.tar.gz) hosted by University of San Diego (UCSD) contains some corrupted TIF image frames, which causes errors when loaded by the python image libray. The dataset you will be using is the one we have cleaned up to exclude those corrupted images. So be aware if you intend to use the dataset directly downloaded from the UCSD website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Dataset\n",
    "\n",
    "Run the cells below to download the dataset. After the dataset is downloaded, it will be unzipped to the directory called video_dataset. You can see the `video_dataset` in the file browser in this Jupyter lab IDE. The `video_dataset` directory contains two sub-folders: *UCSDped1* and *UCSDped2*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset_dir = 'video_dataset'\n",
    "datafile_url  = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/UCSD_Anomaly_Dataset.v1p2.zip'\n",
    "download_data(base_dataset_dir, datafile_url, extract=True, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the UCSDped1 dataset here. The dataset is split into two subsets: one for training, and one for testing. \n",
    "The training data (consists of 24 video clips) are in Train subfolder. Each clip is in a separate subfolder 'Train001', 'Train002', etc, and each of these subfolders contains 200 image frames.\n",
    "\n",
    "In the code below, we are just setting up all the filepaths to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we use the UCSDped1 dataset.\n",
    "dataset = 'UCSDped1'\n",
    "\n",
    "# setup all the relative path\n",
    "root_path = os.path.join(base_dataset_dir, dataset)\n",
    "train_dir = os.path.join(root_path, 'Train')\n",
    "test_dir = os.path.join(root_path, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Train dataset\n",
    "\n",
    "Our training set contains only video scenes that are 'normal'. Let's look at a few samples. \n",
    "\n",
    "You can change the variable `train_sample_folder` to another folder e.g. Train010, Train200, etc. \n",
    "\n",
    "The variable `image_range = (1,9)` allows you to view images from 1 to 8 (the left-hand number in the bracket is excluded). Feel free to change the range to view other images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the following train_sample_folder to another folder to view other clips\n",
    "train_sample_folder = 'Train034' \n",
    "image_range = (1,9)  # this display image from 1 to 8\n",
    "image_folder = os.path.join(train_dir, train_sample_folder)\n",
    "display_images(image_folder, image_range=image_range, max_per_row=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize as 'Video'\n",
    "\n",
    "We will convert the image frames to a 'video' (actually as animated gif) for easier viewing. The video consists of 200 frames. From the left navigation panel, you will see that a gif file called `<train_sample_folder_name>.gif` has been created, e.g. `Train034.gif`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_filename = train_sample_folder + '.gif' \n",
    "create_gif(image_folder, gif_filename, img_type='tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will play the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gif_filename,'rb') as file:\n",
    "    display(ipyImage(file.read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Test dataset\n",
    "\n",
    "Let us visualize the video frames from the test dataset folder Test001, as an animated gif. You should be able to see some anomalous event (e.g. someone riding a bicycle) in the animated gif you create. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the test sample folder to folder of Test001 and set the image_folder accordingly\n",
    "test_sample_folder = 'Test001' \n",
    "image_folder = os.path.join(test_dir, test_sample_folder) \n",
    "\n",
    "create_gif(image_folder, gif_filename, img_type='tif')\n",
    "\n",
    "with open(gif_filename,'rb') as file:\n",
    "    display(ipyImage(file.read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training and Validation Dataset\n",
    "\n",
    "Now we create a Tensorflow dataset suitable for use in training the Autoencoder network later. In preparing the dataset, we resize the all the images to same height `(IMG_HEIGHT)` and width `(IMG_WIDTH)`. Typically we set the height same as width (square image) for training, even though the original image may not be square. For deep learning network, it does not matter whether the image is square or rectangle.\n",
    "\n",
    "We also split the data into training set (80%) and validation set (20%). We use the validation set to check if we are overfitting model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT=100\n",
    "IMG_WIDTH=100\n",
    "BATCH_SIZE=16\n",
    "\n",
    "train_fileset = os.path.join(train_dir, '*/*.tif')\n",
    "\n",
    "train_dataset, validation_dataset = prepare_dataset(train_fileset,\n",
    "                                img_height=IMG_HEIGHT, \n",
    "                                img_width=IMG_WIDTH, \n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                split=True,\n",
    "                                test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a total of 34 x 200 = 6800 images. \n",
    "# 80% allocated to train set = (0.8 * 6800)/16 = 340 batches\n",
    "# 20% allocated to validation set = (0.2 * 6800)/16 = 85\n",
    "\n",
    "print('Number of batches of train images = {}'.format(len(list(train_dataset))))\n",
    "print('Number of batches of validation images = {}'.format(len(list(validation_dataset))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Autoencoder Model\n",
    "\n",
    "\n",
    "![autoencoder](https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/autoencoder.png)\n",
    "\n",
    "\n",
    "We will first build the Encoder network. In the lecture, we learnt that autoencoder learns the latent representation of the data by having a bottleneck layer, so that it is forced to capture only the most important features that allows it to reconstruct the input. \n",
    "You can see that as we move deeper into the encoder, the number of neurons typically are getting smaller and is the smallest at the 'latent' layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoder part of the Audo-encoder model\n",
    "inputs = tf.keras.layers.Input(shape=(100,100,1))\n",
    "x = tf.keras.layers.Conv2D(32, kernel_size=5, activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n",
    "x = tf.keras.layers.Conv2D(32, kernel_size=5, activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "encoded = tf.keras.layers.Dense(2000)(x)\n",
    "encoder = tf.keras.Model(inputs=[inputs], outputs=[encoded])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the model's summary so that we can see the number of output (think of these as number of neurons) at each layer. As you will observe, our input layer has input shape of 100x100x1 (Note: the last number is the number of channels, and since we dealing with grayscale image, there is only 1 channel). \n",
    "\n",
    "After the 1st convolutional + maxpooling layer, the number of outputs becomes 48x48x32. This is further reduced after the 2nd convolutional + maxpooling layer, to 22x22x32, and further reduced to 2000  at the latent layer. This means the network is forced to learn to capture the most important 'latent' information in the training data into a mere 2000 neurons. The information captured in this latent layer will then be used by the decoder to reconstruct the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()\n",
    "#tf.keras.utils.plot_model(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the decoder part of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder part of the Audo-encoder model\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(2000))\n",
    "x = tf.keras.layers.Dense(22*22*32, activation='relu')(decoder_inputs)\n",
    "x = tf.keras.layers.Reshape(target_shape=(22,22,32))(x)\n",
    "x = tf.keras.layers.UpSampling2D(2, interpolation='nearest')(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(32, kernel_size=5, activation='relu')(x)\n",
    "x = tf.keras.layers.UpSampling2D(2, interpolation='nearest')(x)\n",
    "decoded = tf.keras.layers.Conv2DTranspose(1, kernel_size=5, activation='sigmoid')(x)\n",
    "decoder = tf.keras.Model(inputs=[decoder_inputs], outputs=[decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we print out the summary of the decoder part, so that we can see the output shape of each decoder layer.\n",
    "Opposite to the encoder, the number of neurons increases as we progress towards the output layer. We can see that from 2000 units in the latent layer, we use UpSampling and Transpose Convolution to increase the number of output neurons, until we get back the original size of the input image, i.e. 100x100x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()\n",
    "#tf.keras.utils.plot_model(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we stack the encoder and decoder to become the complete autoencoder network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = encoder(inputs)\n",
    "decoding = decoder(encoding)\n",
    "conv_ae = tf.keras.Model(inputs=[inputs], outputs=[decoding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we specify to use Mean Squared Error (MSE) as our loss function. Basically the network compute the square of difference between original image and reconstructed image and use this loss (or MSE) to adjust the weights to minimise the loss. The MSE is given by the equation below:\n",
    "\n",
    "$$\\frac{1}{m}\\sum_{i=1}^m (\\hat{y}_{i} - y_i)^2$$\n",
    "\n",
    "\n",
    "where $\\hat{y}$ is the predicted output and $y$ is the actual value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ae.compile(loss=tf.keras.losses.MeanSquaredError(), \n",
    "        optimizer=tf.keras.optimizers.Adam(lr=1e-4, decay=1e-4),\n",
    "        metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's the training begin!! This might take a while so *sit back, relax and wait!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    " \n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "history = conv_ae.fit(train_dataset, \n",
    "                  validation_data=test_dataset,\n",
    "                  epochs=num_epochs, \n",
    "                  callbacks=[tensorboard_cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training and validation loss to see how our network progress with the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are just setting up path to a sample image from train set and test set respectively. The train sample shows a 'normal' scene, while the test sample shows an 'anomalous' scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_image = os.path.join(train_dir, 'Train001/001.tif')\n",
    "sample_test_image = os.path.join(test_dir, 'Test024/140.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take a 'normal' image from the train set, and see how well the autoencoder reconstructs it. We will plot the original image on the left and the reconstructed image on the right. \n",
    "\n",
    "Here you can see that it can mostly reconstruct the original image (the left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(conv_ae, sample_train_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 'abnormal' image from the test set where a cart can be seen driving through the walkway.  \n",
    "\n",
    "Since the cart is something that the autoencoder has never seen before, it failed to reconstruct it properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(conv_ae, sample_test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's us look at how the reconstruction loss varies for each video frame. Here you can clearly see that the loss starting to spike after the cart has entered the scene and continue to rise until the cart disappears from the scene, when the loss drops suddenly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Testing Dataset\n",
    "\n",
    "Now we will create a test dataset that we can use to test the trained auto-encoder. \n",
    "\n",
    "**Exercise**:\n",
    "\n",
    "You can change the following `test_sample_folder` to the video folder you want to test. For now, let's just use the one from Test014 which you have visualized earlier in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "\n",
    "test_sample_folder = 'Test014'\n",
    "\n",
    "test_fileset = os.path.join(test_dir, test_sample_folder, \"*.tif\")\n",
    "\n",
    "test_dataset = prepare_dataset(test_fileset,\n",
    "                                img_height=IMG_HEIGHT, \n",
    "                                img_width=IMG_WIDTH, \n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=False)\n",
    "print(len(list(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruction loss over different video frames\n",
    "\n",
    "The following codes take all the video frames from the test folder and runs through the autoencoder and compute the reconstruction loss and show the reconstruction loss for each frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_losses_animation(conv_ae, test_dataset, \"losses.gif\")\n",
    "with open('losses.gif','rb') as file:\n",
    "    display(ipyImage(file.read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identification of anomalous object from the video frames\n",
    "\n",
    "The function `identify_anomaly()` will compute the differences of each pixel between original frame and the reconstructed frame (for a total of 200 fames), and by comparing the differences over a patch of 4x4 pixels, and if the difference is above certain threshold, it will mark that patch with red color to signify that there is an anomalous object detected within that patch. \n",
    "\n",
    "The 200 frames will be displayed as animated gif to better visualize the changes over time.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Try changing the threshold below to adjust the sensitivity of the certain pixels being classified as anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 4.0\n",
    "identify_anomaly(conv_ae, test_dataset, \"video.gif\", threshold)\n",
    "with open('video.gif','rb') as file:\n",
    "    display(ipyImage(file.read(), format='png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2env",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
