{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hz79Xs_YVMxC"
   },
   "source": [
    "<img src=\"https://www.nyp.edu.sg/content/dam/nyp/logo.png\" width=\"200\" height=\"150\"/>\n",
    "\n",
    "Welcome to the lab! Before we get started here are a few pointers on Jupyter notebooks.\n",
    "\n",
    "1. The notebook is composed of cells; cells can contain code which you can run, or they can hold text and/or images which are there for you to read.\n",
    "\n",
    "2. You can execute code cells by clicking the ```Run``` icon in the menu, or via the following keyboard shortcuts ```Shift-Enter``` (run and advance) or ```Ctrl-Enter``` (run and stay in the current cell).\n",
    "\n",
    "3. To interrupt cell execution, click the ```Stop``` button on the toolbar or navigate to the ```Kernel``` menu, and select ```Interrupt ```.\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_U--l1zVMxD"
   },
   "source": [
    "# Anomaly Detection for Network Traffic\n",
    "\n",
    "<div>           \n",
    "<center>\n",
    "    <img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/intrusion_alert.png\" width=\"300\" height=\"200\"/>\n",
    "</center>\n",
    "</div>\n",
    "\n",
    "In this lab, we will try to build an anomaly detector for network traffic, which can detect attack traffic such as Denial of Service (DoS), Heartbleed, etc. We will make use of two pre-processed network traffic files, which contain features extracted from the raw traffic. These features are extracted from TCP and UDP flows. One file contains only normal traffic, which we will use for training an Autoencoder neural network. The other file contains mixture of normal and attack traffic, which we will use for fine-tuning the threshold of anomaly score, and for final testing.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APMeGN5YVMxD"
   },
   "source": [
    "## Import libraries\n",
    "\n",
    "We begin by importing the libraries that we need, mainly *scikit-learn* (which contains some useful methods for scaling the data, and  for calculating various evaluation metrics, such as precision/recall scores), *tensorflow* (which is the framework that we use to build the autoencoder neural network) and *matplotlib* (which we use for data visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YPh5y0jVMxE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scikit-learn libraries used for evaluation metrics, scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# pandas libraries for manipulating dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# \n",
    "import numpy as np\n",
    "\n",
    "#import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Dropout, ActivityRegularization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Nadam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import mse\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "#from utils2 import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rf1eqRZDVMxG"
   },
   "source": [
    "## Getting the data\n",
    "\n",
    "We download the data that we need for this lab. The is a zip file which contain csv files for different days of network traffic data. After download, the files are unzipped to a directory called ```'ids_dataset'```. You should be able to see the directory at the left sidebar (file browser). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6nXWoCRzVMxH",
    "outputId": "ce783b55-4743-41dd-f9a7-80f851fbc842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset directory already exists, skip download\n"
     ]
    }
   ],
   "source": [
    "base_dataset_dir = 'ids_dataset'\n",
    "datafile_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/ids_dataset.zip'\n",
    "download_data(base_dataset_dir, datafile_url, extract=True, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6Dizj00VMxJ"
   },
   "source": [
    "For this lab exercise, we will use the following two files in the ```ids_dataset``` directory: \n",
    "\n",
    "1. Monday-WorkingHours.pcap_ISCX.csv - contains features extracted from normal traffic \n",
    "2. Wednesday-workingHours.pcap_ISCX.csv - contains a mixture of normal traffic and attack traffic such as DoS/DDoS, Heartbleed, slowloris, Goldeneye, etc.  \n",
    "\n",
    "We use the pandas library to read data from CSV files into panda dataframe (dataframe is a 2D data structure similar to database table that contains columns and rows. Row represents individual data sample, and columns represent the different features of the data sample) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2KpNTd5VMxK"
   },
   "outputs": [],
   "source": [
    "normal_traffic = os.path.join(base_dataset_dir, 'Monday-WorkingHours.pcap_ISCX.csv')\n",
    "mixed_traffic = os.path.join(base_dataset_dir, 'Wednesday-workingHours.pcap_ISCX.csv')\n",
    "\n",
    "# Read the normal (benign) traffic data into pandas dataframe called df_normal\n",
    "df_normal = pd.read_csv(normal_traffic)\n",
    "\n",
    "# Read the mixed (benign + malicious) traffic data into pandas dataframe called df_mixed\n",
    "df_mixed = pd.read_csv(mixed_traffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjKrrj3kVMxM"
   },
   "source": [
    "## Data Exploration \n",
    "\n",
    "Let us explore the data a bit more. We use panda dataframe ```info()``` method to get more information about the features (columns), number of rows with non-null values for each column, and the data type of each column (feature). As we we will see from the display below, that we have 78 features (columns 0-77) and the last column (column 78) is the label of the traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SnClZS1oVMxM",
    "outputId": "45b4db9b-0d28-4e29-ddfe-0f5c881931f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 529918 entries, 0 to 529917\n",
      "Data columns (total 79 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0    Destination Port             529918 non-null  int64  \n",
      " 1    Flow Duration                529918 non-null  int64  \n",
      " 2    Total Fwd Packets            529918 non-null  int64  \n",
      " 3    Total Backward Packets       529918 non-null  int64  \n",
      " 4   Total Length of Fwd Packets   529918 non-null  int64  \n",
      " 5    Total Length of Bwd Packets  529918 non-null  int64  \n",
      " 6    Fwd Packet Length Max        529918 non-null  int64  \n",
      " 7    Fwd Packet Length Min        529918 non-null  int64  \n",
      " 8    Fwd Packet Length Mean       529918 non-null  float64\n",
      " 9    Fwd Packet Length Std        529918 non-null  float64\n",
      " 10  Bwd Packet Length Max         529918 non-null  int64  \n",
      " 11   Bwd Packet Length Min        529918 non-null  int64  \n",
      " 12   Bwd Packet Length Mean       529918 non-null  float64\n",
      " 13   Bwd Packet Length Std        529918 non-null  float64\n",
      " 14  Flow Bytes/s                  529854 non-null  float64\n",
      " 15   Flow Packets/s               529918 non-null  float64\n",
      " 16   Flow IAT Mean                529918 non-null  float64\n",
      " 17   Flow IAT Std                 529918 non-null  float64\n",
      " 18   Flow IAT Max                 529918 non-null  int64  \n",
      " 19   Flow IAT Min                 529918 non-null  int64  \n",
      " 20  Fwd IAT Total                 529918 non-null  int64  \n",
      " 21   Fwd IAT Mean                 529918 non-null  float64\n",
      " 22   Fwd IAT Std                  529918 non-null  float64\n",
      " 23   Fwd IAT Max                  529918 non-null  int64  \n",
      " 24   Fwd IAT Min                  529918 non-null  int64  \n",
      " 25  Bwd IAT Total                 529918 non-null  int64  \n",
      " 26   Bwd IAT Mean                 529918 non-null  float64\n",
      " 27   Bwd IAT Std                  529918 non-null  float64\n",
      " 28   Bwd IAT Max                  529918 non-null  int64  \n",
      " 29   Bwd IAT Min                  529918 non-null  int64  \n",
      " 30  Fwd PSH Flags                 529918 non-null  int64  \n",
      " 31   Bwd PSH Flags                529918 non-null  int64  \n",
      " 32   Fwd URG Flags                529918 non-null  int64  \n",
      " 33   Bwd URG Flags                529918 non-null  int64  \n",
      " 34   Fwd Header Length            529918 non-null  int64  \n",
      " 35   Bwd Header Length            529918 non-null  int64  \n",
      " 36  Fwd Packets/s                 529918 non-null  float64\n",
      " 37   Bwd Packets/s                529918 non-null  float64\n",
      " 38   Min Packet Length            529918 non-null  int64  \n",
      " 39   Max Packet Length            529918 non-null  int64  \n",
      " 40   Packet Length Mean           529918 non-null  float64\n",
      " 41   Packet Length Std            529918 non-null  float64\n",
      " 42   Packet Length Variance       529918 non-null  float64\n",
      " 43  FIN Flag Count                529918 non-null  int64  \n",
      " 44   SYN Flag Count               529918 non-null  int64  \n",
      " 45   RST Flag Count               529918 non-null  int64  \n",
      " 46   PSH Flag Count               529918 non-null  int64  \n",
      " 47   ACK Flag Count               529918 non-null  int64  \n",
      " 48   URG Flag Count               529918 non-null  int64  \n",
      " 49   CWE Flag Count               529918 non-null  int64  \n",
      " 50   ECE Flag Count               529918 non-null  int64  \n",
      " 51   Down/Up Ratio                529918 non-null  int64  \n",
      " 52   Average Packet Size          529918 non-null  float64\n",
      " 53   Avg Fwd Segment Size         529918 non-null  float64\n",
      " 54   Avg Bwd Segment Size         529918 non-null  float64\n",
      " 55   Fwd Header Length.1          529918 non-null  int64  \n",
      " 56  Fwd Avg Bytes/Bulk            529918 non-null  int64  \n",
      " 57   Fwd Avg Packets/Bulk         529918 non-null  int64  \n",
      " 58   Fwd Avg Bulk Rate            529918 non-null  int64  \n",
      " 59   Bwd Avg Bytes/Bulk           529918 non-null  int64  \n",
      " 60   Bwd Avg Packets/Bulk         529918 non-null  int64  \n",
      " 61  Bwd Avg Bulk Rate             529918 non-null  int64  \n",
      " 62  Subflow Fwd Packets           529918 non-null  int64  \n",
      " 63   Subflow Fwd Bytes            529918 non-null  int64  \n",
      " 64   Subflow Bwd Packets          529918 non-null  int64  \n",
      " 65   Subflow Bwd Bytes            529918 non-null  int64  \n",
      " 66  Init_Win_bytes_forward        529918 non-null  int64  \n",
      " 67   Init_Win_bytes_backward      529918 non-null  int64  \n",
      " 68   act_data_pkt_fwd             529918 non-null  int64  \n",
      " 69   min_seg_size_forward         529918 non-null  int64  \n",
      " 70  Active Mean                   529918 non-null  float64\n",
      " 71   Active Std                   529918 non-null  float64\n",
      " 72   Active Max                   529918 non-null  int64  \n",
      " 73   Active Min                   529918 non-null  int64  \n",
      " 74  Idle Mean                     529918 non-null  float64\n",
      " 75   Idle Std                     529918 non-null  float64\n",
      " 76   Idle Max                     529918 non-null  int64  \n",
      " 77   Idle Min                     529918 non-null  int64  \n",
      " 78   Label                        529918 non-null  object \n",
      "dtypes: float64(24), int64(54), object(1)\n",
      "memory usage: 319.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# display the information about the dataframe\n",
    "df_normal.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttp7kFxUVMxP"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Take a look athe feature column ```Flow Bytes/s``` from the display, what do you observe? \n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "We can see that there a few rows that have missing values for ```Flow Bytes/s```. There are 529854 rows which have non-null values for ```Flow Bytes/s```, compared to others that have 529918 non-null rows. That means there are a total of 64 empty values (or in Machine learning jargon, we call it NaN or Not-a-Number).\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's us take a look at the values of the target label to see what are different values for the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "20LOcfb6VMxQ",
    "outputId": "987a451f-d25b-429d-b76f-f1f847cda837"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BENIGN    529918\n",
       "Name:  Label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normal[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCaLlCjaVMxS"
   },
   "source": [
    "Since this dataset only contains 'normal' traffic, the label should only have one value **BENIGN**. Let us also find out what type of labels we have for mixed_traffic.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "In the following cell, modify the codes to display the types of labels we have for mixed traffic data. What are different types of attack traffic? \n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "    \n",
    "```\n",
    "df_mixed[' Label'].value_counts()\n",
    "```\n",
    "    \n",
    "<br/>\n",
    "    \n",
    "From the display, we see that most of the attack traffic are ``DoS Hulk``, the rest being DoS ``GoldenEye``, ``DoS slowloris``, ``DoS Slowhttptest``, and ``Heartbleed``.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "LLC-EUMDVMxS",
    "outputId": "02a284cd-9ad5-4eff-dc97-943da3303260"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BENIGN              440031\n",
       "DoS Hulk            231073\n",
       "DoS GoldenEye        10293\n",
       "DoS slowloris         5796\n",
       "DoS Slowhttptest      5499\n",
       "Heartbleed              11\n",
       "Name:  Label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: MODIFY THE LINE BELOW ##\n",
    "\n",
    "df_mixed[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the display, we see that most of the attack traffic are DoS Hulk, the rest being DoS GoldenEye, DoS slowloris, DoS Slowhttptest, and Heartbleed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7CzXRZTVMxV"
   },
   "source": [
    "Let us also take a look at the statistics of the different feature columns to have some ideas of the numeric values we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qiGvRTvNVMxV",
    "outputId": "e8c51baf-90a7-4fbd-a08c-d8fc7d75d4b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Destination Port</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.064437e+04</td>\n",
       "      <td>2.139021e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>443.000000</td>\n",
       "      <td>6.553500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow Duration</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.038927e+07</td>\n",
       "      <td>2.875195e+07</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>31303.000000</td>\n",
       "      <td>355744.750000</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.039032e+01</td>\n",
       "      <td>8.924128e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.197590e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.151710e+01</td>\n",
       "      <td>1.173319e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.919220e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Length of Fwd Packets</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>5.324195e+02</td>\n",
       "      <td>6.228642e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>1.323378e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.789841e+04</td>\n",
       "      <td>2.675470e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>6.554530e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.908972e+02</td>\n",
       "      <td>4.488338e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>2.336000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>2.027728e+01</td>\n",
       "      <td>3.627579e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>2.293000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>5.074408e+01</td>\n",
       "      <td>9.196471e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.638923e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>5.745227e+01</td>\n",
       "      <td>1.465181e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.162951</td>\n",
       "      <td>7.125597e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packet Length Max</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>4.060733e+02</td>\n",
       "      <td>7.850381e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>1.314000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packet Length Min</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>4.992438e+01</td>\n",
       "      <td>7.480431e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>2.146000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packet Length Mean</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.647693e+02</td>\n",
       "      <td>2.769877e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>2.976322e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packet Length Std</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.313097e+02</td>\n",
       "      <td>2.758482e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.677670</td>\n",
       "      <td>2.728559e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow Bytes/s</th>\n",
       "      <td>529854.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.200000e+07</td>\n",
       "      <td>170.549752</td>\n",
       "      <td>5567.659587</td>\n",
       "      <td>240000.000000</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow Packets/s</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.000000e+06</td>\n",
       "      <td>19.312148</td>\n",
       "      <td>115.245071</td>\n",
       "      <td>21164.021164</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow IAT Mean</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.069388e+06</td>\n",
       "      <td>5.113239e+06</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>10783.900000</td>\n",
       "      <td>86985.000000</td>\n",
       "      <td>1.197482e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow IAT Std</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.540446e+06</td>\n",
       "      <td>6.057313e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.820907</td>\n",
       "      <td>54031.902322</td>\n",
       "      <td>8.480026e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow IAT Max</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>4.323909e+06</td>\n",
       "      <td>1.402750e+07</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>30899.000000</td>\n",
       "      <td>204816.250000</td>\n",
       "      <td>1.199997e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow IAT Min</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>2.936914e+05</td>\n",
       "      <td>3.883156e+06</td>\n",
       "      <td>-1.400000e+01</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>1.197482e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd IAT Total</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.009897e+07</td>\n",
       "      <td>2.858167e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>261537.250000</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd IAT Mean</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>2.115864e+06</td>\n",
       "      <td>1.024547e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40828.979167</td>\n",
       "      <td>1.199870e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd IAT Std</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.045617e+06</td>\n",
       "      <td>3.911767e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21064.976257</td>\n",
       "      <td>8.460293e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd IAT Max</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>4.174560e+06</td>\n",
       "      <td>1.397196e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>105485.750000</td>\n",
       "      <td>1.199999e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd IAT Min</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.466088e+06</td>\n",
       "      <td>1.002922e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.199870e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd IAT Total</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>9.134728e+06</td>\n",
       "      <td>2.768821e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>63969.750000</td>\n",
       "      <td>1.199996e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd IAT Mean</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.817322e+06</td>\n",
       "      <td>9.714902e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12521.062500</td>\n",
       "      <td>1.199741e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd IAT Std</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>8.093950e+05</td>\n",
       "      <td>3.598336e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>396.458660</td>\n",
       "      <td>8.441801e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd IAT Max</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>3.372632e+06</td>\n",
       "      <td>1.290213e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>36706.000000</td>\n",
       "      <td>1.199741e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd IAT Min</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.245437e+06</td>\n",
       "      <td>9.456427e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.199741e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd PSH Flags</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>6.413823e-02</td>\n",
       "      <td>2.449992e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd PSH Flags</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd URG Flags</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd URG Flags</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Header Length</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>-1.320351e+04</td>\n",
       "      <td>3.609256e+06</td>\n",
       "      <td>-1.929350e+09</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>4.644908e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Header Length</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>-3.055556e+03</td>\n",
       "      <td>6.209914e+05</td>\n",
       "      <td>-1.677705e+08</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>5.838440e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packets/s</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>6.023015e+04</td>\n",
       "      <td>2.289737e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.710836</td>\n",
       "      <td>63.202136</td>\n",
       "      <td>10989.010989</td>\n",
       "      <td>3.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packets/s</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>6.009861e+03</td>\n",
       "      <td>3.539694e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.249794</td>\n",
       "      <td>28.310033</td>\n",
       "      <td>4807.692308</td>\n",
       "      <td>2.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min Packet Length</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.965280e+01</td>\n",
       "      <td>2.499980e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.359000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max Packet Length</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>4.473085e+02</td>\n",
       "      <td>8.404360e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>2.336000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet Length Mean</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.026244e+02</td>\n",
       "      <td>1.602147e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>99.200000</td>\n",
       "      <td>2.456000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet Length Std</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.352019e+02</td>\n",
       "      <td>2.358366e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.190890</td>\n",
       "      <td>28.481573</td>\n",
       "      <td>112.005952</td>\n",
       "      <td>4.414547e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet Length Variance</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>7.389834e+04</td>\n",
       "      <td>2.099224e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>811.200000</td>\n",
       "      <td>12545.333333</td>\n",
       "      <td>1.948823e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIN Flag Count</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>2.294695e-02</td>\n",
       "      <td>1.497345e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYN Flag Count</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>6.413823e-02</td>\n",
       "      <td>2.449992e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RST Flag Count</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.358701e-04</td>\n",
       "      <td>1.165555e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSH Flag Count</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>2.486271e-01</td>\n",
       "      <td>4.322176e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACK Flag Count</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>3.065003e-01</td>\n",
       "      <td>4.610404e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URG Flag Count</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.199921e-01</td>\n",
       "      <td>3.249527e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE Flag Count</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECE Flag Count</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.377572e-04</td>\n",
       "      <td>1.173620e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Down/Up Ratio</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>6.389026e-01</td>\n",
       "      <td>5.526892e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.080000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Packet Size</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.166137e+02</td>\n",
       "      <td>1.661667e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>75.750000</td>\n",
       "      <td>126.500000</td>\n",
       "      <td>3.684000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg Fwd Segment Size</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>5.074408e+01</td>\n",
       "      <td>9.196471e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.638923e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg Bwd Segment Size</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.647693e+02</td>\n",
       "      <td>2.769877e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>2.976322e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Header Length.1</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>-1.320351e+04</td>\n",
       "      <td>3.609256e+06</td>\n",
       "      <td>-1.929350e+09</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>4.644908e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Avg Bytes/Bulk</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Avg Packets/Bulk</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Avg Bulk Rate</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Avg Bytes/Bulk</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Avg Packets/Bulk</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Avg Bulk Rate</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Fwd Packets</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.039032e+01</td>\n",
       "      <td>8.924128e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.197590e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Fwd Bytes</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>5.324195e+02</td>\n",
       "      <td>6.228642e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>1.323378e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Bwd Packets</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.151710e+01</td>\n",
       "      <td>1.173319e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.919220e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Bwd Bytes</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.789841e+04</td>\n",
       "      <td>2.675470e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>6.554530e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Init_Win_bytes_forward</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>9.079724e+03</td>\n",
       "      <td>1.817531e+04</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>8192.000000</td>\n",
       "      <td>6.553500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Init_Win_bytes_backward</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>3.055876e+03</td>\n",
       "      <td>1.013709e+04</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>6.553500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>7.412509e+00</td>\n",
       "      <td>8.517624e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.135570e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>-3.614576e+03</td>\n",
       "      <td>5.526328e+05</td>\n",
       "      <td>-8.388531e+07</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.260000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Mean</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>6.843482e+04</td>\n",
       "      <td>5.872322e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.016597e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Std</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>4.321930e+04</td>\n",
       "      <td>3.971455e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.434950e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Max</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>1.453907e+05</td>\n",
       "      <td>1.028606e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.016597e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Min</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>4.380369e+04</td>\n",
       "      <td>4.993677e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.016597e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle Mean</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>3.463918e+06</td>\n",
       "      <td>1.297057e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.199997e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle Std</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>2.024408e+05</td>\n",
       "      <td>2.170149e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.514502e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle Max</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>3.620657e+06</td>\n",
       "      <td>1.340649e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.199997e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle Min</th>\n",
       "      <td>529918.0</td>\n",
       "      <td>3.274066e+06</td>\n",
       "      <td>1.273216e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.199997e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 count          mean           std  \\\n",
       " Destination Port             529918.0  1.064437e+04  2.139021e+04   \n",
       " Flow Duration                529918.0  1.038927e+07  2.875195e+07   \n",
       " Total Fwd Packets            529918.0  1.039032e+01  8.924128e+02   \n",
       " Total Backward Packets       529918.0  1.151710e+01  1.173319e+03   \n",
       "Total Length of Fwd Packets   529918.0  5.324195e+02  6.228642e+03   \n",
       " Total Length of Bwd Packets  529918.0  1.789841e+04  2.675470e+06   \n",
       " Fwd Packet Length Max        529918.0  1.908972e+02  4.488338e+02   \n",
       " Fwd Packet Length Min        529918.0  2.027728e+01  3.627579e+01   \n",
       " Fwd Packet Length Mean       529918.0  5.074408e+01  9.196471e+01   \n",
       " Fwd Packet Length Std        529918.0  5.745227e+01  1.465181e+02   \n",
       "Bwd Packet Length Max         529918.0  4.060733e+02  7.850381e+02   \n",
       " Bwd Packet Length Min        529918.0  4.992438e+01  7.480431e+01   \n",
       " Bwd Packet Length Mean       529918.0  1.647693e+02  2.769877e+02   \n",
       " Bwd Packet Length Std        529918.0  1.313097e+02  2.758482e+02   \n",
       "Flow Bytes/s                  529854.0           inf           NaN   \n",
       " Flow Packets/s               529918.0           inf           NaN   \n",
       " Flow IAT Mean                529918.0  1.069388e+06  5.113239e+06   \n",
       " Flow IAT Std                 529918.0  1.540446e+06  6.057313e+06   \n",
       " Flow IAT Max                 529918.0  4.323909e+06  1.402750e+07   \n",
       " Flow IAT Min                 529918.0  2.936914e+05  3.883156e+06   \n",
       "Fwd IAT Total                 529918.0  1.009897e+07  2.858167e+07   \n",
       " Fwd IAT Mean                 529918.0  2.115864e+06  1.024547e+07   \n",
       " Fwd IAT Std                  529918.0  1.045617e+06  3.911767e+06   \n",
       " Fwd IAT Max                  529918.0  4.174560e+06  1.397196e+07   \n",
       " Fwd IAT Min                  529918.0  1.466088e+06  1.002922e+07   \n",
       "Bwd IAT Total                 529918.0  9.134728e+06  2.768821e+07   \n",
       " Bwd IAT Mean                 529918.0  1.817322e+06  9.714902e+06   \n",
       " Bwd IAT Std                  529918.0  8.093950e+05  3.598336e+06   \n",
       " Bwd IAT Max                  529918.0  3.372632e+06  1.290213e+07   \n",
       " Bwd IAT Min                  529918.0  1.245437e+06  9.456427e+06   \n",
       "Fwd PSH Flags                 529918.0  6.413823e-02  2.449992e-01   \n",
       " Bwd PSH Flags                529918.0  0.000000e+00  0.000000e+00   \n",
       " Fwd URG Flags                529918.0  0.000000e+00  0.000000e+00   \n",
       " Bwd URG Flags                529918.0  0.000000e+00  0.000000e+00   \n",
       " Fwd Header Length            529918.0 -1.320351e+04  3.609256e+06   \n",
       " Bwd Header Length            529918.0 -3.055556e+03  6.209914e+05   \n",
       "Fwd Packets/s                 529918.0  6.023015e+04  2.289737e+05   \n",
       " Bwd Packets/s                529918.0  6.009861e+03  3.539694e+04   \n",
       " Min Packet Length            529918.0  1.965280e+01  2.499980e+01   \n",
       " Max Packet Length            529918.0  4.473085e+02  8.404360e+02   \n",
       " Packet Length Mean           529918.0  1.026244e+02  1.602147e+02   \n",
       " Packet Length Std            529918.0  1.352019e+02  2.358366e+02   \n",
       " Packet Length Variance       529918.0  7.389834e+04  2.099224e+05   \n",
       "FIN Flag Count                529918.0  2.294695e-02  1.497345e-01   \n",
       " SYN Flag Count               529918.0  6.413823e-02  2.449992e-01   \n",
       " RST Flag Count               529918.0  1.358701e-04  1.165555e-02   \n",
       " PSH Flag Count               529918.0  2.486271e-01  4.322176e-01   \n",
       " ACK Flag Count               529918.0  3.065003e-01  4.610404e-01   \n",
       " URG Flag Count               529918.0  1.199921e-01  3.249527e-01   \n",
       " CWE Flag Count               529918.0  0.000000e+00  0.000000e+00   \n",
       " ECE Flag Count               529918.0  1.377572e-04  1.173620e-02   \n",
       " Down/Up Ratio                529918.0  6.389026e-01  5.526892e-01   \n",
       " Average Packet Size          529918.0  1.166137e+02  1.661667e+02   \n",
       " Avg Fwd Segment Size         529918.0  5.074408e+01  9.196471e+01   \n",
       " Avg Bwd Segment Size         529918.0  1.647693e+02  2.769877e+02   \n",
       " Fwd Header Length.1          529918.0 -1.320351e+04  3.609256e+06   \n",
       "Fwd Avg Bytes/Bulk            529918.0  0.000000e+00  0.000000e+00   \n",
       " Fwd Avg Packets/Bulk         529918.0  0.000000e+00  0.000000e+00   \n",
       " Fwd Avg Bulk Rate            529918.0  0.000000e+00  0.000000e+00   \n",
       " Bwd Avg Bytes/Bulk           529918.0  0.000000e+00  0.000000e+00   \n",
       " Bwd Avg Packets/Bulk         529918.0  0.000000e+00  0.000000e+00   \n",
       "Bwd Avg Bulk Rate             529918.0  0.000000e+00  0.000000e+00   \n",
       "Subflow Fwd Packets           529918.0  1.039032e+01  8.924128e+02   \n",
       " Subflow Fwd Bytes            529918.0  5.324195e+02  6.228642e+03   \n",
       " Subflow Bwd Packets          529918.0  1.151710e+01  1.173319e+03   \n",
       " Subflow Bwd Bytes            529918.0  1.789841e+04  2.675470e+06   \n",
       "Init_Win_bytes_forward        529918.0  9.079724e+03  1.817531e+04   \n",
       " Init_Win_bytes_backward      529918.0  3.055876e+03  1.013709e+04   \n",
       " act_data_pkt_fwd             529918.0  7.412509e+00  8.517624e+02   \n",
       " min_seg_size_forward         529918.0 -3.614576e+03  5.526328e+05   \n",
       "Active Mean                   529918.0  6.843482e+04  5.872322e+05   \n",
       " Active Std                   529918.0  4.321930e+04  3.971455e+05   \n",
       " Active Max                   529918.0  1.453907e+05  1.028606e+06   \n",
       " Active Min                   529918.0  4.380369e+04  4.993677e+05   \n",
       "Idle Mean                     529918.0  3.463918e+06  1.297057e+07   \n",
       " Idle Std                     529918.0  2.024408e+05  2.170149e+06   \n",
       " Idle Max                     529918.0  3.620657e+06  1.340649e+07   \n",
       " Idle Min                     529918.0  3.274066e+06  1.273216e+07   \n",
       "\n",
       "                                       min         25%           50%  \\\n",
       " Destination Port             0.000000e+00   53.000000     80.000000   \n",
       " Flow Duration               -1.000000e+00  176.000000  31303.000000   \n",
       " Total Fwd Packets            1.000000e+00    2.000000      2.000000   \n",
       " Total Backward Packets       0.000000e+00    1.000000      2.000000   \n",
       "Total Length of Fwd Packets   0.000000e+00   18.000000     68.000000   \n",
       " Total Length of Bwd Packets  0.000000e+00    0.000000    144.000000   \n",
       " Fwd Packet Length Max        0.000000e+00    6.000000     40.000000   \n",
       " Fwd Packet Length Min        0.000000e+00    0.000000      6.000000   \n",
       " Fwd Packet Length Mean       0.000000e+00    6.000000     38.000000   \n",
       " Fwd Packet Length Std        0.000000e+00    0.000000      0.000000   \n",
       "Bwd Packet Length Max         0.000000e+00    0.000000     91.000000   \n",
       " Bwd Packet Length Min        0.000000e+00    0.000000      6.000000   \n",
       " Bwd Packet Length Mean       0.000000e+00    0.000000     81.000000   \n",
       " Bwd Packet Length Std        0.000000e+00    0.000000      0.000000   \n",
       "Flow Bytes/s                 -1.200000e+07  170.549752   5567.659587   \n",
       " Flow Packets/s              -2.000000e+06   19.312148    115.245071   \n",
       " Flow IAT Mean               -1.000000e+00   68.000000  10783.900000   \n",
       " Flow IAT Std                 0.000000e+00    0.000000    121.820907   \n",
       " Flow IAT Max                -1.000000e+00  150.000000  30899.000000   \n",
       " Flow IAT Min                -1.400000e+01    3.000000      4.000000   \n",
       "Fwd IAT Total                 0.000000e+00    1.000000      4.000000   \n",
       " Fwd IAT Mean                 0.000000e+00    1.000000      4.000000   \n",
       " Fwd IAT Std                  0.000000e+00    0.000000      0.000000   \n",
       " Fwd IAT Max                  0.000000e+00    1.000000      4.000000   \n",
       " Fwd IAT Min                  0.000000e+00    1.000000      3.000000   \n",
       "Bwd IAT Total                 0.000000e+00    0.000000      3.000000   \n",
       " Bwd IAT Mean                 0.000000e+00    0.000000      3.000000   \n",
       " Bwd IAT Std                  0.000000e+00    0.000000      0.000000   \n",
       " Bwd IAT Max                  0.000000e+00    0.000000      3.000000   \n",
       " Bwd IAT Min                  0.000000e+00    0.000000      1.000000   \n",
       "Fwd PSH Flags                 0.000000e+00    0.000000      0.000000   \n",
       " Bwd PSH Flags                0.000000e+00    0.000000      0.000000   \n",
       " Fwd URG Flags                0.000000e+00    0.000000      0.000000   \n",
       " Bwd URG Flags                0.000000e+00    0.000000      0.000000   \n",
       " Fwd Header Length           -1.929350e+09   40.000000     64.000000   \n",
       " Bwd Header Length           -1.677705e+08   20.000000     40.000000   \n",
       "Fwd Packets/s                 0.000000e+00    9.710836     63.202136   \n",
       " Bwd Packets/s                0.000000e+00    0.249794     28.310033   \n",
       " Min Packet Length            0.000000e+00    0.000000      6.000000   \n",
       " Max Packet Length            0.000000e+00    6.000000     94.000000   \n",
       " Packet Length Mean           0.000000e+00    6.000000     60.000000   \n",
       " Packet Length Std            0.000000e+00    2.190890     28.481573   \n",
       " Packet Length Variance       0.000000e+00    4.800000    811.200000   \n",
       "FIN Flag Count                0.000000e+00    0.000000      0.000000   \n",
       " SYN Flag Count               0.000000e+00    0.000000      0.000000   \n",
       " RST Flag Count               0.000000e+00    0.000000      0.000000   \n",
       " PSH Flag Count               0.000000e+00    0.000000      0.000000   \n",
       " ACK Flag Count               0.000000e+00    0.000000      0.000000   \n",
       " URG Flag Count               0.000000e+00    0.000000      0.000000   \n",
       " CWE Flag Count               0.000000e+00    0.000000      0.000000   \n",
       " ECE Flag Count               0.000000e+00    0.000000      0.000000   \n",
       " Down/Up Ratio                0.000000e+00    0.000000      1.000000   \n",
       " Average Packet Size          0.000000e+00    9.000000     75.750000   \n",
       " Avg Fwd Segment Size         0.000000e+00    6.000000     38.000000   \n",
       " Avg Bwd Segment Size         0.000000e+00    0.000000     81.000000   \n",
       " Fwd Header Length.1         -1.929350e+09   40.000000     64.000000   \n",
       "Fwd Avg Bytes/Bulk            0.000000e+00    0.000000      0.000000   \n",
       " Fwd Avg Packets/Bulk         0.000000e+00    0.000000      0.000000   \n",
       " Fwd Avg Bulk Rate            0.000000e+00    0.000000      0.000000   \n",
       " Bwd Avg Bytes/Bulk           0.000000e+00    0.000000      0.000000   \n",
       " Bwd Avg Packets/Bulk         0.000000e+00    0.000000      0.000000   \n",
       "Bwd Avg Bulk Rate             0.000000e+00    0.000000      0.000000   \n",
       "Subflow Fwd Packets           1.000000e+00    2.000000      2.000000   \n",
       " Subflow Fwd Bytes            0.000000e+00   18.000000     68.000000   \n",
       " Subflow Bwd Packets          0.000000e+00    1.000000      2.000000   \n",
       " Subflow Bwd Bytes            0.000000e+00    0.000000    144.000000   \n",
       "Init_Win_bytes_forward       -1.000000e+00   -1.000000    115.000000   \n",
       " Init_Win_bytes_backward     -1.000000e+00   -1.000000     -1.000000   \n",
       " act_data_pkt_fwd             0.000000e+00    0.000000      1.000000   \n",
       " min_seg_size_forward        -8.388531e+07   20.000000     32.000000   \n",
       "Active Mean                   0.000000e+00    0.000000      0.000000   \n",
       " Active Std                   0.000000e+00    0.000000      0.000000   \n",
       " Active Max                   0.000000e+00    0.000000      0.000000   \n",
       " Active Min                   0.000000e+00    0.000000      0.000000   \n",
       "Idle Mean                     0.000000e+00    0.000000      0.000000   \n",
       " Idle Std                     0.000000e+00    0.000000      0.000000   \n",
       " Idle Max                     0.000000e+00    0.000000      0.000000   \n",
       " Idle Min                     0.000000e+00    0.000000      0.000000   \n",
       "\n",
       "                                        75%           max  \n",
       " Destination Port                443.000000  6.553500e+04  \n",
       " Flow Duration                355744.750000  1.200000e+08  \n",
       " Total Fwd Packets                 4.000000  2.197590e+05  \n",
       " Total Backward Packets            3.000000  2.919220e+05  \n",
       "Total Length of Fwd Packets      187.000000  1.323378e+06  \n",
       " Total Length of Bwd Packets     392.000000  6.554530e+08  \n",
       " Fwd Packet Length Max            83.000000  2.336000e+04  \n",
       " Fwd Packet Length Min            40.000000  2.293000e+03  \n",
       " Fwd Packet Length Mean           53.000000  4.638923e+03  \n",
       " Fwd Packet Length Std            26.162951  7.125597e+03  \n",
       "Bwd Packet Length Max            212.000000  1.314000e+04  \n",
       " Bwd Packet Length Min            94.000000  2.146000e+03  \n",
       " Bwd Packet Length Mean          164.000000  2.976322e+03  \n",
       " Bwd Packet Length Std            17.677670  2.728559e+03  \n",
       "Flow Bytes/s                  240000.000000           inf  \n",
       " Flow Packets/s                21164.021164           inf  \n",
       " Flow IAT Mean                 86985.000000  1.197482e+08  \n",
       " Flow IAT Std                  54031.902322  8.480026e+07  \n",
       " Flow IAT Max                 204816.250000  1.199997e+08  \n",
       " Flow IAT Min                     82.000000  1.197482e+08  \n",
       "Fwd IAT Total                 261537.250000  1.200000e+08  \n",
       " Fwd IAT Mean                  40828.979167  1.199870e+08  \n",
       " Fwd IAT Std                   21064.976257  8.460293e+07  \n",
       " Fwd IAT Max                  105485.750000  1.199999e+08  \n",
       " Fwd IAT Min                      49.000000  1.199870e+08  \n",
       "Bwd IAT Total                  63969.750000  1.199996e+08  \n",
       " Bwd IAT Mean                  12521.062500  1.199741e+08  \n",
       " Bwd IAT Std                     396.458660  8.441801e+07  \n",
       " Bwd IAT Max                   36706.000000  1.199741e+08  \n",
       " Bwd IAT Min                       4.000000  1.199741e+08  \n",
       "Fwd PSH Flags                      0.000000  1.000000e+00  \n",
       " Bwd PSH Flags                     0.000000  0.000000e+00  \n",
       " Fwd URG Flags                     0.000000  0.000000e+00  \n",
       " Bwd URG Flags                     0.000000  0.000000e+00  \n",
       " Fwd Header Length               108.000000  4.644908e+06  \n",
       " Bwd Header Length                80.000000  5.838440e+06  \n",
       "Fwd Packets/s                  10989.010989  3.000000e+06  \n",
       " Bwd Packets/s                  4807.692308  2.000000e+06  \n",
       " Min Packet Length                40.000000  1.359000e+03  \n",
       " Max Packet Length               250.000000  2.336000e+04  \n",
       " Packet Length Mean               99.200000  2.456000e+03  \n",
       " Packet Length Std               112.005952  4.414547e+03  \n",
       " Packet Length Variance        12545.333333  1.948823e+07  \n",
       "FIN Flag Count                     0.000000  1.000000e+00  \n",
       " SYN Flag Count                    0.000000  1.000000e+00  \n",
       " RST Flag Count                    0.000000  1.000000e+00  \n",
       " PSH Flag Count                    0.000000  1.000000e+00  \n",
       " ACK Flag Count                    1.000000  1.000000e+00  \n",
       " URG Flag Count                    0.000000  1.000000e+00  \n",
       " CWE Flag Count                    0.000000  0.000000e+00  \n",
       " ECE Flag Count                    0.000000  1.000000e+00  \n",
       " Down/Up Ratio                     1.000000  1.080000e+02  \n",
       " Average Packet Size             126.500000  3.684000e+03  \n",
       " Avg Fwd Segment Size             53.000000  4.638923e+03  \n",
       " Avg Bwd Segment Size            164.000000  2.976322e+03  \n",
       " Fwd Header Length.1             108.000000  4.644908e+06  \n",
       "Fwd Avg Bytes/Bulk                 0.000000  0.000000e+00  \n",
       " Fwd Avg Packets/Bulk              0.000000  0.000000e+00  \n",
       " Fwd Avg Bulk Rate                 0.000000  0.000000e+00  \n",
       " Bwd Avg Bytes/Bulk                0.000000  0.000000e+00  \n",
       " Bwd Avg Packets/Bulk              0.000000  0.000000e+00  \n",
       "Bwd Avg Bulk Rate                  0.000000  0.000000e+00  \n",
       "Subflow Fwd Packets                4.000000  2.197590e+05  \n",
       " Subflow Fwd Bytes               187.000000  1.323378e+06  \n",
       " Subflow Bwd Packets               3.000000  2.919220e+05  \n",
       " Subflow Bwd Bytes               392.000000  6.554530e+08  \n",
       "Init_Win_bytes_forward          8192.000000  6.553500e+04  \n",
       " Init_Win_bytes_backward         252.000000  6.553500e+04  \n",
       " act_data_pkt_fwd                  3.000000  2.135570e+05  \n",
       " min_seg_size_forward             32.000000  1.260000e+02  \n",
       "Active Mean                        0.000000  1.016597e+08  \n",
       " Active Std                        0.000000  6.434950e+07  \n",
       " Active Max                        0.000000  1.016597e+08  \n",
       " Active Min                        0.000000  1.016597e+08  \n",
       "Idle Mean                          0.000000  1.199997e+08  \n",
       " Idle Std                          0.000000  7.514502e+07  \n",
       " Idle Max                          0.000000  1.199997e+08  \n",
       " Idle Min                          0.000000  1.199997e+08  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## By default, jupyter notebook only display certain number of rows, we set it to 500 so we can see all the info\n",
    "pd.set_option('display.max_rows', 500)\n",
    "df_normal.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qaPOvjjyVMxX"
   },
   "source": [
    "From the display, we can see that there are a few feature columns that have ```infinite``` (indicated by```inf```) values (as caclulated by CICFlowMeter). \n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Can you identify which columns? \n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "    \n",
    "The columns ```Flow Bytes/s``` and ```Flow Packets/s``` have **inf** as max value, and thus also have **inf** as mean amd stardard deviation.  \n",
    "    \n",
    "<br/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XT4HF60RVMxX"
   },
   "source": [
    "**Exercise** \n",
    "\n",
    "Now modify the codes below to examine the values of `df_mixed`. Are there any abnormal values in `df_mixed`?\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "\n",
    "```\n",
    "df_mixed.describe().T\n",
    "```\n",
    "\n",
    "<br/>\n",
    "    \n",
    "The columns ```Flow Bytes/s``` and ```Flow Packets/s``` also contain inf as max value.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wnW1fFrJVMxY",
    "outputId": "922aeb4b-8c25-4a7b-8e8d-150ed097fff7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Destination Port</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>5.686869e+03</td>\n",
       "      <td>1.572742e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.430000e+02</td>\n",
       "      <td>6.548700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow Duration</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.800168e+07</td>\n",
       "      <td>4.276680e+07</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>61437.000000</td>\n",
       "      <td>8.302437e+07</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>9.556261e+00</td>\n",
       "      <td>7.471978e+02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>2.039430e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.021408e+01</td>\n",
       "      <td>9.842046e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2.723530e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Length of Fwd Packets</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>5.550930e+02</td>\n",
       "      <td>6.163663e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.650000e+02</td>\n",
       "      <td>1.224076e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.699644e+04</td>\n",
       "      <td>2.241175e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>1.159500e+04</td>\n",
       "      <td>6.270000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.335939e+02</td>\n",
       "      <td>6.037519e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>3.410000e+02</td>\n",
       "      <td>2.482000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.502218e+01</td>\n",
       "      <td>5.106883e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>2.065000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>6.055544e+01</td>\n",
       "      <td>1.576438e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>5.666667e+01</td>\n",
       "      <td>4.640758e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>8.289586e+01</td>\n",
       "      <td>2.261261e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.289169e+02</td>\n",
       "      <td>6.429191e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packet Length Max</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.661546e+03</td>\n",
       "      <td>2.613924e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>2.896000e+03</td>\n",
       "      <td>1.953000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packet Length Min</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>3.383292e+01</td>\n",
       "      <td>6.458650e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.800000e+01</td>\n",
       "      <td>1.983000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packet Length Mean</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>5.519408e+02</td>\n",
       "      <td>7.974496e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>9.176000e+02</td>\n",
       "      <td>4.370687e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packet Length Std</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>6.586363e+02</td>\n",
       "      <td>1.098043e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.276034e+02</td>\n",
       "      <td>6.715738e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow Bytes/s</th>\n",
       "      <td>691695.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-12000000.0</td>\n",
       "      <td>102.825135</td>\n",
       "      <td>519.002613</td>\n",
       "      <td>1.890071e+04</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow Packets/s</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2000000.0</td>\n",
       "      <td>0.285757</td>\n",
       "      <td>63.383406</td>\n",
       "      <td>1.834862e+04</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow IAT Mean</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.502809e+06</td>\n",
       "      <td>5.595945e+06</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>25095.666670</td>\n",
       "      <td>3.706981e+06</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow IAT Std</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>6.844318e+06</td>\n",
       "      <td>1.175401e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15099.925670</td>\n",
       "      <td>4.973849e+06</td>\n",
       "      <td>8.480000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow IAT Max</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.289307e+07</td>\n",
       "      <td>3.839395e+07</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>54510.000000</td>\n",
       "      <td>2.500000e+07</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow IAT Min</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.224592e+05</td>\n",
       "      <td>3.673248e+06</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.900000e+01</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd IAT Total</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.774476e+07</td>\n",
       "      <td>4.277094e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1002.000000</td>\n",
       "      <td>8.290000e+07</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd IAT Mean</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>5.069287e+06</td>\n",
       "      <td>1.103613e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>967.000000</td>\n",
       "      <td>6.997846e+06</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd IAT Std</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>9.016685e+06</td>\n",
       "      <td>1.600574e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.009074e+06</td>\n",
       "      <td>8.370000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd IAT Max</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.279837e+07</td>\n",
       "      <td>3.843629e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>996.000000</td>\n",
       "      <td>2.370000e+07</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd IAT Min</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.030778e+06</td>\n",
       "      <td>8.866760e+06</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.900000e+01</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd IAT Total</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.386407e+07</td>\n",
       "      <td>3.335293e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.575400e+05</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd IAT Mean</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.647068e+06</td>\n",
       "      <td>9.593760e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.609575e+04</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd IAT Std</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>3.522882e+06</td>\n",
       "      <td>1.073828e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.970159e+04</td>\n",
       "      <td>8.290000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd IAT Max</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>9.305284e+06</td>\n",
       "      <td>2.616633e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.382425e+05</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd IAT Min</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>9.268549e+05</td>\n",
       "      <td>8.107914e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.800000e+01</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd PSH Flags</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>4.220135e-02</td>\n",
       "      <td>2.010484e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd PSH Flags</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd URG Flags</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd URG Flags</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Header Length</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.420580e+02</td>\n",
       "      <td>1.564277e+04</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>4.290372e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Header Length</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.492460e+02</td>\n",
       "      <td>1.968957e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>5.447060e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packets/s</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>9.545305e+04</td>\n",
       "      <td>3.198607e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149272</td>\n",
       "      <td>31.728908</td>\n",
       "      <td>9.708738e+03</td>\n",
       "      <td>3.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packets/s</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>4.052544e+03</td>\n",
       "      <td>3.091927e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059417</td>\n",
       "      <td>0.360223</td>\n",
       "      <td>6.104633e+01</td>\n",
       "      <td>2.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min Packet Length</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.372377e+01</td>\n",
       "      <td>2.752172e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>1.448000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max Packet Length</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.725129e+03</td>\n",
       "      <td>2.634372e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>3.072000e+03</td>\n",
       "      <td>2.482000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet Length Mean</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.782466e+02</td>\n",
       "      <td>3.691305e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>70.454545</td>\n",
       "      <td>6.638333e+02</td>\n",
       "      <td>2.279754e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet Length Std</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>5.232656e+02</td>\n",
       "      <td>7.834425e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.323259</td>\n",
       "      <td>9.845126e+02</td>\n",
       "      <td>4.364023e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet Length Variance</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>8.876018e+05</td>\n",
       "      <td>1.748895e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1876.904762</td>\n",
       "      <td>9.692650e+05</td>\n",
       "      <td>1.900000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIN Flag Count</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>9.730144e-02</td>\n",
       "      <td>2.963680e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYN Flag Count</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>4.220135e-02</td>\n",
       "      <td>2.010484e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RST Flag Count</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>3.378071e-04</td>\n",
       "      <td>1.837644e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSH Flag Count</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.936934e-01</td>\n",
       "      <td>3.951917e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACK Flag Count</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>4.180493e-01</td>\n",
       "      <td>4.932387e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URG Flag Count</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>6.637188e-02</td>\n",
       "      <td>2.489312e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE Flag Count</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECE Flag Count</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>3.392507e-04</td>\n",
       "      <td>1.841565e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Down/Up Ratio</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>5.569573e-01</td>\n",
       "      <td>5.707336e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.300000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Packet Size</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>3.056649e+02</td>\n",
       "      <td>3.980459e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>6.960656e+02</td>\n",
       "      <td>2.612000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg Fwd Segment Size</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>6.055544e+01</td>\n",
       "      <td>1.576438e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>5.666667e+01</td>\n",
       "      <td>4.640758e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg Bwd Segment Size</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>5.519408e+02</td>\n",
       "      <td>7.974496e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>9.176000e+02</td>\n",
       "      <td>4.370687e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Header Length.1</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.420580e+02</td>\n",
       "      <td>1.564277e+04</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>4.290372e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Avg Bytes/Bulk</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Avg Packets/Bulk</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Avg Bulk Rate</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Avg Bytes/Bulk</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Avg Packets/Bulk</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Avg Bulk Rate</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Fwd Packets</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>9.556261e+00</td>\n",
       "      <td>7.471978e+02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>2.039430e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Fwd Bytes</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>5.550930e+02</td>\n",
       "      <td>6.163663e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.650000e+02</td>\n",
       "      <td>1.224076e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Bwd Packets</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.021408e+01</td>\n",
       "      <td>9.842046e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2.723530e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Bwd Bytes</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.699489e+04</td>\n",
       "      <td>2.240953e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>1.159500e+04</td>\n",
       "      <td>6.270464e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Init_Win_bytes_forward</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>5.299671e+03</td>\n",
       "      <td>1.186974e+04</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>4.855000e+02</td>\n",
       "      <td>6.553500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Init_Win_bytes_backward</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.476643e+03</td>\n",
       "      <td>7.315273e+03</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.350000e+02</td>\n",
       "      <td>6.553500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>6.121279e+00</td>\n",
       "      <td>7.151551e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.971240e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.676114e+01</td>\n",
       "      <td>6.322368e+00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>6.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Mean</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>9.224478e+04</td>\n",
       "      <td>7.007049e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.910000e+02</td>\n",
       "      <td>1.000000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Std</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>4.760852e+04</td>\n",
       "      <td>4.742081e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.420000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Max</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>1.627363e+05</td>\n",
       "      <td>1.094616e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.910000e+02</td>\n",
       "      <td>1.050000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Min</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>6.315186e+04</td>\n",
       "      <td>6.051023e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.880000e+02</td>\n",
       "      <td>1.000000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle Mean</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.211122e+07</td>\n",
       "      <td>3.812415e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.590000e+07</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle Std</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>4.743744e+05</td>\n",
       "      <td>4.488512e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.690000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle Max</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.252174e+07</td>\n",
       "      <td>3.848292e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.650000e+07</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle Min</th>\n",
       "      <td>692703.0</td>\n",
       "      <td>2.173373e+07</td>\n",
       "      <td>3.807725e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 count          mean           std  \\\n",
       " Destination Port             692703.0  5.686869e+03  1.572742e+04   \n",
       " Flow Duration                692703.0  2.800168e+07  4.276680e+07   \n",
       " Total Fwd Packets            692703.0  9.556261e+00  7.471978e+02   \n",
       " Total Backward Packets       692703.0  1.021408e+01  9.842046e+02   \n",
       "Total Length of Fwd Packets   692703.0  5.550930e+02  6.163663e+03   \n",
       " Total Length of Bwd Packets  692703.0  1.699644e+04  2.241175e+06   \n",
       " Fwd Packet Length Max        692703.0  2.335939e+02  6.037519e+02   \n",
       " Fwd Packet Length Min        692703.0  1.502218e+01  5.106883e+01   \n",
       " Fwd Packet Length Mean       692703.0  6.055544e+01  1.576438e+02   \n",
       " Fwd Packet Length Std        692703.0  8.289586e+01  2.261261e+02   \n",
       "Bwd Packet Length Max         692703.0  1.661546e+03  2.613924e+03   \n",
       " Bwd Packet Length Min        692703.0  3.383292e+01  6.458650e+01   \n",
       " Bwd Packet Length Mean       692703.0  5.519408e+02  7.974496e+02   \n",
       " Bwd Packet Length Std        692703.0  6.586363e+02  1.098043e+03   \n",
       "Flow Bytes/s                  691695.0           inf           NaN   \n",
       " Flow Packets/s               692703.0           inf           NaN   \n",
       " Flow IAT Mean                692703.0  2.502809e+06  5.595945e+06   \n",
       " Flow IAT Std                 692703.0  6.844318e+06  1.175401e+07   \n",
       " Flow IAT Max                 692703.0  2.289307e+07  3.839395e+07   \n",
       " Flow IAT Min                 692703.0  2.224592e+05  3.673248e+06   \n",
       "Fwd IAT Total                 692703.0  2.774476e+07  4.277094e+07   \n",
       " Fwd IAT Mean                 692703.0  5.069287e+06  1.103613e+07   \n",
       " Fwd IAT Std                  692703.0  9.016685e+06  1.600574e+07   \n",
       " Fwd IAT Max                  692703.0  2.279837e+07  3.843629e+07   \n",
       " Fwd IAT Min                  692703.0  1.030778e+06  8.866760e+06   \n",
       "Bwd IAT Total                 692703.0  1.386407e+07  3.335293e+07   \n",
       " Bwd IAT Mean                 692703.0  2.647068e+06  9.593760e+06   \n",
       " Bwd IAT Std                  692703.0  3.522882e+06  1.073828e+07   \n",
       " Bwd IAT Max                  692703.0  9.305284e+06  2.616633e+07   \n",
       " Bwd IAT Min                  692703.0  9.268549e+05  8.107914e+06   \n",
       "Fwd PSH Flags                 692703.0  4.220135e-02  2.010484e-01   \n",
       " Bwd PSH Flags                692703.0  0.000000e+00  0.000000e+00   \n",
       " Fwd URG Flags                692703.0  0.000000e+00  0.000000e+00   \n",
       " Bwd URG Flags                692703.0  0.000000e+00  0.000000e+00   \n",
       " Fwd Header Length            692703.0  2.420580e+02  1.564277e+04   \n",
       " Bwd Header Length            692703.0  2.492460e+02  1.968957e+04   \n",
       "Fwd Packets/s                 692703.0  9.545305e+04  3.198607e+05   \n",
       " Bwd Packets/s                692703.0  4.052544e+03  3.091927e+04   \n",
       " Min Packet Length            692703.0  1.372377e+01  2.752172e+01   \n",
       " Max Packet Length            692703.0  1.725129e+03  2.634372e+03   \n",
       " Packet Length Mean           692703.0  2.782466e+02  3.691305e+02   \n",
       " Packet Length Std            692703.0  5.232656e+02  7.834425e+02   \n",
       " Packet Length Variance       692703.0  8.876018e+05  1.748895e+06   \n",
       "FIN Flag Count                692703.0  9.730144e-02  2.963680e-01   \n",
       " SYN Flag Count               692703.0  4.220135e-02  2.010484e-01   \n",
       " RST Flag Count               692703.0  3.378071e-04  1.837644e-02   \n",
       " PSH Flag Count               692703.0  1.936934e-01  3.951917e-01   \n",
       " ACK Flag Count               692703.0  4.180493e-01  4.932387e-01   \n",
       " URG Flag Count               692703.0  6.637188e-02  2.489312e-01   \n",
       " CWE Flag Count               692703.0  0.000000e+00  0.000000e+00   \n",
       " ECE Flag Count               692703.0  3.392507e-04  1.841565e-02   \n",
       " Down/Up Ratio                692703.0  5.569573e-01  5.707336e-01   \n",
       " Average Packet Size          692703.0  3.056649e+02  3.980459e+02   \n",
       " Avg Fwd Segment Size         692703.0  6.055544e+01  1.576438e+02   \n",
       " Avg Bwd Segment Size         692703.0  5.519408e+02  7.974496e+02   \n",
       " Fwd Header Length.1          692703.0  2.420580e+02  1.564277e+04   \n",
       "Fwd Avg Bytes/Bulk            692703.0  0.000000e+00  0.000000e+00   \n",
       " Fwd Avg Packets/Bulk         692703.0  0.000000e+00  0.000000e+00   \n",
       " Fwd Avg Bulk Rate            692703.0  0.000000e+00  0.000000e+00   \n",
       " Bwd Avg Bytes/Bulk           692703.0  0.000000e+00  0.000000e+00   \n",
       " Bwd Avg Packets/Bulk         692703.0  0.000000e+00  0.000000e+00   \n",
       "Bwd Avg Bulk Rate             692703.0  0.000000e+00  0.000000e+00   \n",
       "Subflow Fwd Packets           692703.0  9.556261e+00  7.471978e+02   \n",
       " Subflow Fwd Bytes            692703.0  5.550930e+02  6.163663e+03   \n",
       " Subflow Bwd Packets          692703.0  1.021408e+01  9.842046e+02   \n",
       " Subflow Bwd Bytes            692703.0  1.699489e+04  2.240953e+06   \n",
       "Init_Win_bytes_forward        692703.0  5.299671e+03  1.186974e+04   \n",
       " Init_Win_bytes_backward      692703.0  1.476643e+03  7.315273e+03   \n",
       " act_data_pkt_fwd             692703.0  6.121279e+00  7.151551e+02   \n",
       " min_seg_size_forward         692703.0  2.676114e+01  6.322368e+00   \n",
       "Active Mean                   692703.0  9.224478e+04  7.007049e+05   \n",
       " Active Std                   692703.0  4.760852e+04  4.742081e+05   \n",
       " Active Max                   692703.0  1.627363e+05  1.094616e+06   \n",
       " Active Min                   692703.0  6.315186e+04  6.051023e+05   \n",
       "Idle Mean                     692703.0  2.211122e+07  3.812415e+07   \n",
       " Idle Std                     692703.0  4.743744e+05  4.488512e+06   \n",
       " Idle Max                     692703.0  2.252174e+07  3.848292e+07   \n",
       " Idle Min                     692703.0  2.173373e+07  3.807725e+07   \n",
       "\n",
       "                                     min         25%           50%  \\\n",
       " Destination Port                    0.0   53.000000     80.000000   \n",
       " Flow Duration                      -1.0  201.000000  61437.000000   \n",
       " Total Fwd Packets                   1.0    2.000000      2.000000   \n",
       " Total Backward Packets              0.0    1.000000      2.000000   \n",
       "Total Length of Fwd Packets          0.0   12.000000     82.000000   \n",
       " Total Length of Bwd Packets         0.0    0.000000    188.000000   \n",
       " Fwd Packet Length Max               0.0    6.000000     46.000000   \n",
       " Fwd Packet Length Min               0.0    0.000000      0.000000   \n",
       " Fwd Packet Length Mean              0.0    6.000000     41.000000   \n",
       " Fwd Packet Length Std               0.0    0.000000      0.000000   \n",
       "Bwd Packet Length Max                0.0    0.000000    113.000000   \n",
       " Bwd Packet Length Min               0.0    0.000000      0.000000   \n",
       " Bwd Packet Length Mean              0.0    0.000000    102.000000   \n",
       " Bwd Packet Length Std               0.0    0.000000      0.000000   \n",
       "Flow Bytes/s                 -12000000.0  102.825135    519.002613   \n",
       " Flow Packets/s               -2000000.0    0.285757     63.383406   \n",
       " Flow IAT Mean                      -1.0   79.000000  25095.666670   \n",
       " Flow IAT Std                        0.0    0.000000  15099.925670   \n",
       " Flow IAT Max                       -1.0  173.000000  54510.000000   \n",
       " Flow IAT Min                      -14.0    1.000000      3.000000   \n",
       "Fwd IAT Total                        0.0    3.000000   1002.000000   \n",
       " Fwd IAT Mean                        0.0    3.000000    967.000000   \n",
       " Fwd IAT Std                         0.0    0.000000      0.000000   \n",
       " Fwd IAT Max                         0.0    3.000000    996.000000   \n",
       " Fwd IAT Min                        -8.0    1.000000      3.000000   \n",
       "Bwd IAT Total                        0.0    0.000000      4.000000   \n",
       " Bwd IAT Mean                        0.0    0.000000      4.000000   \n",
       " Bwd IAT Std                         0.0    0.000000      0.000000   \n",
       " Bwd IAT Max                         0.0    0.000000      4.000000   \n",
       " Bwd IAT Min                         0.0    0.000000      3.000000   \n",
       "Fwd PSH Flags                        0.0    0.000000      0.000000   \n",
       " Bwd PSH Flags                       0.0    0.000000      0.000000   \n",
       " Fwd URG Flags                       0.0    0.000000      0.000000   \n",
       " Bwd URG Flags                       0.0    0.000000      0.000000   \n",
       " Fwd Header Length                 -11.0   40.000000     64.000000   \n",
       " Bwd Header Length                   0.0   20.000000     64.000000   \n",
       "Fwd Packets/s                        0.0    0.149272     31.728908   \n",
       " Bwd Packets/s                       0.0    0.059417      0.360223   \n",
       " Min Packet Length                   0.0    0.000000      0.000000   \n",
       " Max Packet Length                   0.0    6.000000    123.000000   \n",
       " Packet Length Mean                  0.0    6.000000     70.454545   \n",
       " Packet Length Std                   0.0    0.000000     43.323259   \n",
       " Packet Length Variance              0.0    0.000000   1876.904762   \n",
       "FIN Flag Count                       0.0    0.000000      0.000000   \n",
       " SYN Flag Count                      0.0    0.000000      0.000000   \n",
       " RST Flag Count                      0.0    0.000000      0.000000   \n",
       " PSH Flag Count                      0.0    0.000000      0.000000   \n",
       " ACK Flag Count                      0.0    0.000000      0.000000   \n",
       " URG Flag Count                      0.0    0.000000      0.000000   \n",
       " CWE Flag Count                      0.0    0.000000      0.000000   \n",
       " ECE Flag Count                      0.0    0.000000      0.000000   \n",
       " Down/Up Ratio                       0.0    0.000000      1.000000   \n",
       " Average Packet Size                 0.0    9.000000     91.000000   \n",
       " Avg Fwd Segment Size                0.0    6.000000     41.000000   \n",
       " Avg Bwd Segment Size                0.0    0.000000    102.000000   \n",
       " Fwd Header Length.1               -11.0   40.000000     64.000000   \n",
       "Fwd Avg Bytes/Bulk                   0.0    0.000000      0.000000   \n",
       " Fwd Avg Packets/Bulk                0.0    0.000000      0.000000   \n",
       " Fwd Avg Bulk Rate                   0.0    0.000000      0.000000   \n",
       " Bwd Avg Bytes/Bulk                  0.0    0.000000      0.000000   \n",
       " Bwd Avg Packets/Bulk                0.0    0.000000      0.000000   \n",
       "Bwd Avg Bulk Rate                    0.0    0.000000      0.000000   \n",
       "Subflow Fwd Packets                  1.0    2.000000      2.000000   \n",
       " Subflow Fwd Bytes                   0.0   12.000000     82.000000   \n",
       " Subflow Bwd Packets                 0.0    1.000000      2.000000   \n",
       " Subflow Bwd Bytes                   0.0    0.000000    188.000000   \n",
       "Init_Win_bytes_forward              -1.0   -1.000000    251.000000   \n",
       " Init_Win_bytes_backward            -1.0   -1.000000      0.000000   \n",
       " act_data_pkt_fwd                    0.0    0.000000      1.000000   \n",
       " min_seg_size_forward               -1.0   20.000000     32.000000   \n",
       "Active Mean                          0.0    0.000000      0.000000   \n",
       " Active Std                          0.0    0.000000      0.000000   \n",
       " Active Max                          0.0    0.000000      0.000000   \n",
       " Active Min                          0.0    0.000000      0.000000   \n",
       "Idle Mean                            0.0    0.000000      0.000000   \n",
       " Idle Std                            0.0    0.000000      0.000000   \n",
       " Idle Max                            0.0    0.000000      0.000000   \n",
       " Idle Min                            0.0    0.000000      0.000000   \n",
       "\n",
       "                                       75%           max  \n",
       " Destination Port             4.430000e+02  6.548700e+04  \n",
       " Flow Duration                8.302437e+07  1.200000e+08  \n",
       " Total Fwd Packets            7.000000e+00  2.039430e+05  \n",
       " Total Backward Packets       6.000000e+00  2.723530e+05  \n",
       "Total Length of Fwd Packets   3.650000e+02  1.224076e+06  \n",
       " Total Length of Bwd Packets  1.159500e+04  6.270000e+08  \n",
       " Fwd Packet Length Max        3.410000e+02  2.482000e+04  \n",
       " Fwd Packet Length Min        3.200000e+01  2.065000e+03  \n",
       " Fwd Packet Length Mean       5.666667e+01  4.640758e+03  \n",
       " Fwd Packet Length Std        1.289169e+02  6.429191e+03  \n",
       "Bwd Packet Length Max         2.896000e+03  1.953000e+04  \n",
       " Bwd Packet Length Min        5.800000e+01  1.983000e+03  \n",
       " Bwd Packet Length Mean       9.176000e+02  4.370687e+03  \n",
       " Bwd Packet Length Std        9.276034e+02  6.715738e+03  \n",
       "Flow Bytes/s                  1.890071e+04           inf  \n",
       " Flow Packets/s               1.834862e+04           inf  \n",
       " Flow IAT Mean                3.706981e+06  1.200000e+08  \n",
       " Flow IAT Std                 4.973849e+06  8.480000e+07  \n",
       " Flow IAT Max                 2.500000e+07  1.200000e+08  \n",
       " Flow IAT Min                 4.900000e+01  1.200000e+08  \n",
       "Fwd IAT Total                 8.290000e+07  1.200000e+08  \n",
       " Fwd IAT Mean                 6.997846e+06  1.200000e+08  \n",
       " Fwd IAT Std                  5.009074e+06  8.370000e+07  \n",
       " Fwd IAT Max                  2.370000e+07  1.200000e+08  \n",
       " Fwd IAT Min                  4.900000e+01  1.200000e+08  \n",
       "Bwd IAT Total                 1.575400e+05  1.200000e+08  \n",
       " Bwd IAT Mean                 3.609575e+04  1.200000e+08  \n",
       " Bwd IAT Std                  5.970159e+04  8.290000e+07  \n",
       " Bwd IAT Max                  1.382425e+05  1.200000e+08  \n",
       " Bwd IAT Min                  4.800000e+01  1.200000e+08  \n",
       "Fwd PSH Flags                 0.000000e+00  1.000000e+00  \n",
       " Bwd PSH Flags                0.000000e+00  0.000000e+00  \n",
       " Fwd URG Flags                0.000000e+00  0.000000e+00  \n",
       " Bwd URG Flags                0.000000e+00  0.000000e+00  \n",
       " Fwd Header Length            2.000000e+02  4.290372e+06  \n",
       " Bwd Header Length            2.000000e+02  5.447060e+06  \n",
       "Fwd Packets/s                 9.708738e+03  3.000000e+06  \n",
       " Bwd Packets/s                6.104633e+01  2.000000e+06  \n",
       " Min Packet Length            3.200000e+01  1.448000e+03  \n",
       " Max Packet Length            3.072000e+03  2.482000e+04  \n",
       " Packet Length Mean           6.638333e+02  2.279754e+03  \n",
       " Packet Length Std            9.845126e+02  4.364023e+03  \n",
       " Packet Length Variance       9.692650e+05  1.900000e+07  \n",
       "FIN Flag Count                0.000000e+00  1.000000e+00  \n",
       " SYN Flag Count               0.000000e+00  1.000000e+00  \n",
       " RST Flag Count               0.000000e+00  1.000000e+00  \n",
       " PSH Flag Count               0.000000e+00  1.000000e+00  \n",
       " ACK Flag Count               1.000000e+00  1.000000e+00  \n",
       " URG Flag Count               0.000000e+00  1.000000e+00  \n",
       " CWE Flag Count               0.000000e+00  0.000000e+00  \n",
       " ECE Flag Count               0.000000e+00  1.000000e+00  \n",
       " Down/Up Ratio                1.000000e+00  4.300000e+01  \n",
       " Average Packet Size          6.960656e+02  2.612000e+03  \n",
       " Avg Fwd Segment Size         5.666667e+01  4.640758e+03  \n",
       " Avg Bwd Segment Size         9.176000e+02  4.370687e+03  \n",
       " Fwd Header Length.1          2.000000e+02  4.290372e+06  \n",
       "Fwd Avg Bytes/Bulk            0.000000e+00  0.000000e+00  \n",
       " Fwd Avg Packets/Bulk         0.000000e+00  0.000000e+00  \n",
       " Fwd Avg Bulk Rate            0.000000e+00  0.000000e+00  \n",
       " Bwd Avg Bytes/Bulk           0.000000e+00  0.000000e+00  \n",
       " Bwd Avg Packets/Bulk         0.000000e+00  0.000000e+00  \n",
       "Bwd Avg Bulk Rate             0.000000e+00  0.000000e+00  \n",
       "Subflow Fwd Packets           7.000000e+00  2.039430e+05  \n",
       " Subflow Fwd Bytes            3.650000e+02  1.224076e+06  \n",
       " Subflow Bwd Packets          6.000000e+00  2.723530e+05  \n",
       " Subflow Bwd Bytes            1.159500e+04  6.270464e+08  \n",
       "Init_Win_bytes_forward        4.855000e+02  6.553500e+04  \n",
       " Init_Win_bytes_backward      2.350000e+02  6.553500e+04  \n",
       " act_data_pkt_fwd             2.000000e+00  1.971240e+05  \n",
       " min_seg_size_forward         3.200000e+01  6.000000e+01  \n",
       "Active Mean                   9.910000e+02  1.000000e+08  \n",
       " Active Std                   0.000000e+00  7.420000e+07  \n",
       " Active Max                   9.910000e+02  1.050000e+08  \n",
       " Active Min                   9.880000e+02  1.000000e+08  \n",
       "Idle Mean                     1.590000e+07  1.200000e+08  \n",
       " Idle Std                     0.000000e+00  7.690000e+07  \n",
       " Idle Max                     1.650000e+07  1.200000e+08  \n",
       " Idle Min                     1.000000e+07  1.200000e+08  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Modify the line below to find out more info about the numeric values of df_mixed\n",
    "df_mixed.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zq7AbayRVMxZ"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Data Cleaning \n",
    "\n",
    "We will need to clean up the data by removing the ```NaN``` values and the ```Inf``` values, as machine learning algorithm has problem dealing with these types of values.  Since we don't have too many rows of ```NaN``` and ```Inf``` values, the easiest way is to drop them. The code below first tells pandas to treat `Inf` values as `NaN` (```'mode.use_inf_as_na'```) and then drops all rows that contains ```NaN``` values (effectively dropping both rows with ```NaN``` and  ```Inf``` values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPKF_aiHVMxa"
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN and Inf values\n",
    "\n",
    "with pd.option_context('mode.use_inf_as_na', True):\n",
    "    df_normal = df_normal.dropna(subset=['Flow Bytes/s', ' Flow Packets/s'], how='all')\n",
    "    df_mixed = df_mixed.dropna(subset=['Flow Bytes/s', ' Flow Packets/s'], how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** \n",
    "\n",
    "Now add the code here to see if you still have `NaN` or `Inf` values.  (Hint: refers to the codes in previous cells to see how you can do this)\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "    \n",
    "You can use `info()`, as well as `describe()` methods to check the values of df_normal and df_mixed\n",
    "    \n",
    "<br/>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add code her to check if you still have NaN or Inf values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzZCnvGPVMxc"
   },
   "source": [
    "### Simplify the number of labels into binary labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DBVAGuNDVMxc"
   },
   "source": [
    "In this exercise, we are only interested in knowing if the traffic is 'normal' or 'abnormal', so we will group all the different attack traffic types into just one label, i.e. label '1' (think of 1 as positive case) while the benign traffic will be labelled as 0 (negative case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDDqnIOEVMxd"
   },
   "outputs": [],
   "source": [
    "df_normal[' Label'] = df_normal[' Label'].apply(lambda x: 1 if x != 'BENIGN' else 0)\n",
    "df_mixed[' Label'] = df_mixed[' Label'].apply(lambda x: 1 if x != 'BENIGN' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8R47iBhuVMxg"
   },
   "source": [
    "Now let's see our new traffic labels for ```df_mixed```. We should see that they now have only two values 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "htWlloBKVMxg",
    "outputId": "2548bb8e-9c74-499a-bdc2-1b13bf363e54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    439683\n",
       "1    251723\n",
       "Name:  Label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mixed[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wI3iJKinVMxi"
   },
   "source": [
    "### Create Training dataset and Validation dataset\n",
    "\n",
    "Here we will create the training set and the validation set. We will use the normal-only traffic (df_normal) as training and validation set. Since we are using unsupervised learning, we do not need the 'Label', so we will drop the label column during training.\n",
    "\n",
    "In the code below, we use 20% of data as validation set, and the remaining 80% for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXja-fuNVMxj"
   },
   "outputs": [],
   "source": [
    "# Drop the label column\n",
    "df_normal_nolabel = df_normal.drop([' Label'], axis=1)\n",
    "\n",
    "# Split the normal traffic data into Train and Validation set. \n",
    "X_train, X_val = train_test_split(df_normal_nolabel, test_size=0.2, shuffle=True, stratify=df_normal[' Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RU_vNHdVMxl"
   },
   "source": [
    "### Create Test dataset and 'Tuning' dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BN8kfSQUVMxl"
   },
   "source": [
    "Here we split the mixed traffic into 'tuning' set (df_tune) and test set (df_test).  We will use the 'tuning' set to help us fine tune our anomaly score threshold to achieve the desired precision/recall goal. We will then test our fine-tuned anomaly detector on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Complete the code below to split the mixed traffic dataset into 'tuning' dataset and test dataset (Hint: refer to the code above for splitting data into train and validation set)\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "    \n",
    "```\n",
    "df_tune, df_test = train_test_split(df_mixed, test_size=0.5, shuffle=True, stratify=df_mixed[' Label'])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHO07hX1VMxl"
   },
   "outputs": [],
   "source": [
    "# allocate 50% to fine-tuning set, and 50% to final test set\n",
    "df_tune, df_test = train_test_split(df_mixed, test_size=0.5, shuffle=True, stratify=df_mixed[' Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gh_qM68pVMxn"
   },
   "source": [
    "In the code below, we will separate out the label column and call it ``y_tune`` and ``y_test``. The remaining feature columns will be called``X_tune`` and ``X_test`` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgvdFVtZVMxn"
   },
   "source": [
    "***Note***: Although we do not need label in unsupervised learning, however, in this case, we will keep the labels and use them to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-dyCsj8VMxn"
   },
   "outputs": [],
   "source": [
    "y_tune = df_tune[' Label']\n",
    "X_tune = df_tune.drop(' Label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcevTAncVMxw"
   },
   "outputs": [],
   "source": [
    "y_test = df_test[' Label']\n",
    "X_test = df_test.drop(' Label', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-pC2aCJVMxy"
   },
   "source": [
    "### Scaling the data \n",
    "\n",
    "The last step we need to do in our Data Preparation task is to scale the data by removing the mean and scaling to unit variance. Most machine learning algorithms (including neural networks) required data to be scaled to some small values to be able to learn better. Note that we compute the mean and standard deviation from the **training set only** and use the computed mean and standard deviation to scale the validation and the test set. This is to prevent information leakage from validation/test set to training set. We use the `StandardScaler()` from Scikit-Learn to compute mean/standard deviation and to scale the data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMriNJIsVMxy"
   },
   "outputs": [],
   "source": [
    "# Compute the mean and standard deviation using StandardScaler() and transform (scale) the X_train values.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFazXrT4VMx0"
   },
   "outputs": [],
   "source": [
    "# Now we use the scaler computed from X_train to scale the Validation, Tuning and Test set.\n",
    "X_val_scaled = scaler.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Complete the codes below to scale the X_tune and X_test. \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "<br/>\n",
    "    \n",
    "```\n",
    "X_tune_scaled = scaler.transform(X_tune)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "</details>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale the tuning and test set \n",
    "X_tune_scaled = scaler.transform(X_tune)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SV9jlYxnVMx1"
   },
   "outputs": [],
   "source": [
    "# Good to make sure that our training set does not contain any NaN values, just in case, before we start training.\n",
    "assert not np.any(np.isnan(X_train_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nzkQGiJWVMx3"
   },
   "source": [
    "Congratulations, we have finished our data preparation !!! As in all machine learning projects, data preparation is always the **most** time consuming phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8lJYAsAVMx3"
   },
   "source": [
    "## Build the Variational Autoencoder (VAE) network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a couple of diemension. \n",
    "\n",
    "We set the input dimension (input_dim) to be number of columns (features) in X_train.  X_train.shape will give you a tuple of (#rows, #columns), to get the #columns, we take the 2nd value in the tuple (0 being 1st value, 1 being 2nd value)\n",
    "\n",
    "We also set the latent dimension. Recall from the lecture that Autoencoder will try to learn the essential information from the data and try to compress the information into the 'latent space'. `latent_dim` is the size of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpBJwkeKVMx3"
   },
   "outputs": [],
   "source": [
    "# we set the input dimensions to the number of features in X, i.e. 78.  \n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# we set the dimension of the latent representation to 20 neurons, \n",
    "# in other words, we want to condense the information in the input into the latent space of 20 neurons\n",
    "latent_dim = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FwThzk_VMx5"
   },
   "source": [
    "### Encoder Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6XqjY9_VMx5"
   },
   "source": [
    "We will now build the Encoder part of the Variational Autoencoder (VAE). One main difference between a Variational Autoencoder and a normal Autoencoder is that VAE learns the mean and the standard deviation of the inputs, basically learning the distribution of the inputs. It will then sample from this learnt distribution to produce codings that can be used by the decoder to produce an output.\n",
    "\n",
    "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/vae_encoder.png\" height='500' width='400'>\n",
    "\n",
    "In the following code, we construct our encoder which consists of 3 hidden layers: 1st hidden layer with 80 neurons, 2nd hidden layer with 64 neurons and the 3rd hidden layer with 32 neurons). Our latent layers consist of `laten_dim` neurons. \n",
    "\n",
    "We have two latent layers, one for ```codings_mean``` which is the mean, and one for ```log_var``` which is the log of variance (instead of using standard deviation, it is mathematically easier to use log of variance, for computation purpose) learnt. ```Sampling()``` is a function used to sample from this learnt normal distribution with the learnt mean and variance. (Note: You can examine the `utils.py` file if you want to see how the sampling is done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "id": "765R2lXTVMx5",
    "outputId": "84bb4863-10de-4610-ca52-1d27be2800ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_44\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, 78)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden1 (Dense)                 (None, 80)           6320        Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "hidden2 (Dense)                 (None, 60)           4860        hidden1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hidden4 (Dense)                 (None, 50)           3050        hidden2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activity_regularization_1 (Acti (None, 50)           0           hidden4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hidden3 (Dense)                 (None, 32)           1632        activity_regularization_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activity_regularization_2 (Acti (None, 32)           0           hidden3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 30)           990         activity_regularization_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "log_var (Dense)                 (None, 30)           990         activity_regularization_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "sampling_7 (Sampling)           (None, 30)           0           mean[0][0]                       \n",
      "                                                                 log_var[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,842\n",
      "Trainable params: 17,842\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(input_dim,), name='Input')\n",
    "z = Dense(80, activation='tanh', name='hidden1')(inputs)\n",
    "#z = Dropout(0.1)(z)\n",
    "#z = ActivityRegularization(l1=1e-3)(z)\n",
    "z = Dense(60, activation='tanh', name='hidden2')(z)\n",
    "#z = ActivityRegularization(l1=1e-3)(z)\n",
    "z = Dense(50, activation='tanh', name='hidden4')(z)\n",
    "z = ActivityRegularization(l1=1e-3)(z)\n",
    "z = Dense(32, activation='tanh', name='hidden3')(z)\n",
    "z = ActivityRegularization(l1=1e-3)(z)\n",
    "codings_mean = Dense(latent_dim, name='mean')(z)\n",
    "codings_log_var = Dense(latent_dim, name='log_var')(z)\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "vae_encoder = Model(inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n",
    "\n",
    "vae_encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUqBcvkrVMx-"
   },
   "source": [
    "### Decoder Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6FfEG_BVMx_"
   },
   "source": [
    "Here, we will build the Decoder part of the VAE. \n",
    "\n",
    "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/vae_decoder.png\" height=\"500\" width=\"400\"/>\n",
    "\n",
    "The decoder will be 'inverse' copy of the encoder, with each layer having more and more neurons (32 -> 64 -> 80) until it get backs the original input size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "_ybr2Iu4VMx_",
    "outputId": "fce9ecec-a6c1-4781-e6e2-c306caecedc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "codings (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "hidden4 (Dense)              (None, 32)                992       \n",
      "_________________________________________________________________\n",
      "hidden6 (Dense)              (None, 50)                1650      \n",
      "_________________________________________________________________\n",
      "hidden7 (Dense)              (None, 60)                3060      \n",
      "_________________________________________________________________\n",
      "hidden8 (Dense)              (None, 80)                4880      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 78)                6318      \n",
      "=================================================================\n",
      "Total params: 16,900\n",
      "Trainable params: 16,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = Input(shape=[latent_dim], name='codings')\n",
    "x = Dense(32, activation='tanh', name='hidden4')(decoder_inputs)\n",
    "#x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation='tanh', name='hidden6')(x)\n",
    "#x = Dropout(0.1)(x)\n",
    "x = Dense(60, activation='tanh', name='hidden7')(x)\n",
    "#x = Dropout(0.1)(x)\n",
    "x = Dense(80, activation='tanh', name='hidden8')(x)\n",
    "outputs = Dense(input_dim, activation=\"linear\", name='output')(x)\n",
    "vae_decoder = Model(inputs=[decoder_inputs], outputs=[outputs])\n",
    "vae_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFtTdPvNVMyB"
   },
   "source": [
    "The codes below are just specifying the two loss functions that the model will use to try to minimise the losses at each training step. \n",
    "\n",
    "The first loss function is the MSE (mean-squared-error or mse) loss which is computed from the differences between predicted output and expected output.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{m}\\sum_{i=1}^m (\\hat{y}_{i} - y_i)^2\n",
    "\\end{equation*}\n",
    "\n",
    "wher $\\hat{y}$ is the predicted output and $y$ is the actual value. \n",
    "\n",
    "The second loss which is the latent_loss is given by the following formula: \n",
    "\n",
    "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/latent_loss_formula.png\" height='30' width='400'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Y1wBPy5VMyD"
   },
   "outputs": [],
   "source": [
    "_, _, codings = vae_encoder(inputs)\n",
    "reconstructions = vae_decoder(codings)\n",
    "vae = Model(inputs=[inputs], outputs=[reconstructions])\n",
    "\n",
    "# Compute the latent loss (based on mean and variance)\n",
    "latent_loss = -0.5 * K.sum(\n",
    "    1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),\n",
    "    axis=-1)\n",
    "\n",
    "# reconstruction loss is based on MSE loss\n",
    "reconstruction_loss = losses.mse(inputs, reconstructions)\n",
    "reconstruction_loss *= input_dim\n",
    "\n",
    "# total loss for VAE is the mean of both latest and reconstruction loss\n",
    "vae_loss = K.mean(latent_loss + reconstruction_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "rmsprop = RMSprop()\n",
    "vae.compile(loss='mean_squared_error', optimizer=rmsprop, metrics=['mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "colab_type": "code",
    "id": "z0_xHDnLVMyE",
    "outputId": "42b9209f-461e-4f28-8f31-9b8d93528e12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_48\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, 78)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "functional_44 (Functional)      [(None, 30), (None,  17842       Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "functional_46 (Functional)      (None, 78)           16900       functional_44[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden1 (Dense)                 (None, 80)           6320        Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "hidden2 (Dense)                 (None, 60)           4860        hidden1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hidden4 (Dense)                 (None, 50)           3050        hidden2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activity_regularization_1 (Acti (None, 50)           0           hidden4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hidden3 (Dense)                 (None, 32)           1632        activity_regularization_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activity_regularization_2 (Acti (None, 32)           0           hidden3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "log_var (Dense)                 (None, 30)           990         activity_regularization_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_12 (TensorFlo [(None, 30)]         0           log_var[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_6 (TensorFlowOp [(None, 30)]         0           log_var[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 30)           990         activity_regularization_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_12 (TensorFlowO [(None, 30)]         0           tf_op_layer_AddV2_12[0][0]       \n",
      "                                                                 tf_op_layer_Exp_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Square_6 (TensorFlo [(None, 30)]         0           mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_13 (TensorFlowO [(None, 30)]         0           tf_op_layer_Sub_12[0][0]         \n",
      "                                                                 tf_op_layer_Square_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_SquaredDifference_6 [(None, 78)]         0           functional_46[0][0]              \n",
      "                                                                 Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_6 (TensorFlowOp [(None,)]            0           tf_op_layer_Sub_13[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_12 (TensorFlow [(None,)]            0           tf_op_layer_SquaredDifference_6[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_12 (TensorFlowO [(None,)]            0           tf_op_layer_Sum_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_13 (TensorFlowO [(None,)]            0           tf_op_layer_Mean_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_13 (TensorFlo [(None,)]            0           tf_op_layer_Mul_12[0][0]         \n",
      "                                                                 tf_op_layer_Mul_13[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_13 (TensorFlow [()]                 0           tf_op_layer_AddV2_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_loss_6 (AddLoss)            ()                   0           tf_op_layer_Mean_13[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 34,742\n",
      "Trainable params: 34,742\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZAPUB3CVMyH"
   },
   "source": [
    "### Training the Network \n",
    "\n",
    "Now we are ready to train our VAE. Neural network like VAE learns by trying to minimise the training loss (i.e the error between its predicted output and the actual expected output) at each training cycle (which we call *epoch*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1tJQP00OVMyG",
    "outputId": "0fb4c870-97a5-49b7-f2e1-142604d8fad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  2/828 [..............................] - ETA: 25s - loss: 7.5923 - mean_squared_error: 0.0382WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0579s). Check your callbacks.\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 14.7123 - mean_squared_error: 0.1285 - val_loss: 22.4979 - val_mean_squared_error: 0.2275\n",
      "Epoch 2/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.5157 - mean_squared_error: 0.1260 - val_loss: 23.4204 - val_mean_squared_error: 0.2385\n",
      "Epoch 3/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.4897 - mean_squared_error: 0.1257 - val_loss: 22.0305 - val_mean_squared_error: 0.2226\n",
      "Epoch 4/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.5176 - mean_squared_error: 0.1260 - val_loss: 21.9665 - val_mean_squared_error: 0.2220\n",
      "Epoch 5/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 14.4143 - mean_squared_error: 0.1248 - val_loss: 21.4704 - val_mean_squared_error: 0.2144\n",
      "Epoch 6/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.3318 - mean_squared_error: 0.1236 - val_loss: 21.3117 - val_mean_squared_error: 0.2123\n",
      "Epoch 7/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.3040 - mean_squared_error: 0.1233 - val_loss: 21.5416 - val_mean_squared_error: 0.2156\n",
      "Epoch 8/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.3398 - mean_squared_error: 0.1239 - val_loss: 22.5073 - val_mean_squared_error: 0.2268\n",
      "Epoch 9/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.1961 - mean_squared_error: 0.1220 - val_loss: 21.7590 - val_mean_squared_error: 0.2135\n",
      "Epoch 10/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.1906 - mean_squared_error: 0.1218 - val_loss: 22.6267 - val_mean_squared_error: 0.2287\n",
      "Epoch 11/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.2086 - mean_squared_error: 0.1221 - val_loss: 21.1020 - val_mean_squared_error: 0.2087\n",
      "Epoch 12/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.0623 - mean_squared_error: 0.1202 - val_loss: 22.7268 - val_mean_squared_error: 0.2299\n",
      "Epoch 13/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.0753 - mean_squared_error: 0.1204 - val_loss: 21.3598 - val_mean_squared_error: 0.2121\n",
      "Epoch 14/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.0924 - mean_squared_error: 0.1206 - val_loss: 21.3584 - val_mean_squared_error: 0.2129\n",
      "Epoch 15/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.5138 - mean_squared_error: 0.1260 - val_loss: 20.6569 - val_mean_squared_error: 0.2048\n",
      "Epoch 16/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.9842 - mean_squared_error: 0.1194 - val_loss: 23.2866 - val_mean_squared_error: 0.2371\n",
      "Epoch 17/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.5078 - mean_squared_error: 0.1259 - val_loss: 20.7094 - val_mean_squared_error: 0.2049\n",
      "Epoch 18/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.8724 - mean_squared_error: 0.1179 - val_loss: 20.6642 - val_mean_squared_error: 0.2047\n",
      "Epoch 19/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.7866 - mean_squared_error: 0.1169 - val_loss: 19.9948 - val_mean_squared_error: 0.1952\n",
      "Epoch 20/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 14.0675 - mean_squared_error: 0.1204 - val_loss: 20.4781 - val_mean_squared_error: 0.2011\n",
      "Epoch 21/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.8096 - mean_squared_error: 0.1172 - val_loss: 25.6921 - val_mean_squared_error: 0.2682\n",
      "Epoch 22/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.6613 - mean_squared_error: 0.1152 - val_loss: 19.7683 - val_mean_squared_error: 0.1922\n",
      "Epoch 23/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.6684 - mean_squared_error: 0.1153 - val_loss: 21.5507 - val_mean_squared_error: 0.2157\n",
      "Epoch 24/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.8557 - mean_squared_error: 0.1177 - val_loss: 19.6924 - val_mean_squared_error: 0.1906\n",
      "Epoch 25/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.5877 - mean_squared_error: 0.1143 - val_loss: 20.2316 - val_mean_squared_error: 0.1978\n",
      "Epoch 26/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.5494 - mean_squared_error: 0.1138 - val_loss: 19.5807 - val_mean_squared_error: 0.1902\n",
      "Epoch 27/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.5628 - mean_squared_error: 0.1140 - val_loss: 19.6468 - val_mean_squared_error: 0.1926\n",
      "Epoch 28/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.6134 - mean_squared_error: 0.1147 - val_loss: 19.6693 - val_mean_squared_error: 0.1913\n",
      "Epoch 29/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.4138 - mean_squared_error: 0.1122 - val_loss: 19.8377 - val_mean_squared_error: 0.1925\n",
      "Epoch 30/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.5007 - mean_squared_error: 0.1132 - val_loss: 19.4157 - val_mean_squared_error: 0.1886\n",
      "Epoch 31/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.3872 - mean_squared_error: 0.1119 - val_loss: 19.7640 - val_mean_squared_error: 0.1907\n",
      "Epoch 32/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.2889 - mean_squared_error: 0.1106 - val_loss: 19.6632 - val_mean_squared_error: 0.1905\n",
      "Epoch 33/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.3271 - mean_squared_error: 0.1110 - val_loss: 19.5441 - val_mean_squared_error: 0.1876\n",
      "Epoch 34/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.2681 - mean_squared_error: 0.1104 - val_loss: 19.2713 - val_mean_squared_error: 0.1866\n",
      "Epoch 35/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.1742 - mean_squared_error: 0.1090 - val_loss: 18.5797 - val_mean_squared_error: 0.1780\n",
      "Epoch 36/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 13.5924 - mean_squared_error: 0.1144 - val_loss: 18.6968 - val_mean_squared_error: 0.1805\n",
      "Epoch 37/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.1806 - mean_squared_error: 0.1092 - val_loss: 21.3431 - val_mean_squared_error: 0.2113\n",
      "Epoch 38/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.1029 - mean_squared_error: 0.1081 - val_loss: 20.3666 - val_mean_squared_error: 0.1985\n",
      "Epoch 39/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.1817 - mean_squared_error: 0.1092 - val_loss: 19.5150 - val_mean_squared_error: 0.1892\n",
      "Epoch 40/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.9950 - mean_squared_error: 0.1068 - val_loss: 20.3782 - val_mean_squared_error: 0.2018\n",
      "Epoch 41/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.1185 - mean_squared_error: 0.1084 - val_loss: 20.1243 - val_mean_squared_error: 0.1970\n",
      "Epoch 42/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.9632 - mean_squared_error: 0.1063 - val_loss: 18.1149 - val_mean_squared_error: 0.1728\n",
      "Epoch 43/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.0816 - mean_squared_error: 0.1078 - val_loss: 20.8416 - val_mean_squared_error: 0.2066\n",
      "Epoch 44/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.8957 - mean_squared_error: 0.1056 - val_loss: 18.6726 - val_mean_squared_error: 0.1782\n",
      "Epoch 45/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.8996 - mean_squared_error: 0.1057 - val_loss: 17.9733 - val_mean_squared_error: 0.1701\n",
      "Epoch 46/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.8828 - mean_squared_error: 0.1056 - val_loss: 18.4358 - val_mean_squared_error: 0.1723\n",
      "Epoch 47/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.8937 - mean_squared_error: 0.1056 - val_loss: 17.8611 - val_mean_squared_error: 0.1670\n",
      "Epoch 48/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.7986 - mean_squared_error: 0.1044 - val_loss: 18.7088 - val_mean_squared_error: 0.1797\n",
      "Epoch 49/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.7105 - mean_squared_error: 0.1033 - val_loss: 17.5170 - val_mean_squared_error: 0.1642\n",
      "Epoch 50/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.7266 - mean_squared_error: 0.1033 - val_loss: 22.9358 - val_mean_squared_error: 0.2331\n",
      "Epoch 51/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 13.1768 - mean_squared_error: 0.1091 - val_loss: 18.7263 - val_mean_squared_error: 0.1796\n",
      "Epoch 52/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.9376 - mean_squared_error: 0.1060 - val_loss: 17.7952 - val_mean_squared_error: 0.1681\n",
      "Epoch 53/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.6963 - mean_squared_error: 0.1031 - val_loss: 17.3432 - val_mean_squared_error: 0.1634\n",
      "Epoch 54/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.7111 - mean_squared_error: 0.1032 - val_loss: 20.3014 - val_mean_squared_error: 0.2006\n",
      "Epoch 55/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.5584 - mean_squared_error: 0.1013 - val_loss: 17.2750 - val_mean_squared_error: 0.1601\n",
      "Epoch 56/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.6353 - mean_squared_error: 0.1022 - val_loss: 17.2756 - val_mean_squared_error: 0.1622\n",
      "Epoch 57/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.4950 - mean_squared_error: 0.1005 - val_loss: 19.6807 - val_mean_squared_error: 0.1934\n",
      "Epoch 58/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.6022 - mean_squared_error: 0.1020 - val_loss: 17.3081 - val_mean_squared_error: 0.1611\n",
      "Epoch 59/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.3937 - mean_squared_error: 0.0993 - val_loss: 16.9842 - val_mean_squared_error: 0.1557\n",
      "Epoch 60/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.5902 - mean_squared_error: 0.1016 - val_loss: 16.8534 - val_mean_squared_error: 0.1558\n",
      "Epoch 61/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.6695 - mean_squared_error: 0.1027 - val_loss: 18.5803 - val_mean_squared_error: 0.1779\n",
      "Epoch 62/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.3439 - mean_squared_error: 0.0984 - val_loss: 17.2101 - val_mean_squared_error: 0.1602\n",
      "Epoch 63/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.6149 - mean_squared_error: 0.1021 - val_loss: 17.7130 - val_mean_squared_error: 0.1670\n",
      "Epoch 64/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.2764 - mean_squared_error: 0.0977 - val_loss: 16.6617 - val_mean_squared_error: 0.1503\n",
      "Epoch 65/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.3713 - mean_squared_error: 0.0989 - val_loss: 18.4387 - val_mean_squared_error: 0.1760\n",
      "Epoch 66/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.8860 - mean_squared_error: 0.1055 - val_loss: 16.6733 - val_mean_squared_error: 0.1508\n",
      "Epoch 67/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.1835 - mean_squared_error: 0.0965 - val_loss: 16.1107 - val_mean_squared_error: 0.1471\n",
      "Epoch 68/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.1350 - mean_squared_error: 0.0957 - val_loss: 17.9636 - val_mean_squared_error: 0.1706\n",
      "Epoch 69/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.1417 - mean_squared_error: 0.0961 - val_loss: 18.7000 - val_mean_squared_error: 0.1775\n",
      "Epoch 70/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.3014 - mean_squared_error: 0.0980 - val_loss: 15.9951 - val_mean_squared_error: 0.1438\n",
      "Epoch 71/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.0766 - mean_squared_error: 0.0952 - val_loss: 17.7236 - val_mean_squared_error: 0.1664\n",
      "Epoch 72/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.0352 - mean_squared_error: 0.0946 - val_loss: 15.8165 - val_mean_squared_error: 0.1436\n",
      "Epoch 73/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.7922 - mean_squared_error: 0.1044 - val_loss: 15.7873 - val_mean_squared_error: 0.1416\n",
      "Epoch 74/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.7223 - mean_squared_error: 0.1032 - val_loss: 15.9847 - val_mean_squared_error: 0.1455\n",
      "Epoch 75/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.1731 - mean_squared_error: 0.0964 - val_loss: 22.1724 - val_mean_squared_error: 0.2227\n",
      "Epoch 76/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.9439 - mean_squared_error: 0.1062 - val_loss: 17.3483 - val_mean_squared_error: 0.1638\n",
      "Epoch 77/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.1348 - mean_squared_error: 0.0958 - val_loss: 15.4448 - val_mean_squared_error: 0.1372\n",
      "Epoch 78/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.3227 - mean_squared_error: 0.0981 - val_loss: 15.7332 - val_mean_squared_error: 0.1419\n",
      "Epoch 79/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.1167 - mean_squared_error: 0.0955 - val_loss: 20.7413 - val_mean_squared_error: 0.2059\n",
      "Epoch 80/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.2017 - mean_squared_error: 0.0968 - val_loss: 17.1465 - val_mean_squared_error: 0.1578\n",
      "Epoch 81/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.9136 - mean_squared_error: 0.0932 - val_loss: 19.1406 - val_mean_squared_error: 0.1836\n",
      "Epoch 82/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.9061 - mean_squared_error: 0.0931 - val_loss: 15.4282 - val_mean_squared_error: 0.1378\n",
      "Epoch 83/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.0143 - mean_squared_error: 0.0944 - val_loss: 15.9218 - val_mean_squared_error: 0.1433\n",
      "Epoch 84/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 11.9935 - mean_squared_error: 0.0940 - val_loss: 15.2966 - val_mean_squared_error: 0.1357\n",
      "Epoch 85/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.2327 - mean_squared_error: 0.0971 - val_loss: 14.9094 - val_mean_squared_error: 0.1307\n",
      "Epoch 86/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.0199 - mean_squared_error: 0.0943 - val_loss: 18.0403 - val_mean_squared_error: 0.1694\n",
      "Epoch 87/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.1246 - mean_squared_error: 0.0958 - val_loss: 15.2975 - val_mean_squared_error: 0.1375\n",
      "Epoch 88/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.0991 - mean_squared_error: 0.0955 - val_loss: 19.8937 - val_mean_squared_error: 0.1955\n",
      "Epoch 89/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.4131 - mean_squared_error: 0.0996 - val_loss: 16.9576 - val_mean_squared_error: 0.1572\n",
      "Epoch 90/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.1462 - mean_squared_error: 0.0960 - val_loss: 17.6430 - val_mean_squared_error: 0.1652\n",
      "Epoch 91/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 12.1052 - mean_squared_error: 0.0955 - val_loss: 15.5670 - val_mean_squared_error: 0.1385\n",
      "Epoch 92/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.5570 - mean_squared_error: 0.1012 - val_loss: 14.9117 - val_mean_squared_error: 0.1310\n",
      "Epoch 93/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.0020 - mean_squared_error: 0.0942 - val_loss: 14.8187 - val_mean_squared_error: 0.1309\n",
      "Epoch 94/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.6637 - mean_squared_error: 0.1026 - val_loss: 17.1350 - val_mean_squared_error: 0.1598\n",
      "Epoch 95/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.9065 - mean_squared_error: 0.0932 - val_loss: 15.2620 - val_mean_squared_error: 0.1351\n",
      "Epoch 96/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.6542 - mean_squared_error: 0.0899 - val_loss: 17.1539 - val_mean_squared_error: 0.1596\n",
      "Epoch 97/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.6989 - mean_squared_error: 0.0905 - val_loss: 14.5272 - val_mean_squared_error: 0.1274\n",
      "Epoch 98/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.9659 - mean_squared_error: 0.0937 - val_loss: 14.8680 - val_mean_squared_error: 0.1304\n",
      "Epoch 99/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.7740 - mean_squared_error: 0.0914 - val_loss: 14.6860 - val_mean_squared_error: 0.1292\n",
      "Epoch 100/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.7889 - mean_squared_error: 0.0917 - val_loss: 14.4391 - val_mean_squared_error: 0.1258\n",
      "Epoch 101/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.8305 - mean_squared_error: 0.0921 - val_loss: 14.4200 - val_mean_squared_error: 0.1246\n",
      "Epoch 102/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.4871 - mean_squared_error: 0.0878 - val_loss: 14.5015 - val_mean_squared_error: 0.1266\n",
      "Epoch 103/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.7234 - mean_squared_error: 0.0905 - val_loss: 14.2121 - val_mean_squared_error: 0.1234\n",
      "Epoch 104/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.7130 - mean_squared_error: 0.0906 - val_loss: 14.4314 - val_mean_squared_error: 0.1254\n",
      "Epoch 105/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.6241 - mean_squared_error: 0.0894 - val_loss: 16.8533 - val_mean_squared_error: 0.1563\n",
      "Epoch 106/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.6616 - mean_squared_error: 0.0900 - val_loss: 14.0979 - val_mean_squared_error: 0.1216\n",
      "Epoch 107/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.5673 - mean_squared_error: 0.0888 - val_loss: 14.5128 - val_mean_squared_error: 0.1228\n",
      "Epoch 108/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.6783 - mean_squared_error: 0.0901 - val_loss: 14.3687 - val_mean_squared_error: 0.1214\n",
      "Epoch 109/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 12.2433 - mean_squared_error: 0.0974 - val_loss: 14.4531 - val_mean_squared_error: 0.1246\n",
      "Epoch 110/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.4092 - mean_squared_error: 0.0868 - val_loss: 21.1295 - val_mean_squared_error: 0.2101\n",
      "Epoch 111/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.4128 - mean_squared_error: 0.0866 - val_loss: 13.9490 - val_mean_squared_error: 0.1190\n",
      "Epoch 112/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.8053 - mean_squared_error: 0.0919 - val_loss: 14.0126 - val_mean_squared_error: 0.1168\n",
      "Epoch 113/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.3093 - mean_squared_error: 0.0854 - val_loss: 17.8573 - val_mean_squared_error: 0.1669\n",
      "Epoch 114/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.9239 - mean_squared_error: 0.0933 - val_loss: 15.1729 - val_mean_squared_error: 0.1340\n",
      "Epoch 115/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.3840 - mean_squared_error: 0.0867 - val_loss: 14.1210 - val_mean_squared_error: 0.1206\n",
      "Epoch 116/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.5120 - mean_squared_error: 0.0880 - val_loss: 15.9971 - val_mean_squared_error: 0.1448\n",
      "Epoch 117/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.2465 - mean_squared_error: 0.0846 - val_loss: 14.2298 - val_mean_squared_error: 0.1225\n",
      "Epoch 118/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.9478 - mean_squared_error: 0.0935 - val_loss: 16.0236 - val_mean_squared_error: 0.1454\n",
      "Epoch 119/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.2739 - mean_squared_error: 0.0851 - val_loss: 13.8313 - val_mean_squared_error: 0.1168\n",
      "Epoch 120/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.1949 - mean_squared_error: 0.0840 - val_loss: 13.7828 - val_mean_squared_error: 0.1191\n",
      "Epoch 121/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.1641 - mean_squared_error: 0.0839 - val_loss: 14.0499 - val_mean_squared_error: 0.1196\n",
      "Epoch 122/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.1680 - mean_squared_error: 0.0837 - val_loss: 16.1001 - val_mean_squared_error: 0.1465\n",
      "Epoch 123/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.4074 - mean_squared_error: 0.0868 - val_loss: 13.4542 - val_mean_squared_error: 0.1118\n",
      "Epoch 124/150\n",
      "828/828 [==============================] - 3s 4ms/step - loss: 11.5507 - mean_squared_error: 0.0887 - val_loss: 15.0546 - val_mean_squared_error: 0.1357\n",
      "Epoch 125/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.4710 - mean_squared_error: 0.0878 - val_loss: 16.7476 - val_mean_squared_error: 0.1535\n",
      "Epoch 126/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.4679 - mean_squared_error: 0.0876 - val_loss: 15.9476 - val_mean_squared_error: 0.1446\n",
      "Epoch 127/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.4768 - mean_squared_error: 0.0876 - val_loss: 17.5330 - val_mean_squared_error: 0.1655\n",
      "Epoch 128/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.5089 - mean_squared_error: 0.0880 - val_loss: 18.7592 - val_mean_squared_error: 0.1787\n",
      "Epoch 129/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.3590 - mean_squared_error: 0.0860 - val_loss: 13.9384 - val_mean_squared_error: 0.1202\n",
      "Epoch 130/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.4553 - mean_squared_error: 0.0875 - val_loss: 15.7183 - val_mean_squared_error: 0.1427\n",
      "Epoch 131/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.0606 - mean_squared_error: 0.0824 - val_loss: 16.7713 - val_mean_squared_error: 0.1547\n",
      "Epoch 132/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.0718 - mean_squared_error: 0.0827 - val_loss: 12.9781 - val_mean_squared_error: 0.1071\n",
      "Epoch 133/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.3375 - mean_squared_error: 0.0859 - val_loss: 14.4502 - val_mean_squared_error: 0.1246\n",
      "Epoch 134/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.2864 - mean_squared_error: 0.0851 - val_loss: 13.2162 - val_mean_squared_error: 0.1093\n",
      "Epoch 135/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.3586 - mean_squared_error: 0.0861 - val_loss: 14.4297 - val_mean_squared_error: 0.1262\n",
      "Epoch 136/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.4053 - mean_squared_error: 0.0869 - val_loss: 15.3311 - val_mean_squared_error: 0.1371\n",
      "Epoch 137/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.7005 - mean_squared_error: 0.0904 - val_loss: 14.6135 - val_mean_squared_error: 0.1288\n",
      "Epoch 138/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.4720 - mean_squared_error: 0.0876 - val_loss: 14.5618 - val_mean_squared_error: 0.1265\n",
      "Epoch 139/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.3448 - mean_squared_error: 0.0859 - val_loss: 12.8180 - val_mean_squared_error: 0.1041\n",
      "Epoch 140/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.2921 - mean_squared_error: 0.0853 - val_loss: 12.9256 - val_mean_squared_error: 0.1054\n",
      "Epoch 141/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 11.4902 - mean_squared_error: 0.0878 - val_loss: 12.6730 - val_mean_squared_error: 0.1029\n",
      "Epoch 142/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 10.9131 - mean_squared_error: 0.0806 - val_loss: 12.8817 - val_mean_squared_error: 0.1063\n",
      "Epoch 143/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.1713 - mean_squared_error: 0.0840 - val_loss: 21.9918 - val_mean_squared_error: 0.2216\n",
      "Epoch 144/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.0180 - mean_squared_error: 0.0819 - val_loss: 16.6239 - val_mean_squared_error: 0.1531\n",
      "Epoch 145/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 10.9481 - mean_squared_error: 0.0810 - val_loss: 16.0207 - val_mean_squared_error: 0.1447\n",
      "Epoch 146/150\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 10.8664 - mean_squared_error: 0.0799 - val_loss: 13.4186 - val_mean_squared_error: 0.1124\n",
      "Epoch 147/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.3247 - mean_squared_error: 0.0855 - val_loss: 13.1447 - val_mean_squared_error: 0.1043\n",
      "Epoch 148/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 10.8691 - mean_squared_error: 0.0799 - val_loss: 12.9801 - val_mean_squared_error: 0.1076\n",
      "Epoch 149/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 10.9508 - mean_squared_error: 0.0812 - val_loss: 13.9503 - val_mean_squared_error: 0.1186\n",
      "Epoch 150/150\n",
      "828/828 [==============================] - 4s 4ms/step - loss: 11.1673 - mean_squared_error: 0.0838 - val_loss: 12.5451 - val_mean_squared_error: 0.1025\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "num_epochs = 150\n",
    "batch_size = 512\n",
    "save_model = False\n",
    "\n",
    "if train:     \n",
    "    run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "    history = vae.fit(X_train_scaled, X_train_scaled, \n",
    "                      epochs=num_epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      verbose=1, \n",
    "                      shuffle=True,\n",
    "                      validation_data = (X_val_scaled, X_val_scaled),\n",
    "                      callbacks=[tensorboard_cb])\n",
    "    \n",
    "    if save_model: \n",
    "        vae.save('trained_model')\n",
    "else:\n",
    "    vae = tf.keras.models.load_model('trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PK3MszQGVMyI"
   },
   "source": [
    "Now we will plot the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQGAjk_vVMyI",
    "outputId": "817eb2cf-ef5b-45aa-ece2-98576dd55a5b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot the training loss\n",
    "if train:\n",
    "    with open('training.history', 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    plot_training_loss(history.history)\n",
    "else:\n",
    "    with open('training.history', 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "    plot_training_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Change the epochs to larger value and train. What do you observe? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xvsHmm9LVMyK"
   },
   "source": [
    "### Evaluate the trained model on 'Tuning' Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xXHqEDNIVMyK"
   },
   "source": [
    "Now let us compare the reconstruction error (MSE) between training set (the clean, benign traffic) and that of test set (the one with benign + attack traffic). In the below, we compute the mean of the entire data set (of X_train_scaled and X_tune_scaled), as we just want to have a feel of the overall error of the two different data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUvh6wX6VMyK"
   },
   "outputs": [],
   "source": [
    "X_train_preds = vae.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GWcsUaIVVMyM",
    "outputId": "e9aa01aa-b297-4152-e3ec-9b173367844b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08695916858523008\n"
     ]
    }
   ],
   "source": [
    "X_train_mse  =   np.square(X_train_scaled - X_train_preds).mean()\n",
    "print(X_train_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Complete the code below to compute and display the MSE of 'Tuning' dataset. \n",
    "\n",
    "What do you observe about the value of MSE? Is it higher or lower than the MSE of the training set?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "<br/>\n",
    "    \n",
    "```\n",
    "X_tune_preds = vae.predict(X_tune_scaled)\n",
    "X_tune_mse = np.square(X_tune_scaled - X_tune_preds).mean()\n",
    "print(X_tune_mse)\n",
    "```\n",
    "\n",
    "<br/>\n",
    "    \n",
    "From the value of `X_tune_mse`, we can see that the MSE (the mean squared error) for the tuning dataset is much much than the training set. The presence of attack traffic make it more difficult for Variational Autoencoder to reconstruct the input accurately, leading to higher MSE.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_l3kqOKVMyO",
    "outputId": "0f7f6124-0eef-4605-c1a1-11c5cf1e51c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7827664500264913\n"
     ]
    }
   ],
   "source": [
    "# TODO: complete the code below\n",
    "\n",
    "X_tune_preds = vae.predict(X_tune_scaled)\n",
    "X_tune_mse = np.square(X_tune_scaled - X_tune_preds).mean()\n",
    "print(X_tune_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eh9Z7HdCVMyQ"
   },
   "source": [
    "### Compute the distance between Train set and the 'Tuning' set. \n",
    "\n",
    "Now let us compute the 'distance' between each sample and its reconstructed version. Think of 'distance' as the difference between two different multi-dimensional vector. As each sample is a multi-demensional vector, the difference will be multi-dimensional as well. Therefore we will use the [*Frobenius norm*](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) to measure the distance between the sample and its reconstruction.  In the following, we use the numpy library function `np.linalg.norm()` to compute the *Frobenius norm*.  \n",
    "\n",
    "**Note:** \n",
    "\n",
    "In this section, we are interested in knowing error of each sample, unlike the previous section, where we are just looking at mean error rate of entire train or tuning set.  The reason is we want to compare error of each sample with the error threshold to determine if the sample is anomalous or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyZD3nEYVMyQ",
    "outputId": "ddbc58bb-6034-4cb8-f61f-1ed90dc71037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =  0.29971977275526696\n",
      "max =  686.4434384702425\n",
      "mean =  1.4025069983269993\n",
      "std =  2.2005741387811577\n"
     ]
    }
   ],
   "source": [
    "X_train_preds = vae.predict(X_train_scaled)\n",
    "X_train_errors = np.linalg.norm(X_train_scaled - X_train_preds, axis=1)\n",
    "print('min = ',X_train_errors.min())\n",
    "print('max = ',X_train_errors.max())\n",
    "print('mean = ',X_train_errors.mean())\n",
    "print('std = ',X_train_errors.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdYqVXs7VMyS",
    "outputId": "40c99e8d-8bb1-4f8b-b2b1-1105d38c64fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =  0.31349592684700356\n",
      "max =  437.0330792837683\n",
      "mean =  6.0605194295562805\n",
      "std =  10.119189198702593\n"
     ]
    }
   ],
   "source": [
    "X_tune_preds = vae.predict(X_tune_scaled)\n",
    "X_tune_errors = np.linalg.norm(X_tune_scaled - X_tune_preds, axis=1)\n",
    "print('min = ', X_tune_errors.min())\n",
    "print('max = ',X_tune_errors.max())\n",
    "print('mean = ',X_tune_errors.mean())\n",
    "print('std = ', X_tune_errors.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mSDmT8yqVMyU",
    "outputId": "fc441911-fde2-441f-8454-eb672829bfaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =  0.3094415028330732\n",
      "max =  386.50210679451425\n",
      "mean =  6.081991192073087\n",
      "std =  10.21285628656746\n"
     ]
    }
   ],
   "source": [
    "X_test_preds = vae.predict(X_test_scaled)\n",
    "X_test_errors = np.linalg.norm(X_test_scaled - X_test_preds, axis=1)\n",
    "print('min = ', X_test_errors.min())\n",
    "print('max = ',X_test_errors.max())\n",
    "print('mean = ',X_test_errors.mean())\n",
    "print('std = ', X_test_errors.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcLd_CxtVMyV"
   },
   "source": [
    "## Choosing the Anomlay Score Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AV6MdaGBVMyW"
   },
   "source": [
    "Based on above, it seems reasonable to set the threshold at mean + 3 std deviation of training errors. In the absence of labels to help us adjust threshold for the anomaly detector, we could set the threshold derived by statistical means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUOvsekbVMyW",
    "outputId": "0859e44f-7e6d-4c8c-eb1f-f4ad9075d17d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.004229414670473\n"
     ]
    }
   ],
   "source": [
    "threshold = X_train_errors.mean() + 3 * X_train_errors.std()\n",
    "#threshold = X_train_errors.mean() + 3 * X_train_errors.std()\n",
    "#threshold = 3\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nu1NFVxZVMyY"
   },
   "source": [
    "Let us use this threshold on the test set. If the error of a test sample is greater than the threshold, assign it a label 1 (attack), otherwise, assign it a label 0 (benign). We use `y_test_pred` to keep our predicted labels and compare `y_test_pred` to `y_test` (the ground truth label we have) to compute various evaluation metrics (such as precision recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = X_test_errors > threshold\n",
    "y_test_pred = []\n",
    "for is_anomaly in anomalies: \n",
    "    if is_anomaly: \n",
    "        y_test_pred.append(1)\n",
    "    else:\n",
    "        y_test_pred.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the precision, recall and accuracy of the test set on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold=8.00, precision=0.9646143380553998, recall=0.6421795299613863, acc=0.8611495995117195\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_test_pred, pos_label=1)\n",
    "recall = recall_score(y_test, y_test_pred, pos_label=1)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('threshold={:.2f}, precision={}, recall={}, acc={}'.format(threshold, precision, recall, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNSG-HhgVMyZ",
    "outputId": "ed686842-f875-4226-a4bf-2b1fba6933ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold=8.00, precision=0.9646143380553998, recall=0.6421795299613863, acc=0.8611495995117195\n"
     ]
    }
   ],
   "source": [
    "#threshold= 1.9\n",
    "z = zip(X_test_errors > threshold, X_test_errors)\n",
    "y_label=[]\n",
    "error = []\n",
    "for idx, (is_anomaly, X_test_error) in enumerate(z):\n",
    "    if is_anomaly:\n",
    "        y_label.append(1)\n",
    "    else:\n",
    "        y_label.append(0)\n",
    "    error.append(X_test_error)\n",
    "precision = precision_score(y_test, y_label, pos_label=1)\n",
    "recall = recall_score(y_test, y_label, pos_label=1)\n",
    "accuracy = accuracy_score(y_test, y_label)\n",
    "print('threshold={:.2f}, precision={}, recall={}, acc={}'.format(threshold, precision, recall, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYeZt7CqVMyb"
   },
   "source": [
    "Let's just plot the data points. The normal data is marked green and the abnormal data is marked as red. The current threshold is shown as blue horizontal line. As there are close to 350,000 data points, it will be difficult to visualize on the graph. Let's us just plot first 1000 data points to see how the errors of each data point are distributed and how are they compared to the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7AzjBWTVMyb",
    "outputId": "050058d7-7bcf-48ab-c9bd-19e2bbdb8285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y counts 0    644\n",
      "1    356\n",
      "Name:  Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAHwCAYAAABdQ1JvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdfXxcZZ3///eVNrRNgSDlZlFowsPvIqiRAom3IJ0tKkVUClKrIRRwDZZlBUGrpb+U0m6WJUtK4aePCrta3Ta2VOXGiuxvpRnkC4tuUkCKi3dIU1GWmyoVtkUaev3+ODnNmcmcuT1nzjkzr+fjMY/JnMzNNZPJmfe55nNdl7HWCgAAAIC/hqgbAAAAAMQdoRkAAAAogNAMAAAAFEBoBgAAAAogNAMAAAAFEJoBAACAAgjNAFDnjDHLjTHrx36eaYx5xRgzKep2AUCcEJoBoAqMMfcbY/5kjJkSdVvysdbusNYeaK19Peq2AECcEJoBIGTGmFZJp0mykj4aaWMAAGUhNANA+C6U9BNJ35S00N1ojPmmMearxph7jDEvG2N+aox5s+f37zXGDBljdo2dv9fzu/uNMf9gjPnPsXKKzcaYGcaYAWPMn8eu3+q5/s3GmN+N/W6rMea0XA01xrQaY6wxZvLY5WZjzNeNMc8aY34/9piTxn73f4wxPx5r34vGmNuDfdkAID4IzQAQvgslDYydPmSMOdLzu09Kuk7SGyT9RlKvJBljDpV0j6RbJM2QtErSPcaYGZ7bLpDUJelNkt4s6WFJayUdKulJSdd6rjskadbY774t6TvGmKlFtP1bkkYl/R9JJ0n6oKS/HfvdSkn/Mdb2oyX9v0XcHwAkEqEZAEJkjDlVUoukTdbarZKekvQpz1XusNb+l7V2VE6onjW2/cOSfm2tXWetHbXWbpD0C0kf8dx2rbX2KWvtLkn3SnrKWnvf2H19R07IlSRZa9dba3eO3Ve/pCmS3lKg7UdKmivpSmvt/1prn5d0k5ywLkl7x57bG621r1prHyz5BQKAhCA0A0C4Fkr6D2vti2OXvy1PiYak//H8vFvSgWM/v1HSSNZ9jcjpVXY95/l5T47L7n3JGHO1MebJsVKKlyQ1SzqsQNtbJDVKetYY89LY7W6VdMTY7xdLMpL+yxjzc2PMJQXuDwASa3LUDQCAWmWMmSZpvqRJxhg3HE+RdIgx5sQCN/+DnNDqNVPSv5fRjtMkfUnSHEk/t9buM8b8SU7gzed3kv4i6bCx3usM1tr/kfSZscc4VdJ9xpgHrLW/KbWNABB39DQDQHjOkfS6pLfKKbuYJekESf9XTp1zPj+UdJwx5lPGmMnGmE+M3c8PymjHQXLqkl+QNNkYs0zSwYVuZK19Vk7Ncr8x5mBjTIMx5s3GmNMlyRhzvjHm6LGr/0nO7CBMVQegJhGaASA8C+XUHe+w1v6Pe5L0FUmdyvNtn7V2p6SzJV0taaecUoizPWUepfj/5NQ8/0pOicercnqRi3GhpAMk/becYPxdSUeN/a5D0k+NMa9I+r6kK6y1T5fRPgCIPWOtjboNAAAAQKzR0wwAAAAUEFpoNsZMNcb8lzHmZ2Ojqq8b236oMeZHxphfj52/Iaw2AAAAAEEIrTzDGGMkTbfWvmKMaZT0oKQrJJ0r6Y/W2n8yxnxZ0hustV8KpREAAABAAELrabaOV8YuNo6drKSPyVlhSmPn54TVBgAAACAIodY0G2MmGWMek/S8pB9Za38q6cixaYzc6YyOyHcfAAAAQNRCXdzEWvu6pFnGmEMk3WmMeXuxtzXGdEvqlqTp06efcvzxx4fUSgAAAMCxdevWF621h2dvr8qKgNbal4wx90s6U9JzxpijrLXPGmOOktMLnes2t0m6TZLa29vt8PBwNZoKAACAOmaMGcm1PczZMw4f62F2l5I9Q9Iv5EyAv3Dsagsl3R1WGwAAAIAghNnTfJSkbxljJskJ55ustT8wxjwsaZMx5tOSdkg6P8Q2AAAAABULLTRbax+XdFKO7TslzQnrcQEAAICgVaWmGcipr0/q6JBSqfFt6bQ0NCQtXhxduwAAiJm9e/fqmWee0auvvhp1U2rG1KlTdfTRR6uxsbGo6xOaEZ2ODmn+fGnTJic4p9PjlwEAwH7PPPOMDjroILW2tspZPw6VsNZq586deuaZZ3TssccWdZtQ52kG8kqlnIA8f760bFlmgEYy9fU5Bz9e6bSzHQBQtldffVUzZswgMAfEGKMZM2aU1HNPaEa0Uilp0SJp5UrnnMCcbO63B25wdr896OiItl0AUAMIzMEq9fUkNCNa6bS0Zo3U0+OcZ/dSIln49gAAapYxRldfffX+yzfeeKOWL19e1TZcdNFF+u53v1vVx3QRmhEdbw3zihXjYYvgnGx8ewAAkRvYNqDW1a1quK5BratbNbBtoOL7nDJliu644w69+OKLZd1+dHS04jZEiYGAiM7QUGYvpNtLOTRE0Eqy7G8PUin+ngBQRQPbBtS9uVu79+6WJI3sGlH35m5JUmdbZ9n3O3nyZHV3d+umm25Sb29vxu9GRkZ0ySWX6IUXXtDhhx+utWvXaubMmbrooot06KGH6tFHH9XJJ5+sgw46SE8//bSeffZZ/epXv9KqVav0k5/8RPfee6/e9KY3afPmzWpsbNSKFSu0efNm7dmzR+9973t16623Rl6eQk8zorN48cQwlUox3VyS8e0BAERu6Zal+wOza/fe3Vq6ZWnF9/13f/d3GhgY0K5duzK2X3755brwwgv1+OOPq7OzU5/73Of2/+5Xv/qV7rvvPvX390uSnnrqKd1zzz26++67dcEFFyiVSmnbtm2aNm2a7rnnnv33NzQ0pCeeeEJ79uzRD37wg4rbXilCM4Dg5Pv2AABQFTt27ShpeykOPvhgXXjhhbrlllsytj/88MP61Kc+JUnq6urSgw8+uP93559/viZNmrT/8ty5c9XY2Ki2tja9/vrrOvPMMyVJbW1t2r59uyQpnU7rXe96l9ra2jQ4OKif//znFbe9UpRnAAhOrm8JKM8AgKqa2TxTI7tGcm4PwpVXXqmTTz5ZF198se91vKUU06dPz/jdlClTJEkNDQ1qbGzcf92GhgaNjo7q1Vdf1WWXXabh4WEdc8wxWr58eSwWdaGnGQAAoIb0zulVU2NTxramxib1zun1uUVpDj30UM2fP19f//rX929773vfq40bN0qSBgYGdOqpp5Z9/25APuyww/TKK69ENltGNkIzAABADels69RtH7lNLc0tMjJqaW7RbR+5raJBgNmuvvrqjFk0brnlFq1du1bveMc7tG7dOt18881l3/chhxyiz3zmM2pra9M555yjjpjM9W+stVG3oaD29nY7PDwcdTMAAAAi8eSTT+qEE06Iuhk1J9fraozZaq1tz74uPc0AAABAAYRmAAAAoABCMwAAAFAAoRkAAAAogNAMAAAAFEBoBgAAAAogNAMAAKAod955p4wx+sUvfhFZGw488MBIHpfQDCRNX5+UTmduS6ed7QAAhPg5sWHDBp166qn7V/+rJ4RmIGk6OqT588d3iOm0czkmKyYBACIW0ufEK6+8ooceekhf//rX94fm+++/X7Nnz9bHP/5xHX/88ers7JS7cN6WLVt00kknqa2tTZdccon+8pe/SJJaW1t1zTXX6D3veY/a29v1yCOP6EMf+pDe/OY362tf+9r+x5ozZ45OPvlktbW16e67757Qnq6uroztnZ2d+v73v1/Rc8yH0AwkTSolbdrk7ACXLXPON21ytgMAENLnxF133aUzzzxTxx13nA499FA98sgjkqRHH31Uq1ev1n//93/rt7/9rR566CG9+uqruuiii3T77bdr27ZtGh0d1Zo1a/bf1zHHHKOHH35Yp512mi666CJ997vf1U9+8hMtW7ZMkjR16lTdeeedeuSRR5ROp3X11VcrexXrv/3bv9XatWslSbt27dJ//ud/6qyzzqroOeZDaAaSKJWSFi2SVq50zgnMAACvED4nNmzYoAULFkiSFixYoA0bNkiS3vnOd+roo49WQ0ODZs2ape3bt+uXv/yljj32WB133HGSpIULF+qBBx7Yf18f/ehHJUltbW1617vepYMOOkiHH364pk6dqpdeeknWWl1zzTV6xzveoTPOOEO///3v9dxzz2W05/TTT9dvfvMbPf/889qwYYPOO+88TZ48ueLn6Se8ewYQnnRaWrNG6ulxzlMpgjMAYFzAnxM7d+7U4OCgnnjiCRlj9Prrr8sYo7POOktTpkzZf71JkyZpdHR0Qq9wNvc2DQ0NGbdvaGjQ6OioBgYG9MILL2jr1q1qbGxUa2urXn311Qn309XVpYGBAW3cuFHf+MY3yn5+xaCnGUgatzZt0yZpxYrxr+CyB30AAOpTCJ8T3/3ud3XhhRdqZGRE27dv1+9+9zsde+yxevDBB3Ne//jjj9f27dv1m9/8RpK0bt06nX766UU/3q5du3TEEUeosbFR6XRaIyMjOa930UUXafXq1ZKkt73tbSU+q9IQmoGkGRrKrE1za9eGhqJtFwAgHkL4nNiwYYPmzZuXse28887Tt7/97ZzXnzp1qtauXavzzz9fbW1tamho0Gc/+9miH6+zs1PDw8Nqb2/XwMCAjj/++JzXO/LII3XCCSfo4osvLv7JlMkU6j6Pg/b2djs8PBx1MwAAACLx5JNP6oQTToi6GbGze/dutbW16ZFHHlFzc3PJt8/1uhpjtlpr27OvS08zAAAAEue+++7T8ccfr7//+78vKzCXioGAAAAASJwzzjhDO3bsqNrj0dMMAAAAFEBoBgAAAAogNAMAAAAFEJoBAACAAgjNAOpDX9/Eif3TaWc7ACCvnTt3atasWZo1a5b+6q/+Sm9605s0a9YsHXLIIXrrW98a+OPdf//9Ovvss0u6zezZs5VriuJvfvObuvzyyytuE6EZQH3o6MhcEctdMaujI9p2AUACzJgxQ4899pgee+wxffazn9XnP//5/ZcbGgrHydHR0Sq0MlyEZgD1wV0Ra/58admy8SVm3RWzAABlef311/WZz3xGb3vb2/TBD35Qe/bskeT0/F5zzTU6/fTTdfPNN+uFF17Qeeedp46ODnV0dOihhx6SJP34xz/e34t90kkn6eWXX5YkvfLKK/r4xz+u448/Xp2dnXIX5NuyZYtOOukktbW16ZJLLtFf/vKXCW1au3atjjvuOJ1++un7H6dSzNMMoH6kUtKiRdLKlVJPD4EZQGLNnh3s/d1/f/m3/fWvf60NGzboX/7lXzR//nx973vf0wUXXCBJeumll/TjH/9YkvSpT31Kn//853Xqqadqx44d+tCHPqQnn3xSN954o7761a/qfe97n1555RVNnTpVkvToo4/q5z//ud74xjfqfe97nx566CG1t7froosu0pYtW3Tcccfpwgsv1Jo1a3TllVfub8+zzz6ra6+9Vlu3blVzc7NSqZROOumk8p/gGHqaAdSPdFpas8YJzGvWTKxxBgCU7Nhjj9WsWbMkSaeccoq2b9++/3ef+MQn9v9833336fLLL9esWbP00Y9+VH/+85/18ssv633ve5+uuuoq3XLLLXrppZc0ebLTp/vOd75TRx99tBoaGjRr1ixt375dv/zlL3XsscfquOOOkyQtXLhQDzzwQEZ7fvrTn2r27Nk6/PDDdcABB2S0oRL0NAOoD24Ns1uSkUpRogEgsSrpGQ7alClT9v88adKk/eUZkjR9+vT9P+/bt08PP/ywpk2blnH7L3/5y/rwhz+sH/7wh3r3u9+t++67L+f9jo6O7i/RKMQYU9ZzyYeeZgD1YWgoMyC7Nc5DQ9G2CwDqxAc/+EF95Stf2X/5sccekyQ99dRTamtr05e+9CW1t7frF7/4he99HH/88dq+fbt+85vfSJLWrVun008/PeM673rXu3T//fdr586d2rt3r77zne8E0n5CM4D6sHjxxB7lVMrZDgAI3S233KLh4WG94x3v0Fvf+lZ97WtfkyStXr1ab3/723XiiSdq2rRpmjt3ru99TJ06VWvXrtX555+vtrY2NTQ06LOf/WzGdY466igtX75c73nPe3TGGWfo5JNPDqT9pthu7ii1t7fbXPPuAQAA1IMnn3xSJ5xwQtTNqDm5XldjzFZrbXv2delpBgAAAAogNAMAAAAFEJoBAACAAgjNAAAACZCEcWhJUurrSWgGAACIualTp2rnzp0E54BYa7Vz5879qw8Wg8VNAAAAYu7oo4/WM888oxdeeCHqptSMqVOn6uijjy76+oRmAACAmGtsbNSxxx4bdTPqGuUZAAAAQAGEZgAAAKAAQjMAAABQAKEZQHD6+qR0OnNbOu1sL+U6AADEDKEZQHA6OqT588dDcTrtXO7oKO06AADEDLNnAAhOKiVt2uSE4EWLpDVrnMupVGnXAQAgZuhpBhCsVMoJwytXOue5wnAx1wEAIEYIzQCClU47vcc9Pc55dv1ysdcBACBGCM0AguPWJ2/aJK1YMV6G4Q3FxVwHAICYITQDCM7QUGZ9slu/PDRU2nUAAIgZY62Nug0Ftbe32+Hh4aibAQAAgBpnjNlqrW3P3k5PMwAAAFAAoRkAAAAogNAMAAAAFBBaaDbGHGOMSRtjnjTG/NwYc8XY9uXGmN8bYx4bO50VVhsAAACAIIS5IuCopKuttY8YYw6StNUY86Ox391krb0xxMcGAAAAAhNaaLbWPivp2bGfXzbGPCnpTWE9HgAAABCWqtQ0G2NaJZ0k6adjmy43xjxujPmGMeYNPrfpNsYMG2OGX3jhhWo0EwAAAMgp9NBsjDlQ0vckXWmt/bOkNZLeLGmWnJ7o/ly3s9beZq1tt9a2H3744WE3EwAAAPAVamg2xjTKCcwD1to7JMla+5y19nVr7T5J/yLpnWG2AQAAAKhUmLNnGElfl/SktXaVZ/tRnqvNk/REWG1AHn19UjqduS2ddrYDAAAgQ5g9ze+T1CXpb7Kml+szxmwzxjwuKSXp8yG2AX46OqT588eDczrtXO7oiLZdAAAAMRTm7BkPSjI5fvXDsB4TJUilpE2bnKC8aJG0Zo1zOZWKumUAAACxw4qA9SyVcgLzypXOeRiBmTIQAABQAwjN9SyddnqYe3qc8+xwG4Q4lIEQ3AEAQIUIzfXKDa+bNkkrVoyXagQdnL1lIMuWjT9mNctA4hDcAQBAohGa69XQUGZ4dcPt0FDwj1WNMpBCjx91cKe3GwCARCM016vFiyeGxlTK2R60apSBFBJ1cKe3GwCARCM0I1zVKgMpph1RBvc49HYDAICyEZoRrmqWgfiJS3CPurcbAACUzVhro25DQe3t7XZ4eDjqZiCp+vqcMghvSE2nneAedDlKvsdySzSYFxsAgNgyxmy11rZnb6enGbWv0vptdxCfdzCf97J3MJ9f7fLkyfHo7QYAAGUhNAOFuEHYDb6rVmVe9g7m86tdHh2NvkwFAACUjfIMoBhuj/HcudL69dIFF0j33utfYrFsmVO73NPj9CwDAIBEoDwDqIQ7iG/dOunUU51zv8F8Uc/UAQAAAkdoBorhBuGuLunBB53zXIE4LjN1AACAQBGagULcILxkiVOSceONzvmSJRMDcRym2AMAAIGjphkoxJ1Gzp02LpXKnEYujKnrAABAJPxqmgnNAMpXzTmwAQCoAgYCAgie37zU3mn4AACoAZOjbgCABPPOS81KhwCAGkZPM4DKuNPxrVzpPw0fAAAJR2gGUBnmpQYA1AFCM4DyMS81AKBOEJoBlI95qQEAdYIp5wAAAIAxTDkHAAAAlInQDAAAABRAaAYQnr6+iYMC02lnOwAACUJoBhAeVgwEANQIVgQEEB5WDAQA1Ah6mgGEixUDAQA1gNAMIFysGAgAqAGEZgDhSfqKgQxkBACMITQDCE/SVwz0G8j41FOEaQCoM6wICAD5uEHZO5BRGu9BT6Uye9Sp2QaARPNbEZDZMwAgH+9Axp6e8VDMrCAAUFcozwCAfPwGMjIrCADUFUIz4o/BWIhKvoGMzAoCAHWF0Iz4Y1U5RMVvIOPGjcmeFQQAUDIGAiIZcg3G4utwRKWvzzlo874H02knZC9eHF27AAAV8xsISGhGcixbNj4Ya8WKqFsDAABqkF9opjwDyUD9KAAAiBChGbExsG1Aratb1XBdg1pXt2pg24Dzi6SvKgcAABKP0IxYGNg2oO7N3RrZNSIrq5FdI+re3O0E56SvKlcLmMEEAFDnCM2IhaVblmr33t0Z23bv3a2lW5Y6A6uyB/2lUuEPuCIojmMGEwBAnSM0IxZ27NpR0vaqICiOc3v35893BmSefba0ZMnE2SPq8YACAFAXCM2IhZnNM0vaXhXZQdGtq67Xqe68K+Cdd550/fUcUAAA6gahGbHQO6dXTY1NGduaGpvUO6c3ohaNYankcd4ZTO691+lp5oACAFAnJkfdAECSOts6JTm1zTt27dDM5pnqndO7f3tksqe6S6XqMxh6ZzBxX4P586W5c8fnzq7H1wUAUDdY3ATwkx0Usy/Xk1wr4K1a5fQyX3UVqzQCAGoGi5sApWKqu3HZM5ik005N8+bNzJ0NAKgL9DQDKF2unud02jmgCHsqQAAAQkRPM+KvludFrrXnFtXc2QAARITQjPio5XmRa/m5AQBQBwjNiA+/eZGHhpLfS8ucz9GqtZ5+AEDVEZoRL7nmRa6VXlrmfI5OrbyHAACRITQjXrLnRU6na6eXNtdzQ3XUynsIABAZFjdBfPgtoOFedntpk7iQRqHnhvAl/T0EAIgUPc2Ij3zzIie9l5Y5n6OXTjsLsnR1Zb6HqG0GABSBeZoRf6zMh0q575klS5xFWbLPeS8BAMYwTzOSK669tMzIkBzue+iqq5zz66+X5s51vrkgMAMAikBPM1AuesCTa9my8drmFSuibg0AIEboaQaCxowMyZT0+ngAQCQIzUAlmHs5WbzfBqxYMX7QQ3AGABRAaAYqQa9lssS1Ph4AEHuhhWZjzDHGmLQx5kljzM+NMVeMbT/UGPMjY8yvx87fEFYbgFCF2WsZ1SDDWh/cuHjxxG8DUilnOwAAeYTZ0zwq6Wpr7QmS3i3p74wxb5X0ZUlbrLV/LWnL2GUgecLstYxq2WeWmwYAIKeqzZ5hjLlb0lfGTrOttc8aY46SdL+19i35bsvsGQhNX58TCL29j+m0E3yj7n10A+uiRU7pR7UGGUb1uAAAxECks2cYY1olnSTpp5KOtNY+K0lj50dUow1ATnHuWY1qkCGDGwEAmCD00GyMOVDS9yRdaa39cwm36zbGDBtjhl944YXwGoj6Fudp46IaZMjgRgAAJpgc5p0bYxrlBOYBa+0dY5ufM8Yc5SnPeD7Xba21t0m6TXLKM8JsJ+qct2e1pyc+gdkb4FOpcAO9W6YijT+OJD33XLwOJAAAiEiYs2cYSV+X9KS1dpXnV9+XtHDs54WS7g6rDUBR4tizWo2p0bwzZbhlKqtWSeee62ybP19asIAp2QAAUIgDAY0xp0r6v5K2Sdo3tvkaOXXNmyTNlLRD0vnW2j/muy8GAiI09bwUdvZzXbVK+sIXpAsukO69tz5eAwAAsvgNBAytPMNa+6Ak4/PrOWE9LlCSfD26tR4YvfXc7kwZF1wgrVsXnzIVAABighUBkTxBLsBRz4tduK+XW889d650993SnDnxKVOpllpf1AUAUDFCM5InztPEJUlHhzRvnnTzzVJXl9PD/Prr0tKlwa5umAS8pwAABRCakTxxniYuaayVjJH+8Adp+nRp0iRnexgDD+OM9xQAoABCM5KJBTgqNzQk3XWX9LnPSVu2SFdd5Vx2g3K9lKm4eE8BAPIgNCOZ4jhNXNK4gdj7Onq31xveUwCAPAjNSB7vVGkrVtRf/W1Qongd4zrgjvcUAKAAQjOSpxoLf/iJa+grRxSvY1wH3EX5ngIAJEJoi5sEicVNEBv1vBhKUNzXzJ0bmtcOABAjfoub0NMMlIJZFiqXPeBuaKh2eu8BADWL0AyUilkWKpM94G7y5HiWbAAA4BHaMtpAzcoOfakUwblY2eUsqZRzeckSSjYAALFGTzNQinJmWailwYOV8htwNzoaz957/nYAgDGEZtSvcgJRObMsxHXGiCgsXjwxEKdSzmsRxzmS+dsBAMYwewbqVzVnwmDGCH+V/B36+pwA671eOu0cxAS1SAt/OwCoK8yeAWSr5kwYUQ0eTEJ5QSVzJFejJ5iBnwAAEZpR76oViKJaormcUFntoO1XslFMT3E1DnxYXhsAIEIz6p1fIAoyOIa1RLNfG886a3y7GyrnzZPOOKO4UJm0Ot4wD3xYXhv1JgnfTgERITSjfuULRE895QRNb3CcN8/ZXqpKl2j2+xB76qnc4dYNx97bvPaatGVLcaEyaQu4BN0T7H293b+du53ltVHrknbQDFSTtTb2p1NOOcWiTtxwg7WDg5nbBged7dV8rMFBaw8+2NrmZmt7epzzgw+eeP1qGBy09rDDxh/be9n9uacn93V6ejKfh/c6hfT0WCs559VS6t8/32tTrjDuE0gSv/0KUCckDdsceTTyQFzMidBcR+IUWAYHrZ02zfk3mTzZ2v7+ib/3hrkbbrC2uzuzrYODzrYbbqjsgCDfh5hfuHW3NzWV/npG9aFZ6t/f7zWdO7eygy9CA+pdFAfNQEwQmpEcpQSWMHumBwedwClZO2WK01ubL8x5e6fdXuDsy5UcEOT6ECvU0zxnzsQe8kKvT9QHLkEE1u7uzOft/i26uwvedP3j623LTS12xftlrWQfv/Sc0h8fSDIOGlHnCM1IlmJ7OcIKeIOD4yUZbonD9OmFyxzccNbU5PRSe4O2t32lfhjlup3fc+/vr+w1qWaJjJ9Ke7ly/f2y/xY5rH98vW3qbbKzF8o+3yR73fud8x/96zXltQNImqgPmoEYIDQjOUoNlmH0ivj1VJ58cuEw5wY+v+sVCoTZoTW7l9R9vtmlIO7vKi1NiFpQf0/vNwXeEpU8Wm5q2R+YZy+U1XLn/MXpDYQG1Ic4HDQDESM0IxnK7eUIuv4u1wdHf7/T2xx2T3P2c+7uzn0/tfghFmQvl7cmfdo0/4MMz+tolhv7xTPGA7/vMiIAACAASURBVLN7Si1Ubb7eAIAJCM1IhnJ6OapRf1dMmAuyprleawqD6uXyvvbu7CdNTQXr0ltuaskIy+6p5aaWyp4XACAx/EKzcX4Xb+3t7XZ4eDjqZiCOvHMtp1ITLwelr8+Zp9R7n+m0M1+vu3JdX58zd/KCBePXS6eljRulN7/ZuVzoPryWLXMW7OjpceaRRvEuvVS6/XbpzjvH3xfz5kmnnSb95CfOfNVr1kx4nwxsG1D35m7t3rt7/7amxibd9pHb1NnWGcUzAQBUmTFmq7W2fcJ2QjNiL19glUoLoknhhv9Fi6T+fic8X3VV5u/Dfo7FHCjEVb62v/JK3oORgW0DWrplqXbs2qGZzTPVO6eXwAwAdcQvNE+OojFASdwVqortTU6l4rt6XTGyn98hh0hf+ILzu9FRafJk6frrx1eqCyvI5nvd4y7Xa+G+J+bPH189MMd7pbOtk5AMAJiAZbQRf0lb1rlS2ctuX3WVdOONznN/4gknQC9Zkhlkw1jiNi6vu98y4n19pd1PvmXTAQAogJ5mJEMq5ZQquF+r12JgdksKvL2k3l7kl15ynn9Xl9PT/NJLOetyAxWH1z2oHu/sgxH3oGBoqDbfTwCAQNHTjGRIp52A6H6tXou9g244dJ+btxfZ+/zvvVeaO9cJsosWhRv44vC6B9XjvXjxxNukUvGvzwYQnaC+6UJNIDSjusrZAdXL1+p+4VDKfP5Llkjr1zs9zmEG2Ti97kNDEw8U+OACELZ8nRmoO4RmhCs7JHd0OFN/XXqpczmdls4+2xnc5uUNRPm+Vq813nIINxx6n3867ZRm3Hij9Pa3hxtk4/S6T56ceaCwahUfXADCV82xHfRqx1+uyZvjdmJxkwTLtaCHd9GJww5zVtoLahW4pCu0qEk9LnHrvibu+6Sry1pjnMsAUA1BrzqbS5AroqIiYkVARCZXEMzeAdXrCnhe7DBz8x4ouO+brq7aPlAAEB/V/HziszAW/EIz5RkIX3bJgTRxcFmusoR6E6dyiDhxB/BlD4akNANA2Ko9toPPwlhjRUCEz7u63c03S8ZkLm88f74zuO36632XN0adq9Zy6XGU5JUZgaSr9v+f9/OSz8LI+K0ISE8zwpV9lL5ggeQ9UEulnMC8bFk8ZmmoB3EabFJsW+q5F57R+0B0qjlVZZxmLEJOhGaEKzvs3HqrdNddmWFndFTavLk+A1EU4hTCnnpKOueczLacc46z3aue51iOy8qMAMJVz50DCUF5BlCP4vIVYDrtTEForXTFFRPLd6Rwvh5NYsnDsmXjKzOuWBF1awCgZlGeAWBcJYNNgizvSKWcgDw66rRldDQzMEvB9oy7bffeZzrtzBse55KHOKzMCAB1jtAM1KNSQlgxC9RUGjjdb7xyffM1NOTUvXvLE5YsKe8rSzcsS07v+rx50oc/LN1+e3xLHqhzBIBYIDQD9abUEJbd0ys54fb22yuvsXVrmA84wAnwBxyQWePsPv71148voz13rnO5nJDurQ9Op6XXXpP27JE+97mi2z+wbUCtq1vVcF2DWle3amDbQOntKAV1joibKAcTx2kgM+pPrsmb43ZicRMgQOWsKljMAjXl6O52Vof0LujS3Oxs9+rvd1YBPO20YFYDdNve1OT8PH36xPvM8Zqsf3y9beptslqu/aem3ia7/vH1lbUHSJIoF2IK8rHrcYVVFEWsCAigIt6QnB2iu7vL+/Ap5kPLfayurvHVACv5gHaXcm9qcs4HB8dDuRucfT6I//HsQ+zshU5Y/uIZsrMXOqd/PPuQ4p8zUAuiXLkuqMdmFVb4IDQjvjjajz/vh9TBB+fuHXYDqPf6QXz43HCDE2a9H5L9/eW9P9x2uSHf287+/vGeZ5+2pxbKPt80Hpb/NMU5pRaKD1zUnyC+bYr6sVm2GjkQmhFfHO3HW/bfo7s7MyC71+nuDufDp5pfxxb4IG65qcXOHgvO171/PDSv/mAz71nUl1roaXZFGf4RS4RmxJvfTpBe6OiV8jco5sOn1L9ptd4DRXwQuzXN171f1soJzv+YmswHLupLrdQ0e29PTzM8CM2Iv1yBK+ydcyWBjECfqdgPnzh+s1BCm370r9fYF6c32BXvl9011di/HDiND1zUlyj3fUE+dhz3RYgFQjPiLV/gCrMnoJKdJjvccaW+FnHr3Sn2g9j7vNwBhW59dz3//YEkouMDPvxCM8toI3reeYNTqYmXpcwlhA88MNglkCtZUjouy1FHwbsUtfuzNP53KPQ3SeKy0JU+ZwBA7Pktox15L3IxJ3qaa1yho/3sXkl3JoUge3grGQhS7G1rrVcjiF76uPQ0AwAwRpRnIJH8gln2FGTFhq5cwbW/31ncopwAl28AY/bcxf391k6dOr5wRy18nV9O+KWsBQAQY4RmJFO+3tlyeoezA1qRi1oUdV/Z9a7euYvdy01Nznm1e1jD7OUu9e9QbltqraceABBLhGbUlkq+3vfetqmpqOWTcyqmrKS52dpp0zJXn4tiTtCwenerWWZBDzUAoAoIzagdQYSncoNrqb2d7uP4LT9dzcAX9GNHEWKphQYAhMwvNDdUdzwiEIChocxZKlIp5/LQUHG3T6edmS56epzzdLr4x+7ocGbLcG/jzp7hzqLg6uuTVq2SbrlFmjZNampytp11ltPWFSucc+99hS2Vcmb5WLnSOa90lo9K/w7lCPo5AABQpIKh2RgzyRjzz9VoDFCUxYsnhqVUqrhpvrzT2ZUTXN1gOH++M2Va9tR4rsmTpS98Qdq7V7rnHifk/eUvTp+z5ARoKTNkptPj28NQycFCLtl/B7ft3r9D0M8p6OcAAECRCoZma+3rkk4xxpgqtAfI1Nc3MRhVEsSC6B319naeeOLE36fT0n33SR/+sNTY6Fy+/nqpv19auNB5LLfHWhqf3zdXj3VQKj1YKEaxvfDlqsZzAADAT66ajeyTpH5J35fUJelc91TMbYM4UdNcx+I4+MtbV+udISNX+/LVTlezPrdaM0+E+ZyYPQMAUAWqZEVAY8za3HnbXhJcfPfHioB1Lk6r7uVavXDePKfs4oorMttXTLuTuCpeIbX4nAAAdcNvRcCiBgJaay/OcapKYAaqMvir2DKQXOUdd97plCB421dMKUEt1ufW4nMCAEBFhmZjzNHGmDuNMc8bY54zxnzPGHN02I0DJFUniBVbj5trEKIk/exnme0rVDtdi/W5tficAAAYU+yUc2vl1DS/UdKbJG0e2waEq1pBrNhZMYptX0dH/hk+opiuLWxxfE5BDyQFANStYkPz4dbatdba0bHTNyUdnu8GxphvjPVMP+HZttwY83tjzGNjp7MqaDvqQTWDWL4yEL/w9c//XF77Kpk2D8ULe0YPAEDdKDY0v2iMuWBszuZJxpgLJO0scJtvSjozx/abrLWzxk4/LKWxqEPVDJf5ykD8wtcXvxh9+I2qNzX7cTs6pHPOkS69dLwNUQfUcr9BAAAgy+Qir3eJpK9IukmSlfSfY9t8WWsfMMa0VtI4IHR9feOhzg1UkvTcc5kByxu+4jCLh5cb6LNn7XCfSzUed2jIWdDFGGnjRunII6Wbb5YWLIj+NfJ+g9DTE317AACJVNSKgJLOs9Z+1Fp7uLX2CGvtOdbakTIf83JjzONj5RtvyPO43caYYWPM8AsvvFDmQwEFuMFv48bxkDl/vhP2ssss4rqEc77e1DB7ob2P+8QTzgqIy5Y5U++tXCnt2SO95S2ZbXAfu5p1xczoAQAIQq7Jm7NPku4v5no5btcq6QnP5SMlTZIT1nslfaOY+2FxE4TCXSzDuyDHwQdb292d+/rVXIykHLkWUqnG4jDu43Z1OYu9TJtmbVOTc2putra/33nM7PNqvH5xXBwHABBr8lncpNia5oeMMV8xxpxmjDnZPZUR0J+z1r5urd0n6V8kvbPU+wAC413K2u1BHh11epmzxX06Nb/e1LB7ob2P+/3vO73Le/ZIV18t/eAHzqIvK1ZIc+c6PdFz5zpLilertMUdSDo05LTVO1CTWTQAAKXIlaSzT5LSOU6DRdyuVZk9zUd5fv68pI3FPD49zQjN4GBm76h3SWyvsJZwDuJ+i+lN9euFbmpyen69t+vvL+7xsx/n7LOtNcbpcXa3Dw5aO2eO89innea/pHjY6HEGABRJPj3NxQTmBknzC10vx+02SHpW0l5Jz0j6tKR1krZJelzOvM9HFXNfhOY6F0Zg9ZZmTJs2Xl7Q3V3dMBVEmCv0+uQrK+nvzwy6pZROeB83O3B7Lx92mHP/2YE6KMW+P/xeh7AOiAAAiVR2aHZuqweKuV5YJ0JznQuqlzBXyHv3u62dOnU81PX3Vz8whVkrXcxr19U13hNc7uPnCJ7DX/iU/d8DjP38B2VfnN5gh7/wqXBqmkt5f0RV9w0ASIxKQ3OPpC9IOkbSoe6pmNsGcSI0I5BgmR2GFi1y/gU+8IHCYS7s3shcYS4IxfZCu6UTXV2BPOwjVy6w/9oxyc5eKPvFM2RnL5Q989NT7K/OS423KcgDk2LeH/muE/dBngCAqqk0ND+d4/TbYm4bxInQDGvtxGBZTpD1hqOmJicwe+/T7/Zh9kZGFdj8SijcGucKLLjsSPt8kxOWtdw5f75JdsFlRwbQcB/5DjzKrfsGANSdikJz1CdCM3IGy3KDrHeKtFLCahjhNsrSgBtusPbDH84c/Njf75Sr+E27VySz3OwPyte9X/sDtFluAmh4DoX+NpXUfQMA6kpZoVnSYs/P52f97h/z3TbIE6G5zuULlqWGHff62b2qpQbuoHojox6E1t3tzB7ifW2bmysOzS03tVgtdwKzlXOu5bItN7VU3uZslR54UNMMAPAoNzQ/kuvnXJfDPBGa61yhYFlskPWGoRtumFjDXEppRy2FqhCe1/rH19szPz0lo6f5zE9PsesfXx9Ag7NUeuAR9YELACBW/EKzcX6XmzHmUWvtSdk/57ocpvb2djs8PFyNh0LSuIuOLFrkLLKRb9GMvj5nQRPv79NpZ6GLxYuLexz3/rMvJ92yZc7iLj09zmIklUqn9ep5H9PFn2zS7Uc8r088f4TWbtitqd+7u7zXq5K/HQAAJTDGbLXWtmdvL7QioPX5OddloLpKXaVv8eKJgS2VKhy6+vqkjRszV9J79FHp3HOd0Oa2Jamry/mtJliJoSFN/d7d2vDV/9G+a/dpw1f/xwnM7utVKnf1Rrdt7t++o6PytgIAUIRCoflEY8yfjTEvS3rH2M/u5bYqtA+1KoglnN0lkt0g7F0iOUgdHdIdd4xfnjzZWRL6LW9xAneSA1xYy4OXe4DiJ99y4JUI4n0IAKgLeUOztXaStfZga+1B1trJYz+7lxur1UjUoHJ6DrMDjhvAvAGnkmDmJzuwXX+9dOONznmQAS4K1TrwCEIq5ZThrFzpnAfxetODDQAoVq5C57idGAhYo8qd+SKqWQ6yBxwyr291hTUQs1YHeAIAyiKfgYCFyjOA8JTacxjWV/TFyK77XbUq+Dpg+JdLXHppOGUkUjg92ACAmkNoRnTKGYAWRcDJrvtdssSpaV6yJPgAV+/8yiWk8MpIwhgICQCoOYRmRKPcAWjVDDhur6e37jedlu67z6lpHh11rhfnOuBKVXugnN+3CbfeGuzAQldYAyEBADWH0IxolDMArdoBx+31dOcHdh//i1+UrroqM7CFMQAxDqIYKFfNbxOSNBASABCpvIubxAWLm0BSNAtceBdPWbXKCetXXVW9x4+DUhaQqcbjsdAJACBE5S5uAsRH0HP/FsPb63nuuc40c/U2PVk1e36L+TYhiN5v5mcGAJSI0Azk462hvvdeZ/BfFLN3RKmadeTFlEsEMYsK8zMDAEpEeQbgx9vr6a1pnjtXWrfOCZErVkTdynD5vQZxOFhYtszp/S7371DtshMAQCJQngGUKlev55Il0ve+Vz/Tk8V1oFwQvd/MzwwAKAE9zUCx4tzrWk+C+jvQ0wwAyIGeZqBSce11rTdB/B2YnxkAUCJ6moFqYrq0eODvAADwQU8zEAfM2hAPUUxfCABINEIzUE1+06UNDTFvcDbmUgYAxAihGai2XLM21HMPtF84fuqp+n1N6tjAtgG1rm5Vw3UNal3dqoFtA1E3CQAkEZqB0gTR+5lrurQgFuyIk1JeJ78DhgUL4v+a0BseqIFtA+re3K2RXSOyshrZNaLuzd0EZwCxQGgGSlFpj3C+WRtqad7gUl6nfAcMcX9N6vkbghAs3bJUu/fuzti2e+9uLd2yNKIWAcA4QjNQikp7hPNNl1bN5arDVurr5BeO4/6a1No3BBHbsWtHSdsBoJomR90AIFD5phKTgplmzBvwenpKC0i5Hse9fXYPa9IDWCmvU3Y4TtJrUsn7ARlmNs/UyK6RnNsBIGr0NKO25Pu6PKiv0sPo/azFhVOKfZ2yS1bOPVeaN0/auDFz1T/3NYrbaxL33vAE6Z3Tq6bGpoxtTY1N6p3TG1GLAMDDWhv70ymnnGKBnG64wdrBwcxt/f3WTp9ubU+PtYcdlvn7wUFnW67fFcO9vXu77MtwlPI6Zf8NBwetPfhga7u7C982arwfArf+8fW25aYWa5Yb23JTi13/+PqomwSgzkgatjnyaOSBuJgTobkG5Qq7g4PO9lL4hZauLuft3dMz8TY9Pf6/q1a7a12lr1O5BzfV/vvwfgCAmkNoRrwE2UOXHbD6+/0DV6U9zaiecg5uKnlfEYABAJbQjDgKMsC6Aauryz80BRXUCVflKeV1q+S9Ue5tKbUAAFj/0MxAQEQnqDl4vQOxvvc9acmS3APqghpsx9y85Sn2dcs3l3UxvO+rE0+c+Hu/xUeYPg4AkE+uJB23Ez3NNSqInuZq9A76DTZsaqLMo1TF/M2DrIdubnYGFZby/qik5h0AkHiiPAOxkqRSiXIGG8JfmKE019/KDc49Pc6sKv39E2/jvl+oeQeAukdoRrzEoS64khrbfIMN4W9w0Omhz1V7HsTf3u9vOmeOrVrNOwAg0QjNQLZSQ1Ixgw3hz32d3AOO7POwXr9iD3jicCAHAIicX2g2zu/irb293Q4PD0fdDNQid9DZokXOYEK/gV/e661a5QxQu+qqzN+Xuhx3vfEuce6+nnPnSnfcIW3eHM6AO++gwuzHXbfOGTy6YkXwjwsASCxjzFZrbXv2dmbPQH0rZgaP7NkcNm+Wrr8+czaHVKp+A3Nf38SZLXLNULF4cebMJYsWOcH1qqvCm6Ei14wpS5Y4s6yw7DUAoASEZtQ373R1fgEqqKnqalU5U/AV87oHwRvU3ce9/nrpBz8obzo7r2IPFgAANYHQjPpV7HzA2cFLqu+e5Wylzm9c6TzMlQjyAIj5ugGgrlDTjPrlrbF1UZtcvmXLnDKXQnXCtfS6F1sTDwBIDL+aZkIzgMrVc3gs9mABAJAIDAQE4izq+thKHj/KcouoVas2GwAQOUIzEIXskNrRIZ1zjnTppc7latfHVlKfW68DJev5YAEA6hChGYhCdkiVJGOkjRuLG0wXtFIH87lqqT65VPV6sAAAdWpy1A0A6pI3pLp1wHfe6QROtz622jXB3jmrvY+fLxi74T978ZBNm6rb9ijkOihIpeqnlhsA6gw9zUBUshdWkaKtj/Wrz81XulFuD3UUoq4bBwAkGqEZiIo3pN5yi1PTHFV9bL763ELBuJhVFeOgmLptgjUAwAehGYhCdkj9xCecmmZXtetjC9Xn5gvGcZ9Bwg3C3vB/4YXS2WdP7BVnwRIAgB9rbexPp5xyigVqyg03WDs4mLltcNDZHkeDg9Yedpi1PT3Oudt2d7vf5SBU+lplt6mry1rJOc/1GO71u7qsnT492OcCAIg9ScM2Rx6lpxmIQpKW5nZ7W889N7O3Np12Zvtwt0vh9JBX2vub3cO8fr3U1SXde2/uuu1USpo7V1q3LvO5AUBUKB2LBUIzEKUk7Ajd0o0FC5xgKTmXN26U7rjD2e59Hm74D+p5BDHY0BuEL7hA+rd/86/b9gvWABAVSsfiIVf3c9xOlGfERNxKCuLWnnJUo7whSFGWafT0OGUVPT3ltXv6dKfkIrud3vdLdulG3P8eAOIj7M8kv/0vAief8ozIA3ExJ0JzTMStfnVw0NrGRmsXLcpsz6JF1s6dG1ybwpa0HaFfeA3zeVRy38W+bwcHrW1qKhysASCXuHceoGiEZgQj7IBX6k5n0SLnbfyBD4wHZmOs7e8Ptl1hK2FHuP7x9bblphZrlhvbclOLXf/4+io0cEyhv38YO/RKP4iKORBLWo8/gHiKa+cBSkJoRnDCPtItdcfwgQ847TnmmHgF5mJ7zUt4vusfX2+bepuslmv/qam3qTrBuVCwDGuHXo0ynFoo9QEQD3HsPEBJCM0IRrWOdIvd6bjtOeYY5/ptbZU9bpDhqZidXIk7wpabWjICs3tquaml9PaV6oYbrO3untj+7m7nxA4dQL3z+4ys9LOFA/uqIjSjctU60i120Ja3htkYJzBL4zXO5T52kM+x0EFGiTtCs9zkDM1muSmvfaXye32yw7T7O3boAOpFvs8PeooThdCMylXjSNfdkfT35z73Pv4NN0ysYXZrnCsp0Qi6Nz3Ar+oi7Wl2UVcHABMV+oxk35kYhGYkg9/KbE1NuXcwc+dODMj9/ZXPnhFU0A14JxlpTbMXI7gBoHTsOxOh6qFZ0jckPS/pCc+2QyX9SNKvx87fUMx9EZrrWBQ7mKCCbkhfx0U6e4a19JYAQDnYdyaGX2gOc0XAb0o6M2vblyVtsdb+taQtY5eB3NJpac0aqafHOc9emS2M1fTcVZY2bZIOPFBasmTiKkzF3r+7kl7AS0x3tnVq+5Xbte/afdp+5XZ1tnVWdH8l8b4+K1ZkrqoHAMiNfWdNCC00W2sfkPTHrM0fk/StsZ+/JemcsB4fCVfMDiaMZUW9QbejQ7r+eic4Dw2Vfv+LF09c6tldYjqpQjoQAICaxr6zJhinFzqkOzemVdIPrLVvH7v8krX2EM/v/2StfUOh+2lvb7fDw8OhtRMx1NfnhFNv6EynnR2MN3S6QXbRIqc32rtTCkLY9w8AAGLFGLPVWts+YXtcQ7MxpltStyTNnDnzlJGRkdDaiYRbtkxaudIp41ixItz7P/DA4sI8AABIJL/QHGZNcy7PGWOOGmvQUXIGCuZkrb3NWtturW0//PDDq9ZAJEyhuueg73/y5OBLQgAAQOxNrvLjfV/SQkn/NHZ+d5UfH7XEW/ecSjkn7+Ww7t8dHEjJBgAAdSO0nmZjzAZJD0t6izHmGWPMp+WE5Q8YY34t6QNjl4HyhD2wwu/+R0edwLxypXNOYAYAoOaFWtMcFAYCJlyxg/qSgsGBAADUrLjUNKMehTE1XFSYaxMAgLpEaEb43LKG+fOdmSiCrDuuNubaBACgLlGegeoJe2o4AACAClGegWiFPTUcAABAiAjNCB91wEDsDWwbUOvqVjVc16DW1a0a2DYQdZMAIFYIzQgfdcBArA1sG1D35m6N7BqRldXIrhF1b+4mOAOABzXNAFDnWle3amTXyITtLc0t2n7l9uo3CAAiRE0zACCnHbt2lLQdAOoRoRkA6tzM5pklbQ8KddQAkoTQDAB1rndOr5oamzK2NTU2qXdOb2iPSR01gKQhNANAnets69RtH7lNLc0tMjJqaW7RbR+5TZ1tnaE95tItS7V77+6Mbbv37tbSLUtDe0wAqMTkqBsAAIheZ1tnqCE5G3XUAJKGnmYAQNVFVUcNAOUiNEMSA3IAVFcUddQAUAlCMxiQA6DqoqijBoBKsLgJfBc2+MfhQ7TkqjvGV/KTnKWvh4akxYur2EIAAIDqYHET+PIbePOjGS9J8+c7QVlyzufPlzo6qtg6AACA6BGa4Tvw5rezWqRNm5ygvGyZc75pU2bPMwAAQB0gNCP/gJxUSlq0SFq50jknMAMAgDpEaEb+ATnptLRmjdTT45y7pRoAAAB1hIGA8OfWMLslGdmXAQAAagwDAVG6oaHMgJxKOZeHhqJtFwAAQJXR0wwAAACM8etpnhxFY5Ji9uyoWwAAAFB/7r8/6hZMRHkGAAAAUAA9zXnE8SgHAAAA1UdPMwAAAFAAoRkAAAAogNAMAAAAFEBoBgAAAAogNNeqvr6JS16n0852AAAAlITQXKs6Opwlr93g7C6B3dERbbsAAAASiNBcq9wlr+fPl5Ytc869S2JXE73eAAAg4QjNtSyVkhYtklaudM6jCMwSvd4AACDxCM21LJ2W1qyRenqc8+ze3mqJU683AABAGQjNtcrtzd20SVqxYjy0Rhmc49DrDQAAUAZCc60aGsrszXV7e4eGomlPXHq9AQAAymCstVG3oaD29nY7PDwcdTNQLrfX+9xzpQULnG1uL7jkBPnFi6NrHwAAwBhjzFZrbXv2dnqaET6313vBAicsS87ljRsZEAgAABJhctQNQB3w9iK7tdWLFkl33MGAQAAAkAj0NKO6GBAIAAASiNCMcPgtaHLppQwIBAAAiUN5BsLhLmjill+k09K8eZK10l13OdtSKeZsBgAAiUBPM8KRa0GTT3xiPDB7rxPVNHgAAABFIjQjPNn1y7feOrFHOZViujkAiBu/Eru+vmjaA8QAoRnhufRS6ZZbMuuX2ekCsTawbUCtq1vVcF2DWle3amDbQNRNQhTcEjs3OLvz7TNFKOoYoRnhSKel2293apjdMox586Rzzkn+TpceGNSogW0D6t7crZFdI7KyGtk1ou7N3QTnepSrxI7xJ6hzhGaEY2hIuvNOp4bZ7a2w1lngJOk7XXpgUKOWblmq3Xt3Z2zbvXe3lm5ZGlGLECmmCAUyMHsGwuGtU3Z3uj090ooV0bUpKN4emEWLnNITemBQA3bs2lHSdtS4dDpzilB31iOgTtHTjHBl73RrZV5memBQg2Y2zyxpO2qY+w3apk1OZ4fbBo4ETwAAIABJREFUUVAr+/AcqOdHIYRmhKeWd7q1ejCAutY7p1dNjU0Z25oam9Q7pzeiFiEyQ0OZ36DV+BSh1POjGMZaG3UbCmpvb7fDw8NRNwOl6utz6ny9vbDptLPTzTfNXLm3qxbvwYC7cAuDZFAjBrYNaOmWpdqxa4dmNs9U75xedbZ1Rt0sIFStq1s1smtkwvaW5hZtv3J79RuESBljtlpr2ydsJzQjduIeSuMe6gEAJWm4rkFWE/OQkdG+a/dF0CJEyS80U56BcFQyLVupUx1Vewq4xYtZpAUAagj1/CgGoRnhqHRatlIG2jEFHACgAtTzoxiEZoSj0onxSxloxyT8AIAKdLZ16raP3KaW5hYZGbU0t+i2j9xGPT8yEJoRnnKnZStn1g2mgEOdYFos8B4IR2dbp7ZfuV37rt2n7VduJzBjAkIzwlPutGzlTHXEFHCoA/U4LRYBMVM9vgeAuGD2DISj0hkwSpmhIu6zbQABqbdpsdyA6F3au6mxqa6/Nq+39wAQBWbPQHVVOjF+KYP76mwSftSvelvmeumWpRmBWZJ2792tpVuWRtSi6NXbewCIE0IzArX/q9Q9X1brzy7O/MqwlGnZShncxxRwqBOVTIuVxDIHAuJETI0GRIfQjMD41do9+vlPljePMoP7UMdyhdxyp8VKah0sAXEipkYDokNoRmD8vkrtey1d3jzKDO5DnfILuZLKmhYrqWUOBMSJmBoNiA4DARGYvMuQvn+LE5QXLXICcKFBegzuQx0LerBXkpcIHtg2oKVblmrHrh2a2TxTvXN6CYgAQuU3EHByRI3ZLullSa9LGs3VMCTPzOaZOT/oZzbPzCy16OkpHHzzDe4jNKPGBV3Lm/d/M+Y62zoJyQBiIcryjJS1dhaBuXbk/Sq11FILBvehjgVdy0uZQ+1I4oBOoFZQ04zA+NbavfjG0lf4A+pY0CGXOtjakNQBnUCtiKSm2RjztKQ/SbKSbrXW3pbjOt2SuiVp5syZp4yMTPxqEQnR1yc99ZS0YMF473E6LW3cKL35zfQeAzlQy4tsLGwCVIdfTXNUofmN1to/GGOOkPQjSX9vrX3A7/oMBKye0D6oGdgHABVJ8oBOIElitSKgtfYPY+fPS7pT0jujaAcyhfrVXymLlQAAJmDeaiBaVQ/NxpjpxpiD3J8lfVDSE9VuByYKfS5XFisBEIB6HQzHgE4gWlH0NB8p6UFjzM8k/Zeke6y1/x5BO5Al9CVrWawEQIXqeTAcAzqBaLG4SYKEPTAo1EEm1DQDCACD4QCELVY1zShdNXpXQv3qL99iJQAik7RSh9C/EQMAH4TmhAi93lghf/XHYiVA7CSx1IHBcHWkr29iGV867WwHIkBoTohq9a50tnVq+5Xbte/afdp+5XZq5YAAxa1XtxoH40FjMFwd6ejIWAjrvq8v1c6PnKG/+e8vxeL/B/VnctQNQHFmNs/MWcdH7wqQDG6vrhtS3V5dSZEdnCax1MF9rVj4pQ54pirddt6pOnHdXfr4+dL9x0qKwf8P6g89zQlB7wqQbHHs1T102qElbY8LvhErT9y+6SjK2FSlbbfepTXtY4F5TNT/P6g/hOaEqJephhK5UweKkMReXYSrmvu7JNavS9o/VenK90uLhqXZT2f+mv+f4PE57I/QHAPFvkFrvXclsTt1oAhxHMD2xz1/LGk7glPt/V0cv+koyDM16dfntWj++dKm72QGZ0oUg8XncH6E5ojxBh2XyJ06UKQ4lljFMcjXi2rv7xL5TYdnqtLeOb36r+OaNP98qeP3zq+j/v+pRXwO50dojhhv0HGJ3KkDRYpjiVUcg3y9qPb+LpEHSJ6pSt3/n6dntejGU+Px/1OL+BzOj9kzIsYbdBwzhKDWdbZ1xupDnpkoolPt/V3vnN6M2Vuk5B0gBfH/E/bKuknH53B+9DRHLJFH/yGh1wuovlofKxFX1d7fxfGbDle1Bp5RDlkYn8P5EZojxht0XJx36gDCVW8j9qPY38XxAKmaQZZyyML4HM7PWGujbkNB7e3tdnh4OOpmhCZuXxfFrT0AKhP3/+nshV8kp/OgWh/WcX99alnr6tac5QAtzS3afuX2QB+r4boGWU3MPEZG+67dF+hjIdmMMVutte3Z2+lpjoE4Hf3z9RVQW6r9P11Oj3GlPYCV9FKzz4tWNcf1UA6JShGakYGvr4DaUs3/6XIDaCXBqdLQW+v7vLiXvVQzyFIOiUoRmmMg104tqh1drq/J8m0HEG/V7MkrN4BWEpz8HvOKe68oah9ayzMYJaEXvZpBlnpdVIrQHLFcO7VL7r5EF991cSQ7uklmUknbAcRbNXvyyg2glQQnv/veuWdnUfvQJH1lX2pnShJ60asdZONUDlm0vj5ndUSvdNrZjqoiNEcs107ttddf0959ezO2uT0n2YLukX7dvl7SdgDxVs2evHIDaCXBqdhw6xcWk/KVfTm9xknpRU9kkK2mjg5nOXE3OLvLi3d0RNuuOkRojlgpO6+de3Zm7CDD+OqtpbmlpO0A4q2aPXln/fVZMjIZ24oNoJ1tneqd06uZzTO1Y9cOLd2ytKh9Wa7Q6yfX/jaI16ca5XTl9BonqRcdeaRSznLi8+dLy5Y552PLi6O6CM0RO3TaoSVd37uDzLcTLXcnHkSvS9wHngD1ptyevFL+lwe2DehbP/tWxpReRkYLT1xY1OOV2wmQK/TOmDYj53X9wmIlPZ3Vqhsup9c4Kb3oKEIqJS1aJK1c6ZwTmCNBaE4Y7w7Sb2fp7rTL2YlX2uuShIEnAAor9X8510G8ldUPf/3Doh6vkvrb7NB789ybqxYWq1U37Bf4razvAQ0D32pIOi2tWSP19Djn2TXOqApCc8T+uOePJV3fu+P024kamYp24uV+TSolY+AJUIuC/oan1P/lSutng6y/rWZYrFbdcL4ylHwHNNQL1wC3hnnTJmnFivFSDYJz1RGaI5avtqyxoTHjcnZPSe+c3gnXkZRzxSOp+J34wLaBCbN3XHzXxUV9CCdl4AmQdN6QfFjfYbrk7ksC/Yan1P/lSutng66/DSMs5jowqVbdsPdAIJc4dk5QqheQoSFp0yYNHPYH5/V8YI4+OX+SHv3+bVG3rO4QmiPWO6d3wsAZ18FTDs7bU9LZ1qmDpxxc9GPl+xrP64p7r5gwe8fefXtzzt6RjYEnQPmKDRnZpRM79+zUa6+/lnGdSkNUof/l7Lae9ddnVVQSEff6W79ylUqfdyncAwG/z4w4dU5QqufhM2Xco5//ZHEHFYsXa+CwP2S8nhuPeE6nHv79+nw9I0RojlhnW6dvz/Af9/xR26/crnXnrpMkdd3RNeEfq9TyjmJ2XDv37Cxpu1fcP/iAuColZOQqncilkhCV7385V1u/9bNvaeGJC8suiYh7/a1fucoPf/3Dqs++kYTOCUr1PHJMGffqeR/TNS/fWdGYgbp9PSNEaI4Bv6/bGkyDzHVGXXd0+f5j+e0kZ0ybEcnXeHH/4APiqpQPxWLDcCUhKt//cr4AWUlJRCXjKcLiBlq/VVF37NqRtxSkUCAup0fWr775xd0vxqYUopxSvVK+aUlU2UeOKeMu/mST/v2Yv2RcLcwxAwgGoTkG/HaA7oIi2T3R3n8sv96gm+ferO1Xbvd9zHzLYvtN1+S3PRsDT2pYAlamStwH6phSPhSLCcNBfMPj978c1gd4MQGy1GnwCl0333XO+LczdMEdF+TdX+b7WxTzfMrpQexs69TCExdOKNP4373/G5tSiFJ7w4s9eEhi2cfAtgG1/uxirXzri9LKldp23qm6/Yjnc143rDEDCAahOQaye3SKWbLa/ccq1LNbzrLYN8+9WQdMOiBj2wGTDtDNc28u9imVJwGBrO7FfGWqJH6gunJ9+H3xQekTzx+RuTGd1p0j78m7oMeMaTNC/YYnrA/wQgEy19/3gjsukLnOTAi8ua7bdUdXxnXzvV8uu+cybXl6S972NjY07j8wyR6YeVjfYbrgjgsKBuJyD0B++Osf+pb2uY9TzDiUsJRaqlfswUMx16v0wDnIA2/3PXbsYyP67LC04v3SX627Sx/9w0E5r+/3P0TpYzwQmmPC26Ozz+4reH3vP1a+nt18y2L77Rg62zr1jY99IyOIf+Nj31BnW2e4vXgxD2SlSGpvZ0FBrkwV0EGS97VeeOfC+Nf9+TzvXEF4W8sUrd2we8L/xEkf7dZtH7nN9+D3wAMOLCswF/u+DesDvFCAzFfLnX2A5DdvtPe6V9x7he/75dattxZsrzFOT2/2jEM79+zMOwbE+zzLPQApplc/exXZaiq1VK/Yg4dC1/M7WLrsnsuKanfQB95LtyzVO3+1W5u+I80/X7r2b5zzfx14WWf+bkrGdfP9D1H6GA/GWv8j1bhob2+3w8PDUTejavLVz0nOP1ax/yx+9zVj2gztGd2T8YFR6H7dnUkptymZG5QXLXImcE/gUqElv059fc6Bgfd5ptPONEOLF5fegFLvr5zHX7bMWZmqp8eZN7ScNk6erFf/Ybku/mSTbj/ieV336CH68n/8rxrv+fei/+a5XutcjIz2TfunYF/ncnnnXE2lMi4PHPYHLd2yVDt27dDM5pnqndOrzhffuP9/4tWvrN7/es1snum7nzAy2ndt4YNvr1LftwPbBia2tcL9gN/+qqW5Rduv3K6G6xry9q6Wel0/Rqbo27Y0t+iV114paqB0dhul8verhT4ncj1WUKL42xd7Pb/fGxmtO3ddwXYW245iNVzXoC88aDX0Jun+Y8e3p56W+t+wQPNaHg70dUQwjDFbrbXt2dvpaY6hXL04bu1aqUeXfj1CknL2sFxx7xW+PU1VGb0bl6VCK+gFLfl1ytfDXk47CvXY9/VJl146/vuODmnePOkjHxl/vHw9/EGsTNXRoVf/YbmufferuuWbz+mbd1gtvftP+n9mWw0c9oeJ1/d5HXZcc3nGa/3FB6XZT2debfbTUu9wc97XpeJvBrJfU/f+L7104t8qT299zm+NPP8TN83ao41HPLe/B8xv6rFyyiSKKY3wvkaSAh+7UKgHu5jn5fY4VlIqUsptR3aNlBSYs3sTy+1BzLfYiVfQA8XCKoEq9tuLQtfze75WtqjPqqDr9Wc2z9Q/n5oZmCXpt7NadNJNGxj/kzCE5hjKtRNdd+462Wttyf9Yfjtkv6nqdu7Z6bszrMro3bgsFVpBqUjJr5Mbos4+W7rwwsxeyMmTnTBbSjsKlVB0dEgbNzpB2b3fvXule+6Rnngif8lFUCtTpVK6+JNN+sKP9+rJw6QLH5fWv0Pqe9fe3B9sPn+PH814KeNqQ2+SNn1nPDjPftq53HHOZb6vS/b8p2WFgI4O6fbbpXPOcdqWTkvz5um1b6/TJ0dWTQzjpRwcjv1P3PzBZv3tT0czDgr8ekPP+uuzim/7mHzv23KDUqkHI4UCZL557V1u4C0mVM6YNsM3gE1vnJ73tuXwC8TlDJ7Ofq0aTO6P86AHioXVeVLswUOh6+V7vsV8VgVdr08tcm2hPCPhBrYN6Ip7r9jf0zFj2oz9A/byfX1W7Fd7UuGvvQL7+i/P19aR9DiXWSqS93V67TL/EoEnnpDWrZO6uqR/+7fxx1+yRLr++tJLVvKVUKTTTsAbHZWslQ44QProR53Hz1dyEWApScN1DfrmHVYXPi49MFM64UWn1u/Hx/qUFuT4e7T+7OIJr7UblL/WLl22tUGP3vxlnfFpzwfU/9/eucdHVZ39/rdmMgMJwSiJ4q1E9FiPWkRLaL3VEhMvXESILYQGiGJfINFjLCivSIWiH15qFCq8NkTqLTKBCBZQbqctF8vH9lhBESm1VHkheKnQhApCArmt88faa2bPnn1Ze88kmZDn+/nkk2TP3mvWevbeaz/7Wc/FIJeEXdeaooymJoAxNPk47h7bFpVWKrzsrnO5sD2nunvAtz0PPzzAw76RRsuVse/z8ua5WkK3kwNgnnHHTkbt4c5VvacaU9ZNwcnmk6afG9uXbgTSKq9/yUgLpKF4YDFW7l0ZM3/K+I3iNcWWcSFuSLgbmwlO8k6US4WV24sXl6D2oHpPNSasnmDaR5V7ur2u20S7s3TFPnQlyD3jDKR6TzUmvTkpammwvrEe41ePD6dJsrIKqVhsJLXHah0rfiUk8E0rFRpWHqRlcMcO920lAo+uIraWBSsLdkoKsGmTUJhDoWiL87Rp7vvhZLHPzQXKyoCGBqCxUSjMmzY5W/hnzIj9/txcTz7Bc3edjfEfAa9dIxTm/7pZKLsx2SL032OQg5ms3/t2Gr6aMApPbAcyp82KVphN5GK0PkkXj6jtKq45ubnAQw8JeTY0YMmNQdM8rOsrp6tb63X3RL+Mfni7v1CYB39h3xV5z5tlmTDLNAFYu4XVHqu1zVFsRM4FKpkj3CCVGaPCLC2sVlVTDz58EHwOx7KCZVHWyeKBxajaXRU1fx5tPIrxq8eH3U+qRldFHVOSU6KcelPSUQFbdhbYRLpUJHvqs6IBRZiaMzXm+aZq3W2PgLvOTsPalbMKJRtkae7CuLEWA7HBJ+NXj3f1fdIys/GTjVFvqwDaP0CwM4gjKNH2rd7YrrQky/YnTgxbnKsfvQPrK6dj8auHsfzmDEzZ0Yaev33TeTnfyWKvtzS3tAj3jGefFQp6R1j4t21D8/Ch+PmQNpR/vzlsHX72hwEUXnEPrvvVCutxGc6HUdYv9i5C/mNLY8+bhVzGjfGj5rzD4a+RfXno3r5Y8euv1OVhsDR/3dqA0YWxFuEZ7wBPP77VtbVeZmgwlrg3w8/8jhbSgC+As3qchaONR6PuZSvLrBlGy51KYKYMyHJr9Ur0SpfT/Gkmn6IBRUrzrrT0J8v8l0jZdUhAeAIgy2qEdl8lPgOxsjST0tyFcRsdLpfPVDMOmGF2k52RN2R7u4roXQTS0yMuD/J7hg5F8xsr8fMhbXjkj83h5fg7P+uBNauD9oqzkwuFVJgZA9asEf7NoRAQCIj/ZT/aM6uE1kd9toixR87DjGCuvcLsdD7s9tuxw1Quu95aipvPfSvqfgjL+cGH1V6YpMLMObB2LQDg+LA8tHEeozibKZoqD3e5stTU2uQoXi/oFR8VxdBMUVI5zsd88DN/lPKvonSpugWoypPNVVtpM/bRaunfaRwq/WovRS/RLhXt1c+OllF3UayT3aUmGSGl+QzEq6XZ7rigP4jewd6W0eBmN9kZeUMmOg2cHisLtkHhG/fA+fjNbw7jiVzguRsjhxce6YsV2dO896O8HNi/HygsjLY819QAl13WsenXZH+cZK16PjyeN9OH56p96mn1TGS6+aVZqH3haezLaMUzN4vdzHxuVa12qvd7Zmom0oPpruYGiZ/5UTW6ylYx1O9n7GM8ad6cXrJVXs6tDAJ6X2VJypMprv2V7fy79fsYlS+V8+zFglu6oRRL31+KVt4KP/Nj8qDJqBheEbNfVzBsJEpGbl5Cu4LF3C1m45erR0ZUfby7w4uFEVKauzBWF23phlIs2blEqQ39ZGD3YAsVhGwtTd3G0txeuLCE+ub68MMDHIO/QFjpArr4y4gZyRYAqu9TnPnCnR44bu4dFYVU3ucAHBVfKwK+gKMLiNU16PZF3qlNvfz6pPbB8dPHbS3Udt9v3NetpVn2cVnBMseVOmMAnlVAof48W/Xd6gXFav4vySmJUZy7goJody/YKX76/YZdPgxVu6uixsnAMDVnaoxMzsTnltV5Lh5YjBc/eDHmvjZ7mVRpL5mum/aCAgG7KHYO/Bs/2ajUhrGkrlXARnZGdlRqJ9U0OZRSxwV2wY6GILt+Gf0w+AuRRk3P2CPnxVU1L+kqFCayymAisEur5zJvtmUAkNaOPpBuyAERhAiYB9hZ3bd+5o8JWJLBUF5Q8Zl2U+pXFWObxrmvvrEejDFkpmZaBmjZpRQz5pxWDYTW0ye1T0wVQbNUb/K75BisLNr6/lr1vZW3mgZtLX1/qen+S3YuAZvLkPJkSjjoE0DSV5OzGr8+qNWO2mO1qNxZaVoFcsnOJcgqz4qSoUpq0NINpWE5pjyZElVVMBnnVKt0gCv3rgxXr9RT31hvGxBoVzGzu0JKc5JjlxPT7gEhJ8ZQQQh1M+qiJkcVJddNBDGV93SBi+wT8/LmYU92j6i8w3d+ppVVdlFaPO7I6QSVu7YlWYraAPYvNlbZT/bvN5XRrp+Ni3qwlm4oxSXPXYJb//afqL8rHyO/7A0gEnwoX5DMlFKr+7ZqdJVpVH7F8AqECkJR2R6kopiZmomgP+hJPKqlfuNt02zua2ptQnow3TILgVMGh9pjtWHrtZ0VnoHFyCfoD+LrU1/HuK61cfNVn0PHDtmW/Tb2167vZoqKk2uJ/Fze70Dii9Ekkj6pfUy3+5hPOf7G7pwaFUQrect+SEu+lGMrb8WSnUtQuqHU9ZxqVLDlPKBXuPX7ZJVnIas8y7VCbqUT1DfWW8ZCWCnB1XuqLd00E10wpytB7hlJjp2/sFUZ3fbwU3Kzf3f1gWoPqvdUu8+eYSDuZciOcJ/oSuXTzfoKxMjo1D13Y3RBU0zaOYlUlJcO9mHyjrZwsGdHla72kofYylXAqn07NwapwFuNw27us8q+oRLknBZIU1LC5MvG0caj6JPaB/8+9W9LBdmM7IxsW+uoir+uHqP7iluf7GR3O8gqz3JVWdErUg5W2WiC/iBevvtly3vDz/y4+KyLlefU0g2lqNxZ6RgjYHcuVecEH/N5yituFkxrNzck+7WUCMinuYtipfDICd04ybSHv5HdZG70ierOPlDx4KgM2RUqcSAhgZrtqdQmo0+zE2bnwyAjYyo7M+ZuBWZvBxbdnoGf3Xi8w18y3aSe9HIfO7XP51g/f+zmvsaWRtsiHvqCT2aopOST7cp8zm6yDcn+2CkeMn5Ej6r/M2Dt02xFZ8VCqL7oxRNE6ga9HKwUdacXHrt0jNKgJVdOvMYWmPXJqKiqviSmpqTa3g8qwbR6zK7dMw3yae6imC3JBv1BHD99POYmMPouJwq7Jcb6xnoUrykOLyUVrykmHyiXOC71GQtyTJniyl0iIcUI2tN9ItmK2jhhVTjGICMnhXnIAaBkJ/DULUDZBwG03bIlHPBkXJZtL//JogFFSsU6vM4tRQOKLF01GJjtOKzcUQBYzjFSQXOyWLbyViXf64bmBix9f6krhZmBhWVlp5hbubpVja5SihGpGF6BkpwS+JlfqV9293t7XV9Oc1v1nmpklWeBzbVWQnsFepnKoySnxFOf9HI42njUdJ9Dxw7ZytVOCdaPs2xTWcJeBMxcIqyezcY4h0VDF1le78Zry8mlKDM184xXmO0gpTnJMfMX7h3sbRqskx5Mt1y+iWdCdPJfauWtqG+sBwdXCng5Y/Ho+2vnt24alPb66yLPstGv1sLPOSGBmk4VBuPxe05glcF2xy5I0CCjvIPW06t0zRjzY+Cl0dnAypU4dc/dCC26P0bBKN1Q2q7VvMZcPcbyM31cBABP84hV9VEOjuI1xZbtWMVKWCk6qgFjgFAqGpobwoqRnYLkdrlbKkkyAM8MsxcJOU9PWD0BqSmpUQGPxQOLTV+mKoZXoGV2C/gcjlBBSFkxMn5ve11fdnObdI9wesHpmdLT9DqoGF5h+cKXmZpp+plRDlYvEhwcqYFUp+HZ0tDckFB3E7O+Wj1X23hblO+6MdZAXu9ug2nTAmlYNHRRPMPo8pDSnCTYBQrM2jIL8/LmhW8Cu7djs3bjnRATUR7VUxsdEYCWyO+xChJzCNqzjeI2s8KuWSPyAatkmygvR1HdhVEPncIjffHOv0ZGv2DZyWDbNmDECFG5UK8oLlwYkZHZ2O+6S5QHN2tTQ173/3kbw7gHzo++LtvjXMeLlVW8piZGmV6xsi0cwGlk8BdCYX7v29pDPDcX941Lw4Da2LLbS3Yucb164+ZF2SoLj1yylektJ6ye4GkeKRpQZGlta+WtmLB6gmVpb7PsI1YBY1IRVkEf3CWDKa0s4qqWXD12yruZAmuWKaSxpRHLCpZhXt48VO2ucpS9G8VIj+1Luwluri27uW3WlllKmVrqG+tjnoFyLGYWVAaGMVePQd2MOoQKQrYB6nbZXk40nUCKL8VTlpVEY/XSY6f0G8+NvqS8fNFyE0zrZ35yswT5NCcFqqVnZa7JRORQ9jM/2ngb+mX0w7DLh2Hl3pXht2IzP+V4KpF59mnuKF/XRH6PB99fz4F6Kn7ObirpjRgh2tOX0p45E9i8GcjPjy71vXCh+P5166ILpJiVBy8oEEo+EPluIKoSn7S8ThzXA+PLXkJR3YXt6tec8GBVk6Iq4x44H9/6x+GoHNt6jPeZF59OM59gt3EFTj7v1XuqLf0yVQOCVPM3O80V1XuqMXH1RLQhsb65MhewUW4qpcSN+JjPMmDQWPhEXodWsrErppKoYCyV8+8mV7Yeu7nt0LFDrmRr9T1mgXYBXwA9UnrgRNMJAPb5iJ3OgVOxILugfDP/e1XSg+k40XQi7H+vWjTHrA92uZiNmMmzO8YlkU9zEuPkQwSIN8fKnZWo3lPtarndLventFos2bkkahmpvrEek96cFH5LLRpQhN7B3q7GZJY71jUdlb/X4nuqs750bwX14PvryX3CyV3CamzSYqzv15QpwlL61FPAI48AEyeKktDXXy+U3kcfFYq0vp3586MVZrOxy2Nefx0YPly0qcs0Ud60LXzdv91fWF5fW3EadY880O4Kc8KXo01cTEZMXYBf51r7zRrdqbysxujzxkrcWg6dfN7t0rOpFjJRzd/c0Nxg67JRtqks4QozIOZJaanVL+t78UdVzbChvw7t+qWSTzge7M6/mQXcaB22u7bMzjsDQ+2xWtP81nZYfc/GTzbGnKfmtuawwgyIZ9p9a+8zva6kBdbKolzfWG97juRLt9kcvmjoIk8pGEtySlA5ohJpgbSY1IHxdamGAAAe5UlEQVRG67FT+065mPVU76lG1e6qKHkyMBQPLO5WCrMdZGlOAtxYmPTpcqxSLum3n2g64dmvysd84JxbvkVbkfC30jgyR9hitA7K78nLQ/Wv7gu/wbuygnrMMuHK8unFMi7HNmECsGlT9LGjRwOcA2vXAq+8AixbJtwqevUSriD6NvPzgS1bRDuvvRbpT02N+Hv16tixy+9OTRVKufaZb3te1HX/6DvAd44AEz9C5FwnqnS5jo6sBGaV4eDRd4CdFwFbX42Mf/NLs/D2qmcw7wbnJWs9oYIQAISvH7u5pCSnJGZVaczVY2KyQ6hWEAWE8l85otLxfneT4k5mrdj4ycaoe0I104dbZCo9AI6Wu3gJ+oPoHeytNC9nZ2RbzuGZqZlhX/N4sFuZcKrCJ7HLzKG35Hqx3Nt9j5vsL4D9Pe6lmqX+Oq09VmtrFXbTvl3GC6sxON2nKuki7TLWpAfTu1UaWUo5l8S4uZmcJifj5BfwBcAY8+xaof9elcmOgeHW/rfi06Ofqt1gJsvaUYpSR6U6A4Bhw4C2NiA1FeOKeqLmvMMYcgDhqnwrVwHLb85A2QcBe4W5vd1JnGRmNU6j24Qxz/CoUUBLC9DUJH7rFWPZzrBhQGsr0NwMLFgAXHed8F1ubBRKsbQ+b9sm2vvBD4C//EV814IFQENDWCGef9c5+H3m13i7v2j+4T8DC34P7L0ogAGnMyL9TLD8EpKCTxGre3vIAeCNN3zIXLdZjE2z9m9e+CDu/OJptPLW8LVn5d4hCfqD4Jwr+YeaEfAF8NPv/jRGQVUpTa3vw/3X3W/ZhkRlOdmun17HqMcqT7NKaq6ORCquVunzzJRmu5dvL5+pGnScXji95AR3+h4v15LdPa6ST1mPygunxK1yb4fZi4Pqy42TQUu1tHx3cNcg94wkxk3pWbslXLNl2ea2ZgR8gXAwhJegFkAsU6oERHBwbD2wVX3p2y54zi5TQSKQrgujRgm3BZ9PKH6zZ2Pxq4fx8J8jVdre7g8syQHKfn/M2u2io1Knuck2IWVYUBDpz/z5wNChwvo7cGCkrZYWodQGg0JhDoWE77K+nXnzhIx69gSmTwfuuAM4eVKcL2PQX2ur8IeWMklJAdLSgEWLgG3bMHhUabja4ZADwJw/Ag0B4IpjKaJ/jzwS60qSABKSgs+IRSDl2A3mD7K3+wO7Fj0WcXepqQEYQ/6l+agaXYU7P+sRVSHQjqbWpriUyea2Zmz8ZGM44M6Y9m7Y5cMc56em1iZU7qyMuu8nvTkppqqZ14qBsp+qQVlW85x0FzP7PNHZDuJBn+LPKvDbuN3O7cjJJcmq3LvKPeHkSiaDSONVmAFg2OXDwn+ruDUasRqPW4UZABpbGrFy70pHVyiZJSRR6MegD9BVwa76X1Z5lnIfunMaWVKakwCz1Ep5/fNiHhBOk5PVjXOy+WTYvyueNDqqE4pxP9sbTO9zm58f8XvNzRXK5syZEaUzgUpoOAJ8ex7e6dsklMXp04U7wvz52H5VLzy1DeEqbTKn7qLbM6x9iPXKrFSi9MpsZ2SDkIq8zLYBCJmuWiWU3507o90r0tKEcnvffcCzzwqFTlqxV64Ufspr1wJMuzabm4WC/e67QnZ6v/CiIuEKAoj/164F1q8P9yX/0nzsXvw43njDh1nbAR9jeHfJ4wj+7BHhIjJ+PHZ9tiM2Uj/ObCcJScFnZP9+ce3qX/5Gj8Y1J3qZ7p6Zmon8++dFfMDLysLyK1q1D2tWB/HQvX3xx/6R+aA9kf6xZspV1e4qFA8sdnzhNt73Ta1N4VSUeiVNH8XvFg7uWP47LZCGIZcMMf2s9lgtZm2ZlRAFrj2R/u7Ve6otfX99zBdljCjbVGapwLn1c5dYGXR6BXopxaxI9yTXwZQWqknV7qrwmN26UgR8gfA9rs8PzeYyT320e8mS5doBcV68vtQar3X9PFW9p9q1oi/7JpFyGL96vOsXxm6RRtYEcs9IYko3lGLp+0vRylvhZ35MHjQZFcMrLPdXXVqJOkZzp9h6YGvCkrBbfY/t0rfe73XDBnX3BreuCohe1pP+yi9+PwU/+zBVlKjetg146inMz03B4z9s8ZbZoT1dNTyMObyPdMFISRFKLBDxaS4sNM1yEdPutm3CEnz6dMR6vHZttP+y3v+8vFwolYWF0Zk2amqAyy4DTpyIHJObG3YlOfX8czFlqNMCaXiz78PIf2wpNv9yMn76TTUu/bAWq97wYdeix4QiqkDCsmfIcwEI2TIGjBwpXkqCQWxe+CDuPvyc+fKtvI70bjLatWfmv+/l/lalV6AXstKyLBUROf94eUjrMS7je/EjDRWEos7dsMuHhd1C+qT2wenW01FBYGY4VXRzGqNdhoxEkZmaiW+avrF1rZPXEgBPy/9mGTKMMTKT1k5CU1t0H6Rf9tHGo7auOImqhqdHZn5y225JTgkqhlfEnQ1KFTe+61bor0VjFgwv945ExkHE479/ppfSJp/mLoJd0ISdH5Hbkqp6rNItJRLbG0zvc7t4sVDgysrUfJg9KKdysql8Cxi7FxhdKKzJhUf6YkXVCeFW8OijOPX8c7hvXBr6/eMwDn27L0ZMXRCRvZOSWl4uFEq97/DMmUJhjSeoTd+uXfo3K2Qgn14pmzJF/H7hhch+VuOT+ZdPnhQW5jffFPLy+4E5c2J9pY3p6IznSe9fvWiRUDq14MNxD5yP3/zmMJ7IBZ67MdKFwiN98eThq3D2pm3Y3RfI+VKcw/e+rVNGExw8aInRL16+TPToIazsubnmConxxUu+0DAGPPSQ6bUfz0MyEaQF0nDDxTdgy4EtcbWjtzB78SO18uNNpGxkSjR9OWTjOfzToT+FjRqdiZ/5cXbPsz0pZ1Zzvwxwe/GDF5WspGbPps6+Xo3IZ1Cy9UsVo4zt/M2dyoAnwn9fvoScqVgpzSlmOxOdgzGwwczNoWxTWUyAQbwPDJluCYg8GGQBAZWbSkYMW2G79G1UpnJzI/mCpeXRDr17h2KwYHhZyWC4u+DTw0ADE+nRnnwSPXNzsWLMGGDlVnMfYru+SV9t6Ts8YUJE0Y0H2e7MmZH2QyHhSqHvj5k1euFC4UYh09TJMeiVZafx1dQIxX/Bgkg+51GjgCuvjFbcpcVYf16N58kY6Hf4cMRNBMDr5x3B+bnAU9uADy+IuMksXnUYD90LXJkDzN4OnNRmsYbmBqyvnI6ila3xy1kV/bikwgyIlwgNWZErivLy2OuUMWDsWFT/+Aqsr/dj8Yhb8R/jz8I7lwVwtPGoZVGPjqKhuQGfHv0UoYJQVHBaejAdnHOcbD6p1E5WeVZ4PN80fROT3urKrCvxt7q/xRyX4ksJVyNLZEYGI1Yv+MZzWDSgCBXDKzzl104ksiKrW4L+IE40nTC1UMvy4aovBGbPpmRbvj907BCq91R3SYUZiLjT6N12zM4PAwvHJViNtaG5IW4DmVVRpDMd8mlOIlQCG+ob68O+Um6DAKyQPmv6YJC6GXW29eoB8XCR1YXsAnRso2yNwXOAsKTm5dnnH9bjMjeyDKSYepewUK5cBczdCjy1jQnlc926SLtefahzc4VSGAqJDBKhUGKC2vTBfFdeGfb9xbRp0fsZAywXLhSBdU8+GV9Q5WWXCQuq/L7cXOGa0adPtKXbTHbG89TSEn3uX3hBtKUd0y+jH567EbjrJ5FztG45UHnbOTh88jBKdgJP3gJwBmwMic8Xv3rYeXUiTp/omIpoWV8KhXnZMmFhfuIJcQ1rPs6mFdSMwZzPPAPMno3qB2/B5HWTUXPeYfzXzcB/bD0e9guub6y39PXsKGqP1aJsUxmONh5FdkY2SnJK0MbblBVmAFHjMS6Rc3B8XPdxzDG9Ar3w6qhXwwqDPr+xV4U1MzXTtW+72blMRMXUjqanv2fY59wKtxb0+sZ6ZJVnhZ9PySiXSW9O6uwuxEXtsVqwuQzjV4+3VJin5kxF0YAiVwkGvJBsL0UdBblnJBGqFgvp26TqL6aSqknvM61ivTYuFSUk961XP2CXaemMFv25W4XFcs+UURhQuUatr6pjkcqUMT+yri+qvrX6fVetT8M9O08Khfzjj51zRi9YEKn2p/+8I9wY9H6/ejecsWPNrdwa+vMkz9Hya/340adBnGo5hbvHimt/TQ3QswXo2SoCNct+97V9f+LwNzdLc3XnZz2wblkrUphfKM1r12Lz/2zG4Afno+ZqjpK7oi2hm5b78J3zr8HFv1gY+b7SUmDpUiz/XiqK7jiBIQfEC4LRNUU1DV1HkWgrrxX6PLFWFjYvbY65eoxjmjyJVT7jRLisJCNOK4hWpPhS0NrW2qnW92QmMzUTRxuPJlw+PubDOT3PiVqVas9sMOTTnMR0F6XZja+Vk8+SpCSnBDf1u0nZhSPoC8YEfVi1a1Sw3fhgm+IUMGaVUu2uu4T1VF/++frrgQsvjPXTfeYZ4NFHUZ31JWZtmYVLP6zF2tcZTg68ChfsN7FSeg26U/BpdlPuWL/vW8uB4f8AVlzrwz21vdDz578Q47/iCuCXv4zu68SJQmn3WhjG6/i1Ph96/EH8q+lrzNnOkJrSE8GfTAA++gh4/30RQLhGe0kxO8fl5diceQwvffASFr96GMtvzkDJn5sQvDgbTZ/XYsHgZvz0Ly149yIg9yDw3rd8uPHfvUQwJ2DfxylTRKVCvf+w0zGIvUdlkOgfBqbjJ/PeAgCcuudujC5owqhdpwEmVjQefSeSPm7a/xPnz9ezJ3DNNcCgQaIPPXvia34K+88BrvoXsPlS4AeHxGoIABTuET74r18NTB1pK3pCkaA/iJfvftnU5c2oSFvNz16Vy2TGrU8zoUZe/zy8+/m7rlZmkpVQQYjyNBOdy7y8eQj4Akr7qiyNpAfTcVO/m8JuFyqoKMyASP1TuqE0ZplUuml4Kp89Y0YkNZp++Xz16oil0siOHUIZnD8/kuJt5kyRH7imJjb/c36+SOtVdyEODnwFW9dm4Kxgb1ww/7/NXRbs8kjbMXhwxF9XukPMnx91nJs0UHLfIQeA/P0in/GLA9tw37g0YO5coYyfe26sS0YoJKzcqq4uZuPwMH6p5P8+82vMfAfY/i2OhuZGfPnRn4Rftc8XyU88erT4bWxz8GDkT3seK6pP4dz1W1H22BoEfQHgn/9EcPSPMHNbC7Zf1Qs/OAS0+RnaHn9MKMyjRok27fpYWCiKuEhXEUBpXMb7bvAXIi3h+DtOhn237xuXhgG1p1EzACj4WCjWOy4SFvE1NcDCG4DKHACnTomUf5rCjI0bsWVALwz6CgAHtvYX37FuObC+GijaI/7flHNWzLJrwBcwTcWmLwlNxNLU2oSyTWXh//Uub8ZUeVZGh3gUZqvz1pn4mA8NzQ3Y+MlG3JJ9S2d354xiy4EtZ4TC3J0hS3OSkVWepbSkorosGm9KIjusLCxxL9t4qQJodgxg3o7cd+BAoXTLdGmyHbMUa277o2ChdVOZTu6rt1iuXCWUryd2Z0RcHfRuITJAUG+B95LyzsP49VY5aY39OAu45RCA224D3ntPKK2MRdLfmbWpVcqLyqaya5ewnE+fHnE7ue46V64f4fLhKn2wGJce/TWvP69y7EtygEf/BLT6hNL8wPs+ZF56NbBH04QDAWDIEPDNm7F8IMPwj9uQ0gbsywS++5WIWT2ZAvyoWKQ9BGKzOZhts7OQ2iF9I2V54DMdPofbpkjLzsjG58c/T7hFWab+aq/y4GcaJTklcac9JBJDd3XPIEtzkmFV/cmI6qShT26faKweIHEHCLgM7LM8xqoduX3LFqGM6ds3q6znpT8KVfvcVKaT2565WWSRkBUKn9gO4WIgFUTZV2OAYLxBjS7Hr78G3u4PbPpfQmHe3g9C6R05UpTebmiIPQd6XnhBfK63CM+fL4qkPPmk+D1/vtgu+6iXhxnyJWDNGhEc2dAgLPUKqBRG0Z8/eZ5mbwdWXS2U39nbgdMDrowozIBI2/eHP4Dl5wOvvYb/vvMc9GoGBn0FtGizNGMM02+YHs7GYazgZlXVzW1AUHZGNpYVLEPF8Ip2DyZqT7IzshEqCClXEZy1ZZblvHro2CFXCnPAF3C08mdnZJtnVjFBdQxnOjf1u4kU5iShuwYCktKcZLRHxPGhY4fa5QK3qhAW9xi2bRNWRZkaTcWtwOwYq3bctu+lPwq4qUxn3FdWKNwzZZT12DZtiu6rValtJzyMX38NPPxnYPxHwGvXAFfX+4Af/1hYwYPBqLLaSt9dU2NeqrymRr2PMmMLEDkmJSUq3Z0VZtU7jW5I+nMlz9NTtwAj9wEcQGuKHxe+u1fsvGCBUPbb2oTLyubNKHrmd3ji7TawHj2AlBQE2gBMmIC01N7In/5r19efsc92ypy0HsnxmB2bmZoZHrtdW52p6Mn7qGhAEabmTLXtixyD3RzZL6Ofcunv7IxsvDLqFdTNqEOoIGTqfqGvTgdYz6WyPVIUhRy6U+lm4zUb9AeTyt0qGbOjdASkNCcZ7WHZ6ZfRL+EXeFogDZMHTU58OWK9G4FqajSzY0aPFr6txnYWLnTXvpf+KKKigJntm3sAeOMNH3Yvflxk+/A6NhU8jl9ex0MOiDzL028HSsam4WBJIVBZKRTme+8VlmLGxLkySwNn/O7Vq82/cPVq9T7KFwd922vXijYUZGVl0dV/vvSupSg80hcrVwEP3dsXN0x8HGf3OAvpaRnwn9dX7BjUlKlVq4TinJMDXHutWCVobBT+92lpQrHetEkUj+FcSbm363PdjDqU5JTEPJSt7l3jsXUz6sJjN0tLmRZIQ6gghGUFy5RjNFTQx0uU5JRYKpp+5o+6jyqGV2BZwTJThSPgC4RzP1vNkTLvrcrcbPbS8fLdL0d9d2ZqJl4Z9UrUdTN50GTT9kpySnDw4YPKCrsKiTwnHYU8B+1l3UwPprdLu15JC6Rhas7UqGfDy3e/jLoZdeBzOEIFIdO4ho5SquX56I6Q0pxkmFl24pnk5INQZcLP658X9b1GC4kxyK9ieIWy0qeMMW+ziluB2TFjx0Zn4ZDtbN7srn0v/XGBkwJmtu/Wq55G5rrNkZLRXsemgsfxy+v49vqzMfInwJqh4toYdO5AUTymuFi4UOTmCjeJwsLYNlW/O1HXTILP64rsaTh3/Vas+PVXyK/PEIr57NnAsWPCup2aKjJ4rFwJVFSIzCeffSb8s30+4KKLxDHTpol9WlrE/5ddFnf/pCIZ771r9+JXNKAIr4x6JepBHvQ5B71Jt4pQQSiq3WUFy8DncBx8+CAqhleganSVqcJeNbrK9EVGWn71beqVV7M5Up/3Vj9W+Znxu61eOqSyw+dw1M2oi+lfxfCKqBcBP/NHVVzzYkwx9o+BoSSnJOacJAN+5rd97shzoGr8sVpZMAu8TAukoXJEJUpySlz1OTM1EyU5JeHrycec1Sm5UiORxxhXcOTz1erZYHbf6Vc3ErXK0yvQy/ae6I50SiAgY+xOAIsA+AG8yDn/pd3+3SkQ0Ax9CqQ+qX1w/PTxqDRAMigwOyMbwy4fZpl3tHpPdVQlL/3xU3OmxpTEdJNDmCAIB+zyQwOec0d3NUo3lMZUm5PlnN3OL4meo7zmTe+I+dH4fXKut0r3WTyw2DYHtVkJc7PgbtlW1e6qqGw/DAy39r8V22u3x6Sly+ufhw+/+lApqN2YZtNOrmZpOs3ak2OvPVYbHpO8xgDzgFnZ/qQ3J0UV3Qn6g7j/uvsd83mbHWs3zvZEpTy9U/l1mY4RsJbXmYxVICA45x36A6Eo7wdwKYAggN0ArrI7ZtCgQZyIEPooxLN/lc3ZLxjP/lU2D30U6tDjCYLwwNNPc751a/S2rVvFdrvPCMIBr3O62XFWbbndbvUdJetLEvr8irc9FZl4OTbz6Uye+XRmpz1n3cqJ9IJoAOzkJvpoh1uaGWM3APgF5/wO7f+ZmvI+3+qY7m5pJgiCIAiCIDqGZEo5dxGAz3T/f65tIwiCIAiCIIikJKUTvtPMQz3G3M0YmwxAhhSfYIzta9dedT5ZAOo6uxNdGJJffJD84oPkFx8kv/ghGcYHyS8+zjT5maas6Qyl+XMA39L9fzGAL407cc6XAljaUZ3qbBhjO82WAgg1SH7xQfKLD5JffJD84odkGB8kv/joLvLrDPeMHQAuZ4z1Z4wFARQCeKsT+kEQBEEQBEEQSnS4pZlz3sIYexDA7yAyabzMOd/b0f0gCIIgCIIgCFU6wz0DnPONADZ2xncnMd3GFaWdIPnFB8kvPkh+8UHyix+SYXyQ/OKjW8ivU4qbEARBEARBEERXgspoEwRBEARBEIQDpDR3MoyxOxlj+xhjnzLGHuvs/nQ0jLGXGWNHGGN/1W3rwxj7A2PsE+33ObrPZmqy2scYu0O3fRBjbI/22WLGGNO292CMva5t/wtj7BLdMcXad3zCGCvumBEnFsbYtxhj2xhjHzPG9jLGyrTtJEMFGGM9GWPvMcZ2a/Kbq20n+bmAMeZnjO1ijK3X/if5KcIYO6iN+0PG2E5tG8lPEcbY2YyxNxhjf9fmwRtIfuowxq7Qrj35c5wx9jDJ0AKzMoH0k7wlxc+0HwC3APgugL/qtpUDeEz7+zEAT2t/X6XJqAeA/prs/Npn7wG4ASIP+CYAQ7XtpQAqtb8LAbyu/d0HwP9ov8/R/j6ns+XhQX4XAPiu9ndvAP/Q5EQyVJMfA5Cu/R0A8BcA15P8XMtxGoDlANZr/5P81GV3EECWYRvJT11+VQB+qv0dBHA2yc+zLP0AvoLIUUwyNJNRZ3egO/9oF9fvdP/PBDCzs/vVCXK4BNFK8z4AF2h/XwBgn5l8IDKw3KDt83fd9nEAXtDvo/2dApF8nen30T57AcC4zpZFAmT5JoDbSIaeZJcG4AMA3yf5uZLbxQC2ALgVEaWZ5Kcuv4OIVZpJfmqyOwvAAWjxWSS/uOV5O4A/kQytf8g9o3OhkuLm9OWc/xMAtN/nadut5HWR9rdxe9QxnPMWAMcAZNq01WXRlryug7CWkgwV0VwLPgRwBMAfOOckP3c8B2AGgDbdNpKfOhzA7xlj7zNRCRcg+alyKYB/AXhFcw96kTHWCyQ/rxQCWKH9TTI0gZTmzkWppDgRxkpednL0ckyXgzGWDuC3AB7mnB+329VkW7eWIee8lXN+LYTF9HuMse/Y7E7y08EYGwHgCOf8fdVDTLZ1W/lp3MQ5/y6AoQAeYIzdYrMvyS+aFAj3viWc8+sAnIRwJbCC5GcBE8XmRgJY5bSrybZuI0NSmjsXpZLi3ZDDjLELAED7fUTbbiWvz7W/jdujjmGMpQDIAHDUpq0uB2MsAKEwV3POV2ubSYYu4Zx/DeBtAHeC5KfKTQBGMsYOAqgBcCtjLASSnzKc8y+130cArAHwPZD8VPkcwOfa6hAAvAGhRJP83DMUwAec88Pa/yRDE0hp7lyopLg5bwEo1v4uhvDTldsLtUjc/gAuB/CetnT0DWPsei1ad6LhGNnWjwBs5cJ56ncAbmeMnaNFBd+ubetSaON9CcDHnPOFuo9Ihgowxs5ljJ2t/Z0KIB/A30HyU4JzPpNzfjHn/BKI+Wsr53w8SH5KMMZ6McZ6y78hxvBXkPyU4Jx/BeAzxtgV2qY8AH8Dyc8L4xBxzQBIhuZ0tlN1d/8BMAwi48F+ALM6uz+dMP4VAP4JoBnirfN+CF+nLQA+0X730e0/S5PVPmiRudr2HIiHzX4AzyNSuKcnxHLTpxCRvZfqjpmkbf8UwH2dLQuP8rsZYjnrIwAfaj/DSIbK8rsGwC5Nfn8FMFvbTvJzL8shiAQCkvzUZHYpRCaC3QD2QnsGkPxcyfBaADu1e3gtRBYGkp87GaYBqAeQodtGMjT5oYqABEEQBEEQBOEAuWcQBEEQBEEQhAOkNBMEQRAEQRCEA6Q0EwRBEARBEIQDpDQTBEEQBEEQhAOkNBMEQRAEQRCEA6Q0EwRBdCEYY62MsQ8ZY3sZY7sZY9MYY7ZzOWPsEsbYTzqqjwRBEGcipDQTBEF0LRo559dyzq8GcBtEXu45DsdcAoCUZoIgiDigPM0EQRBdCMbYCc55uu7/SyGqi2YByAawDEAv7eMHOed/Zoy9C+BKAAcAVEGUa47Zr4OGQBAE0SUhpZkgCKILYVSatW3/BvC/AXwDoI1zfooxdjmAFZzzHMbYEACPcM5HaPunme3XsSMhCILoWqR0dgcIgiCIuGHa7wCA5xlj1wJoBfBti/1V9yMIgiA0SGkmCILowmjuGa0AjkD4Nh8GMBAiZuWUxWE/U9yPIAiC0KBAQIIgiC4KY+xcAJUAnufC1y4DwD85520AJgDwa7t+A6C37lCr/QiCIAgLyKeZIAiiC8EYawWwB8LFogUioG8h57xN80/+LYAGANsA/B/OeTpjLADg/0IEC74KYL3Zfh09FoIgiK4EKc0EQRAEQRAE4QC5ZxAEQRAEQRCEA6Q0EwRBEARBEIQDpDQTBEEQBEEQhAOkNBMEQRAEQRCEA6Q0EwRBEARBEIQDpDQTBEEQBEEQhAOkNBMEQRAEQRCEA6Q0EwRBEARBEIQD/x+5L3bY54u21QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_anomaly(y_test[:1000], X_test_errors[:1000], threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that very few 'normal' data points lie above the threshold set. Those that are above are mostly anomalous data points. We thus have very few false positives. This shows that we have very **high precision**.  On the other hand, we find that many 'anomalous' data points actually found the threshold, and thus not flagged as 'anomaly'. We thus have quite a lot of false negatives. This shows that we have very **low recall**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AipepLJEVMyd"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "How do you improve the recall (sensitivity)? If you improve the recall, what happens to the precision? \n",
    "Try out your idea by modifying the codes. \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "<br/>\n",
    "    \n",
    "You can change the threshold to a lower value to improve the recall (less false negatives).\n",
    "However, it will also decrease the precision (more false positives). \n",
    "There is trade-off between recall and precision. \n",
    "    \n",
    "Re-run the cell above and change threshold to say 3 and re-run the cells.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4t7157oVMye"
   },
   "source": [
    "Now we have used the validation set to help us find the threshold to use, let see how effective is this on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9tTSqv4VMye",
    "outputId": "5e8ea2f5-1011-478e-c169-4fbf5b0dd74d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold=2.50, precision=0.7474892155630942, recall=0.7090225802863453, acc=0.8068602239494596\n"
     ]
    }
   ],
   "source": [
    "threshold = 2.5\n",
    "\n",
    "X_test_preds = vae.predict(X_test_scaled)\n",
    "X_test_errors = np.linalg.norm(X_test_scaled - X_test_preds, axis=1)\n",
    "\n",
    "z = zip(X_test_errors > threshold, X_test_errors)\n",
    "y_label=[]\n",
    "error = []\n",
    "for idx, (is_anomaly, X_test_error) in enumerate(z):\n",
    "    if is_anomaly:\n",
    "        y_label.append(1)\n",
    "    else:\n",
    "        y_label.append(0)\n",
    "    error.append(X_test_error)\n",
    "precision = precision_score(y_test, y_label, pos_label=1)\n",
    "recall = recall_score(y_test, y_label, pos_label=1)\n",
    "accuracy = accuracy_score(y_test, y_label)\n",
    "print('threshold={:.2f}, precision={}, recall={}, acc={}'.format(threshold, precision, recall, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uue8Jgu0VMyg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wI3iJKinVMxi",
    "_RU_vNHdVMxl",
    "O-pC2aCJVMxy",
    "_FwThzk_VMx5",
    "ZUqBcvkrVMx-",
    "xvsHmm9LVMyK",
    "Eh9Z7HdCVMyQ"
   ],
   "include_colab_link": true,
   "name": "ids-autoencoder.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python3.8 (tf2env)",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
