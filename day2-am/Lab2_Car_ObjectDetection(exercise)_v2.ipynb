{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://www.linkpicture.com/q/nyplogo.jpg)\n",
    "\n",
    "\n",
    "Welcome to the lab! Before we get started here are a few pointers on Jupyter notebooks.\n",
    "\n",
    "1. The notebook is composed of cells; cells can contain code which you can run, or they can hold text and/or images which are there for you to read.\n",
    "\n",
    "2. You can execute code cells by clicking the ```Run``` icon in the menu, or via the following keyboard shortcuts ```Shift-Enter``` (run and advance) or \n",
    "```Ctrl-Enter``` (run and stay in the current cell).\n",
    "\n",
    "3. To interrupt cell execution, click the ```Stop``` button on the toolbar or navigate to the ```Kernel``` menu, and select ```Interrupt ```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7kdWxWYoOtS"
   },
   "source": [
    "# **Lab 2 Car Object Detection**\n",
    "\n",
    "In this Lab we will try to build a car object detector to helps in the recognition and localization of multiple car instances in an image.\n",
    "\n",
    "The predicted output from the object detector will the label of the object and the bounding box to denote the location of the object. In the following, you can see an example of the object detector predicted 2 objects with label vehicle and 2 bounding boxes to indicate the objects' location.\n",
    "\n",
    "![image](https://www.linkpicture.com/q/object-detection-introduction.png)\n",
    "\n",
    "\n",
    "Similar to the Classification problem, we need to collect the data for training. We will collect images that contain cars and provide labels. For Object Detection problem we also need to annotate the locations of the cars in the images to be part of the label. With the prepared dataset, we can do training with the Object Detection neural network.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will build the car object detector based on the following steps\n",
    "\n",
    "1.   Collect images for the class car\n",
    "2.   Annotate each of the objects inside the images \n",
    "3.   Train the model with the training set and evaluate it performance\n",
    "4.   Prediction using trained model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DAdHp9orDa6_"
   },
   "source": [
    "# **1. Collect images for the class car**\n",
    "\n",
    "We have pre-collected about 25 images of the car for this lab.  It is split into 20 images for training and  5 images for validation.\n",
    "\n",
    "It is stored in the following dataset directory.\n",
    "\n",
    "For train data, it store in the path ./dataset/Lab2dataset/data/train.\n",
    "```\n",
    "./dataset/Lab2dataset/data/train\n",
    "                           |- annotations\n",
    "                           |- images\n",
    "                   \n",
    "```\n",
    "For train data, it store in the path ./dataset/Lab2dataset/data/validation.\n",
    "```\n",
    "./dataset/Lab2dataset/data/validation \n",
    "                           |- annotations\n",
    "                           |- images\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "At the upper right corner of the jupyter notebook, select the pre-installed python virtual environment.\n",
    "Look for python virtual environment with the name tf1env. Select this for our lab exercise. \n",
    "\n",
    "![image](https://www.linkpicture.com/q/tf1env.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Try to display different images with the following code.\n",
    "\n",
    "- Set the path to the train image eg. data_path_train_images\n",
    "- Set the file name eg.img1.jpg\n",
    "- The complete string for the path and file will be eg.  data_path_train_images+'img1.jpg'\n",
    "\n",
    "If you can do so, that means the dataset is copied correctly into the lab directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23414,
     "status": "ok",
     "timestamp": 1600853236621,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "KhTgJj138RC4",
    "outputId": "69c61f95-5c94-47a6-d9f0-acf69ca4eaf5"
   },
   "outputs": [],
   "source": [
    "#Set the variables to store the different path for dataset\n",
    "data_dir_path='./dataset/Lab2dataset/'\n",
    "models_path='./dataset/Lab2dataset/models/'\n",
    "data_path_train_images='./dataset/Lab2dataset/train/images/'\n",
    "data_path_validation_images='./dataset/Lab2dataset/validation/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# TODO: complete the code below\n",
    "img = mpimg.imread(#add code - train path and image file name)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ds3nszHmDtys"
   },
   "source": [
    "# **2. Annotate each of the objects inside the image**\n",
    "\n",
    "In this section we will try to practice how to label and annotate image data. With the help of the LabelImage application, we can label the object and define bounding box parameters.\n",
    "\n",
    "This execrise will be running on the local computer with the Window OS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. In local computer command prompt type labelImg, the labelImage application will launch.\n",
    "\n",
    "![image](https://www.linkpicture.com/q/labelimage_1.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Use the internet browser download a image(jpg) eg. image.jpg with cars insides.\n",
    "\n",
    "![image](https://www.linkpicture.com/q/images_141.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. In labelImage application,select menu Open. Then look for the downloaded image file name eg. image.jpg. Select file and click Open.\n",
    "\n",
    "![image](https://www.linkpicture.com/q/labelimage_2.jpg)\n",
    "\n",
    "\n",
    "4. Once the image is loaded. Goto menu Edit select Create RectBox. Use the curser to draw a box to house the car inside.\n",
    "\n",
    "![image](https://www.linkpicture.com/q/labelimage_3.jpg)\n",
    "\n",
    "\n",
    "5. Enter the label name for the object in the box and click on OK button. In the case, our object will car(label).\n",
    "\n",
    "![image](https://www.linkpicture.com/q/labelimage_4.jpg)\n",
    "\n",
    "6. Repeat the step 5 and 6 to complete the annotation of the other cars in the image.\n",
    "\n",
    "![image](https://www.linkpicture.com/q/labelimage_5.jpg)\n",
    "\n",
    "7. Select menu File and set the annotation output to be PascalVOC. It is the format for the label and bounding box parameters to be stored in the XML file.\n",
    "\n",
    "![image](https://www.linkpicture.com/q/labelimage_6.jpg)\n",
    "\n",
    "\n",
    "8. Select menu File and choose Save. \n",
    "\n",
    "![image](https://www.linkpicture.com/q/labelimage_7.jpg)\n",
    "\n",
    "\n",
    "9.  At the dialog box, you will see a XML file name with the same name as your image file. Click on Save button, a XML file will save into the same directory where your image is.\n",
    "\n",
    "![image](https://www.linkpicture.com/q/labelimage_8.jpg)\n",
    "\n",
    "\n",
    "Open up the xml file with any text editor to observe the content inside it.\n",
    "\n",
    "Can you find where is the label and annotation parameters for each of the car?\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "    The car label is stored inside the XML tag name \n",
    "    \n",
    "    ```\n",
    "    \n",
    "    <name>car</name>\n",
    "    \n",
    "    \n",
    "    ```\n",
    "\n",
    "    The bounding box is stored inside the XML tag name \n",
    "    \n",
    "    ```\n",
    "    <bndbox>\n",
    "\t\t\t<xmin>36</xmin>\n",
    "\t\t\t<ymin>22</ymin>\n",
    "\t\t\t<xmax>189</xmax>\n",
    "\t\t\t<ymax>118</ymax>\n",
    "\t</bndbox>\n",
    "\n",
    "    ```\n",
    "\n",
    "<br/>\n",
    "</details>\n",
    "\n",
    "This exercise let us have a feel how to do the image annotation to prepare data for object detection training. It is a tedious process to complete for all the images. Due time constraint we will use the dataset that is copied in the VM to do the rest of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHfoQbCRGjfQ"
   },
   "source": [
    "# **3. Train the model with the training set and evaluate it performance**\n",
    "\n",
    "YOLO is very popular because it achieves high accuracy while also being able to run in real-time. The algorithm “only looks once” at the image in the sense that it only requires the image or the video to pass through the neural network once to make predictions.\n",
    "\n",
    "With YOLO, a single CNN simultaneously predicts multiple bounding boxes and class probabilities for those boxes. This basically means they recognize where the object is at and uses bounding boxes to show where it is at, and uses class probability to determine what the object is.\n",
    "\n",
    "\n",
    "![image](https://www.linkpicture.com/q/yolo_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fks1o7zc83IX"
   },
   "source": [
    "### Execrise\n",
    "\n",
    "In this exercise we are going to define the Custom YOLO Object Detection model and perform training.\n",
    "\n",
    "We will reuse a YOLO pre-trained model in our problem, this is also known as transfer learning. Transfer learning exploits the knowledge gained from a previous task to improve generalization about another. \n",
    "\n",
    "- Set the image dataset path for the training. Remember we have copy our dataset to the directory /dataset/Lab2dataset/data/train previously. Define the path inside the function trainer.setDataDirectory(data_directory= path)\n",
    "\n",
    "   \n",
    "- Set our YOLO object detection model training parameters in the following function \n",
    "```\n",
    "trainer.setTrainConfig(object_names_array=[#add code], batch_size=#add code, num_experiments=#add code, train_from_pretrained_model=car_dir_path+\"pretrained-yolov3.h5\")\n",
    "```\n",
    "\n",
    "    *   object_names_array : this is an array containing the names of the objects in our dataset(in our case we labeled it car) eg. ['car']\n",
    "    *   batch_size : this is to state the batch size for the training\n",
    "    *   num_experiments : this is to state the number of times the network will train over all the training images, which is also called epochs\n",
    "    *   train_from_pretrained_model : this is to train using transfer learning from a pre-trained YOLOv3 model\n",
    "\n",
    "\n",
    "\n",
    "Observe the training loss result by applying different epoch(num_experiements).\n",
    "\n",
    "What happen to the loss if the epoch increase?\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "Typically the increase in the number of iterations of the training will help to reduce the loss. Therefore the accuracy of the prediction will increase.\n",
    "<br/>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The trained weights file is recorded with the number of epoch and the loss value eg. detection_model-ex-025--loss-0005.643.h5. \n",
    "\n",
    "You can use these different trained weights to test the prediction in the next exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FrVHH-Ha9CVl",
    "outputId": "11ce3c80-fc36-4cec-d503-d0b51a9543d0"
   },
   "outputs": [],
   "source": [
    "\n",
    "from imageai.Detection.Custom import DetectionModelTrainer\n",
    "trainer = DetectionModelTrainer()\n",
    "trainer.setModelTypeAsYOLOv3()\n",
    "\n",
    "# TODO: complete the code below-add the main path where data is stored\n",
    "trainer.setDataDirectory(data_directory=  #add code)\n",
    "                         \n",
    "# TODO: complete the code below-Set the training parameters \n",
    "trainer.setTrainConfig(object_names_array=[#add code], batch_size=4, num_experiments=#add code, train_from_pretrained_model=data_dir_path+\"pretrained-yolov3.h5\")\n",
    ".\n",
    "trainer.trainModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iX-LsyOFHnup"
   },
   "source": [
    "# 4. Prediction using trained model\n",
    "\n",
    "The trained object detection model will predict the objects’ output labels with the corresponding bounding boxes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "For this exercise, we will load the trained weight into the YOLO neural network and perform the prediction.\n",
    "\n",
    "- Choose one of the weights file from the previous exercise. Assign the file path and file name in the following YOLO neural network function. eg. models_path+'detection_model-ex-009--loss-0016.248.h5'\n",
    "```\n",
    "detector.setModelPath(file path + file name)\n",
    "```\n",
    "- Assign the unknow image file path and file name to the following YOLO neural network function. Define a file path and different file name to store predicted result. eg. data_dir_path+'image5.jpg'\n",
    "```\n",
    "detector.detectObjectsFromImage(input_image=file path + file name, output_image_path=file path + different file name)\n",
    "```\n",
    "\n",
    "\n",
    "Run the prediction and observe the result label name, probability and box_points.\n",
    "\n",
    "What are all there predicted results represent?\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "The label name shows the object being classified and the probability displayed how confidence the object being predicted. And the box points show the localization of the object using the bottom left coordinate and the upper right coordinate to form a bounding box.\n",
    "<br/>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "Try with another weights with lower loss value and compare the results.\n",
    "Observe the probability and the postion of the bounding box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15890,
     "status": "ok",
     "timestamp": 1600854038207,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "jBTP2vr0Hubs",
    "outputId": "c6f5fb01-bdcf-4e07-9ac5-13fbffdb2458"
   },
   "outputs": [],
   "source": [
    "from imageai.Detection.Custom import CustomObjectDetection\n",
    "detector = CustomObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "\n",
    "# TODO: complete the code below\n",
    "detector.setModelPath(#add code)\n",
    "\n",
    "detector.setJsonPath(data_dir_path+\"json/detection_config.json\")\n",
    "\n",
    "detector.loadModel()\n",
    "\n",
    "# TODO: complete the code below\n",
    "detections = detector.detectObjectsFromImage(input_image=#add code, output_image_path=#add code)\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3076,
     "status": "ok",
     "timestamp": 1600854071904,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "PKPKBwYyEpuS",
    "outputId": "7f4e2f6b-96c2-4375-a6f1-d51b61d23fe2"
   },
   "outputs": [],
   "source": [
    "detections = detector.detectObjectsFromImage(input_image=data_dir_path+\"image5.jpg\", output_image_path=data_dir_path+\"image5a-detected.jpg\")\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM+0kFKqv5vdeYNERkCEuvP",
   "name": "Lab3_Car_ObjectDetection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
