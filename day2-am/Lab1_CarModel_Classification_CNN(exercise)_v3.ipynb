{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://www.linkpicture.com/q/nyplogo.jpg)\n",
    "\n",
    "Welcome to the lab! Before we get started here are a few pointers on Jupyter notebooks.\n",
    "\n",
    "1. The notebook is composed of cells; cells can contain code which you can run, or they can hold text and/or images which are there for you to read.\n",
    "\n",
    "2. You can execute code cells by clicking the ```Run``` icon in the menu, or via the following keyboard shortcuts ```Shift-Enter``` (run and advance) or ```Ctrl-Enter``` (run and stay in the current cell).\n",
    "\n",
    "3. To interrupt cell execution, click the ```Stop``` button on the toolbar or navigate to the ```Kernel``` menu, and select ```Interrupt ```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFoLOed-Vm5R"
   },
   "source": [
    "# Lab 1 : Car Model Classification with CNN\n",
    "In this lab we will try to build a Vehicle Classifier to classify three different models of the car. We will collect 3 groups of images namely  Honda Civic, Toyota Altis and Volkswagen Passat to be our dataset. The  collected dataset will be used to train the Resnet50 CNN(Convolution Neural Network). Upon completion of the training, we will be able to classify an unknown image to give a prediction of the model of the car.  \n",
    "\n",
    "\n",
    "We will build the car model classifier based on the following steps\n",
    "\n",
    "1.   Select the python virtual environment   \n",
    "2.   Import the libraries needed for the program\n",
    "3.  Prepare the data\n",
    "4.   Prepare the CNN model\n",
    "5.  Train the model with the training set and evaluate its performance\n",
    "6.   Use the trained model to classify input data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gr9UQHf5JwE3"
   },
   "source": [
    "# 1. Select the python virtual environment\n",
    "\n",
    "At the upper right corner of the jupyter notebook, select the pre-installed python virtual environment.\n",
    "Look for python virtual environment with the name tf1env. Select this for our lab exercise. \n",
    " \n",
    " \n",
    " ![image](https://www.linkpicture.com/q/tf1env.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hj1dHMgCLr9a"
   },
   "source": [
    "# 2. Import the libraries needed for the program\n",
    "We will begin by importing the libraries that we need, mainly Keras.  \n",
    "Keras is based on a minimal framework that provides a simpler way to create deep learning models based on TensorFlow.\n",
    "Keras contains useful functions for pre-preprocessing of image data and definition of convolution neural network.\n",
    "Matplotlib is used for data visualization. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2369,
     "status": "ok",
     "timestamp": 1600739644086,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "LjhErGYmV2Qi",
    "outputId": "129e7331-85da-4362-9c7d-4fc502d1be67"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras import optimizers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 961,
     "status": "ok",
     "timestamp": 1600739647808,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "EAx4b2JXiWYm",
    "outputId": "e8e8bc65-31a7-46ae-836a-ff427ed175ae"
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhr5JMTPMBlo"
   },
   "source": [
    "# 3. Prepare the data\n",
    "\n",
    "Usually in ML, we divide our data into 3 different sets\n",
    "\n",
    "- Training Data: Collection of sample data used to train the neural network.\n",
    "\n",
    "- Validation Data: Collection of sample data used to provide an unbiased evaluation of neural netowork during the training.\n",
    "\n",
    "- Test Data: Collection of sample data used to evaluate the trained neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptJqGwgQNaDf"
   },
   "source": [
    "### 3.1 Collect data for training and validation\n",
    "\n",
    "We have pre-collected data that are stored in the following directory structure.\n",
    "\n",
    "For train data, it is stored in the path ./dataset/Lab1dataset/data/train.\n",
    "```\n",
    "./dataset/Lab1dataset/data/train\n",
    "                           |- Honda\n",
    "                           |- Toyota\n",
    "                           |- Volkswagen \n",
    "\n",
    "```\n",
    "Each of the above sub-directories(Honda, Toyota, Volkawagen) is stored with\n",
    "60 different jpg images.\n",
    "\n",
    "\n",
    "For validation data, it is stored in the path ./dataset/Lab1dataset/data/validation.\n",
    "```\n",
    "./dataset/Lab1dataset/data/validation.\n",
    "                               |- Honda\n",
    "                               |- Toyota\n",
    "                               |- Volkswagen \n",
    "```\n",
    "Each of the above sub-directories(Honda, Toyota, Volkawagen) is stored with\n",
    "15 different jpg images.\n",
    "\n",
    "\n",
    "For test data, it is stored in the path /dataset/Lab1dataset/prediction_images\n",
    "```\n",
    "./dataset/lab1dataset/\n",
    "    |- prediction_images\n",
    "\n",
    "```\n",
    "A few test images are stored in the above test directory.\n",
    "\n",
    "\n",
    "Start the terminal. Use the following command to copy the dataset for the lab\n",
    "```\n",
    "cp -rf dataset ~/git/mindef-ai/day2-am\n",
    "```\n",
    "\n",
    "![image](https://www.linkpicture.com/q/terminal.jpg)\n",
    "\n",
    "\n",
    "We will define the following variables for the different data path.\n",
    "\n",
    "It will be easier for us to reference these paths in other sections of the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1208,
     "status": "ok",
     "timestamp": 1600739785215,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "UqJN2hz7ZUy_"
   },
   "outputs": [],
   "source": [
    "# Define train data paths, validation data path and test data path \n",
    "data_dir_path='./dataset/Lab1dataset/data/'\n",
    "train_data_dir = data_dir_path+'train/'\n",
    "validation_data_dir = data_dir_path+'validation/'\n",
    "prediction_data_dir = './dataset/Lab1dataset/prediction_images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Let explore the images we had collected for the train dataset.\n",
    "\n",
    "Display the following images from the train data.\n",
    "\n",
    "* image 1 ( train_data_dir+\"Honda/100.jpg\")\n",
    "* image 2 (  train_data_dir+\"Toyota/Altis.jpg\")\n",
    "\n",
    "What do you notice about the size of each of the image?\n",
    "Read the printed image size ( height,width, color).\n",
    "\n",
    "*color(1-greysacle, 3-color)\n",
    "\n",
    "Why are we interested in the image size?\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "    \n",
    "When we build a Convolution Neural Network(CNN), we usually specify a fixed input size. Therefore we need to pre-process the collected images to the same size as the expected input before we can do training.\n",
    "    \n",
    "<br/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# TODO: complete the code below\n",
    "#image 1\n",
    "# img = mpimg.imread(#add code)\n",
    "print(img.shape)\n",
    "imgplot = plt.imshow(img)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the code below\n",
    "#image 2\n",
    "# img = mpimg.imread(#add code)\n",
    "print(img.shape)\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wDuVmIyrSoFe"
   },
   "source": [
    "### 3.2 Prepare Data for training and validation\n",
    "\n",
    "From the previous exercise, we noticed that the collected data need to be pre-processed before we can use it to train our CNN. In this section, we will look at some of the methods that can help us to do data pre-processing.\n",
    "\n",
    "Data augmentation is a strategy that enables developer to significantly increase the diversity of data available through data pre-processing.\n",
    "\n",
    "Data augmentation techniques such as resizing, rotation, and cropping are commonly used to train large neural networks.\n",
    "\n",
    "\n",
    "![image](https://www.linkpicture.com/q/Augmentation.png)\n",
    "\n",
    "Kera provides us a function to do data augmentation. We can configure the parameters in the function to achieve the required image pre-processing. We will do some of the parameters configuration in the exercise.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "We will configure the image augmentation parameters in the Keras ImageDataGenerator() function for the train and validation data.\n",
    "\n",
    "Here are some for the parameters.\n",
    "\n",
    "- rescale factor. Defaults to None. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided. For training neural network, we usually want to keep our values small (e.g. between 0 and 1.0) so that the network can be more stable and converge faster.\n",
    "\n",
    "- shear_range: Float. Shear Intensity (Shear angle in counter-clockwise direction in degrees) eg. 0.2 degrees\n",
    "\n",
    "- zoom_range: Float. zoom range for width and height. eg. 0.2\n",
    "\n",
    "- horizontal_flip: Boolean. Randomly flip inputs horizontally.eg. True\n",
    "\n",
    "Set the appropriate parameter values in the following codes marked with #add code \n",
    "\n",
    "\n",
    "Also examine the parameter *target_size* in the function train_datagen.flow_from_directory() and validation_datagen.flow_from_directory.\n",
    "\n",
    "What is the target_size we need to choose for the data pre-processing?\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "\n",
    "As mentioned previously, we usually specify an expected fixed data input size when we build a CNN. Therefore we need to set the target_size to be the same as the expected input size for pre-processing. How do we know the expected size? We will find out in the following section when we setup our CNN.\n",
    "    \n",
    "<br/>\n",
    "</details>\n",
    "\n",
    "<br/>\n",
    "\n",
    "We will put the above pre-processing steps for train and validation inside the function name PrepareData().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Data for training and validation\n",
    "\n",
    "def PrepareData(img_width,img_height,batch_size ):\n",
    "\n",
    "    # This augments the data. This is usefull when working with a small sample size\n",
    "    # TODO: complete the code below\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale= #add code,\n",
    "        shear_range= #add code,\n",
    "        zoom_range= #add code,\n",
    "        horizontal_flip= #add code)\n",
    "    \n",
    "    # TODO: complete the code below\n",
    "    validation_datagen = ImageDataGenerator(rescale=#add code)\n",
    "\n",
    "    print(\"train generator\")\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    print(\"validation generator\")\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "    return train_generator,validation_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h6KWkyz_PjU3"
   },
   "source": [
    "### 3.3 Get the total number of predicted classes/labels\n",
    "\n",
    "The label of the training samples are determined by the subdirectory name, e.g.In the train sub-directories, Honda is the label to tag to all the images collected for that group.\n",
    "\n",
    "In our case, the number of predicted classes (or labels) are based on the numbers of sub-directories in the train folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "We are going to find the number of classes and the labels for our collected data.\n",
    "\n",
    "Where can we get the information for the number of classes and the Label names?\n",
    "Look for the directory where we stored our train data. Complete the following code. \n",
    "\n",
    "Observe the number of classes printed at the output. Why the number of classes is 3?\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "In our classification problem, we need to classify three types of cars. Therefore the classes is defined to be 3.\n",
    "\n",
    "The number of classes is to allow the CNN to predict the number of outputs.\n",
    "    \n",
    "<br/>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the total no. of classes and labels\n",
    "\n",
    "# TODO: complete the code below -what is the directory of the train data\n",
    "classes = ImageDataGenerator().flow_from_directory(#add code).class_indices\n",
    "print(classes)\n",
    "print(\"number of classes=\"+ str(len(classes)))\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QO0RcRNQSS0-"
   },
   "source": [
    "# 4. Prepare the CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J3-_hKmxN20M"
   },
   "source": [
    "### 4.1 CNN Convolution Netural Network\n",
    "\n",
    "\n",
    "![image](https://www.linkpicture.com/q/resnet50.png)\n",
    "\n",
    "There are many CNN implementation, such as LeNet, AlexNet, VGG, GoogLeNet, ResNet and more.\n",
    "\n",
    "In this lab we will be using ResNet50 CNN.\n",
    "\n",
    "The ResNet50 is built from the different combination of the following layers\n",
    "\n",
    "        *   Conv2D-> number of feature maps, feature map size (width x height)\n",
    "        *   Activation function -> relu , sigmoid \n",
    "        *   MaxPooling-> kernel size (width x height)\n",
    "\n",
    "together with the output layers\n",
    "\n",
    "        *   Flatten-> 2D to 1D\n",
    "        *   Dense layer->number of neutrons\n",
    "        *   Activation Function->softmax(probability of each the classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execrise\n",
    "We have a brief understanding of how the ResNet50 layers are formed. In this exercise, we will customise our ResNet50 to train our pre-process dataset.\n",
    "\n",
    "Assuming we want to have input image size of (197, 197, 3) (height, width, channels). In the code below, change the function ResNet50() to have the the specified input size.\n",
    "\n",
    "- In the following code, goto function ResNet50() set the parameters input_size to (width,height,color)->(197,197,3)\n",
    "\n",
    "Based on the number of classes we have discuss in the previous exercise 3.3, what do you think is the number of neurons required in the output Dense layer? \n",
    "\n",
    "- Add the number of neurons required in the output Dense layer? \n",
    "\n",
    "\n",
    "Run the function compileModel() to build the custom ResNet50.\n",
    "\n",
    "Observe the print out of the ResNet50 layers. What have we customised in our ResNet50 CNN?\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "    \n",
    "We have setted our customise ResNet50 with the input size (197,197,3), and output to be (3)\n",
    "\n",
    "    \n",
    "<br/>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileModel():\n",
    "    print(\"compiling model\")\n",
    "    learning_rate=1e-4\n",
    "\n",
    "     ## TODO: Add in the input size\n",
    "    conv_base = ResNet50(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(#add code, #add code, #add code))\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(conv_base)\n",
    "    #Add dense and classification layer\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    \n",
    "    ## TODO: Change the Output Dense layer to have the correct number of neurons  \n",
    "    model.add(layers.Dense(#add code, activation='softmax'))\n",
    "\n",
    "    print(conv_base.summary())\n",
    "    print(model.summary())\n",
    " \n",
    "    model.compile(loss='categorical_crossentropy',optimizer= optimizers.adam(lr=learning_rate),metrics=['accuracy'])\n",
    "    for layer in conv_base.layers:\n",
    "      layer.trainable = False\n",
    "    for layer in conv_base.layers[-4:]:\n",
    "      layer.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compileModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAFGrHhTTIWX"
   },
   "source": [
    "# 5. Train the model with the training set and evaluate it performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1V4dlZyHUVTM"
   },
   "source": [
    "### 5.1 Train and validate the model\n",
    "\n",
    "\n",
    "![image](https://www.linkpicture.com/q/training.png)\n",
    "\n",
    "With the preprocessed training and validation dataset and our defined model. We can start to preform the training process.\n",
    "As part of the training, we will also need to set some parameter values in the training function (the ``fit()``)\n",
    "\n",
    "- Set the number of epoch to  indicates the number of passes of the entire training dataset the model has to complete.\n",
    "\n",
    "- When we have a huge data set it not possible to load the entire data to run one epoch of training. Setting Step per epoch allow huge data to divide into batches to complete entire training.\n",
    "\n",
    "We save the final training iteration weights into to a binary data format(.h5).\n",
    "\n",
    "In the trainModel() function it will save the final iteration weights into a file \"custom_w_supervision_try2.h5\".\n",
    "We also set a ModelCheckpoint() function to monitor the validation accuracy in each of the training iterations. This function will save the weights into a file \"custom_w_supervision_try2_best.h5\" with the best validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1600740440814,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "Nf9oc01xgnos"
   },
   "outputs": [],
   "source": [
    "best_model = keras.callbacks.ModelCheckpoint(data_dir_path+'custom_w_supervision_try2_best' + '.h5', monitor='val_acc',save_best_only=True)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss',factor=0.25, patience=5,min_lr=0.000005)\n",
    "\n",
    "\n",
    "def trainModel(train_data, validation_data,model):\n",
    "\n",
    "    print(\"starting training.... \")\n",
    "    hist = model.fit_generator(\n",
    "        (train_data),\n",
    "        steps_per_epoch=nb_train_samples // batch_size, # The accumulated amount of steps\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_data,\n",
    "        nb_val_samples=nb_validation_samples,\n",
    "        callbacks=[best_model, reduce_lr]\n",
    "    )\n",
    "\n",
    "    plotVal_plotLoss(hist)\n",
    "    model.save_weights(data_dir_path+'custom_w_supervision_try2.h5') # Saving the compile weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following function plotVal_plotLoss() is to plot the accuracy and loss result of the model during the training.\n",
    " \n",
    "- Loss is a number that indicates the difference between the model's prediction output with the ground truth. \n",
    "\n",
    "- Accuracy is a metric that can be applied to classification tasks only. It describes just what percentage of your train/test data are classified correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1600740443959,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "PxFBEdKJg36i"
   },
   "outputs": [],
   "source": [
    "# This function generates graphs of the loss and the accuracy of the model\n",
    "def plotVal_plotLoss (model) :\n",
    "\n",
    "    plt.plot(model.history['acc'])\n",
    "    plt.plot(model.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('loss_plot_4 (simulated vgg1)2')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(model.history['loss'])\n",
    "    plt.plot(model.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('loss_plot_4 (simulated vgg1)2')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now we are ready to start the training process.\n",
    "\n",
    "From section 4.1 exercise, we have set the ResNet50 input size. We preprocess our data to the same width and height before it can be used for the training.\n",
    "\n",
    "- Set the PrepareData() with the correct ``image_height`` and ``Image_width`` \n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "        image_height=197  \n",
    "        Image_width=197\n",
    "<br/>\n",
    "</details>\n",
    "\n",
    "\n",
    "Set the number of epoch(iterations) to conduct the training\n",
    "\n",
    "- At the  variable epochs set a number(integer number) \n",
    "\n",
    "\n",
    "The trainModel() function will start the training.\n",
    "\n",
    "- Since we have 180 samples, and we are using a batch size of 30, how many training steps are required to go through all the samples? Observe the training output and see whether the number of steps is the same as your answer. \n",
    "<details><summary>Click here for answer</summary>\n",
    "<br/>\n",
    "     Step per ecoph = 6 \n",
    "<br/>\n",
    "</details>\n",
    "\n",
    "\n",
    "You can vary the epochs to observe the model accuracy and loss values.\n",
    "\n",
    "- What do you notice when epochs are varied?\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "Typically the increase in the epochs will reduce the loss and improve the prediction accuracy.    \n",
    "\n",
    "<br/>\n",
    "</details>\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 159950,
     "status": "ok",
     "timestamp": 1600741556038,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "RXNggLb7g4MG",
    "outputId": "7e0f3b08-0ce6-4028-9067-4ddb8b04d6e5"
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "## TODO: Set the image width and height \n",
    "img_width, img_height = #add code, #add code\n",
    "\n",
    "# The batch size represents the total amount of pictures that are included in each iteration.\n",
    "## TODO: Set the batch size\n",
    "batch_size = 30\n",
    "\n",
    "#Prepare data\n",
    "train_data, validation_data = PrepareData(img_width,img_height,batch_size)\n",
    "\n",
    "#Load Model\n",
    "model=compileModel()\n",
    "\n",
    "# Defining the total amount of samples in both the training and validation set\n",
    "nb_train_samples =180\n",
    "nb_validation_samples =45\n",
    "\n",
    "## TODO: Set the number to training iterations\n",
    "epochs = #add code\n",
    "\n",
    "#Start the training model\n",
    "trainModel(train_data, validation_data, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBdPXDexTSsX"
   },
   "source": [
    "# 6. Use the trained model to classify input data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Setup a predict image function\n",
    "\n",
    "After completed the training for the customised ResNet50 CNN model. The following predictImg() function will use the trained model to predict the output of an unknown image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1600740763311,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "o_hvMM66goEz"
   },
   "outputs": [],
   "source": [
    "\n",
    "def predictImg(path,img_width,img_height ,model):\n",
    "    imagep = image.load_img(path, target_size=(img_width, img_height))\n",
    "    x = image.img_to_array(imagep)\n",
    "    x = x / 255  # Insures that images are normalized, so it can be compared test on a model that also used normalized training and validation images\n",
    "\n",
    "    x = np.expand_dims(x, axis=0) # flattens the image\n",
    "    prediction = model.predict(x) # Extract the prediction made by the model\n",
    "    print(path)\n",
    "    print(prediction)\n",
    "    findLabel(prediction, 0.2, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1600740765510,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "b3efmUz5gxsj"
   },
   "outputs": [],
   "source": [
    "def findLabel(test, threshold, path):\n",
    "    if (max(test[0]) < threshold):\n",
    "        print(\"no class could be defined for \" + path + \" with threshold 0.85\")\n",
    "    else:\n",
    "        m = max(test[0])\n",
    "        index = [i for i, j in enumerate(list(test[0])) if j == m]\n",
    "        #labeler(index[0], path)\n",
    "        label = list(classes.keys())[index[0]]\n",
    "        print(\"The image '\" + path + \"' belongs to class: \" + label) # Prints the prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now we are ready to use the trained model to predict the image.\n",
    "\n",
    "First we need to load our Resnet50 CNN model with the trained weights. \n",
    "- During traning fit() function call the ModelCheckPoint() to monitor and save the weights with best validation accuracy. Find where is the directory and file name for best weights file. Input into the model.load_weights() function in the following code.\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "data_dir_path+'custom_w_supervision_try2_best.h5'\n",
    "<br/>\n",
    "</details>\n",
    "\n",
    "\n",
    "Next we need to specify an unknown images file for prediction.\n",
    "- Find a test image file in the directory './dataset/Lab1dataset/prediction_images/' pass into  predictImg() function\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "    eg.    prediction_data_dir+\"honda1.jpg\"\n",
    "<br/>\n",
    "</details>\n",
    "\n",
    "Try to load different test images to predict the output. You can check the predicted label against the test image file name. The test image file name is the ground truth label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60167,
     "status": "ok",
     "timestamp": 1600742011234,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "LRPFxU8PhC-6",
    "outputId": "f5ad104c-cd36-4906-e187-0d6afcc49a34"
   },
   "outputs": [],
   "source": [
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "model = compileModel()\n",
    "\n",
    "## TODO: Set the best-trained weigth dir and file name \n",
    "model.load_weights(#add code)\n",
    "\n",
    "## TODO: Set the unknown image dir and file name\n",
    "predictImg(#add code -image dir and file name, 197, 197 , model) \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNkruaBc4pLdY0qKoXWG6qt",
   "name": "Car3_Classification_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyCharm (keras-multi-output-model-utk-face-master)",
   "language": "python",
   "name": "pycharm-401b26e3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
